{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8960e1a",
   "metadata": {},
   "source": [
    "### Fake News Detections\n",
    "\n",
    "[A Survey on Natural Language Processing for Fake News Detection](https://aclanthology.org/2020.lrec-1.747.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "83378e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "fcf4eb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>says the annies list political group supports ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health care reform legislation is likely to ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the chicago bears have had more starting quart...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>when mitt romney was governor of massachusetts...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mccain opposed a requirement that the governme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0   says the annies list political group supports ...      0\n",
       "3   health care reform legislation is likely to ma...      0\n",
       "5   the chicago bears have had more starting quart...      1\n",
       "12  when mitt romney was governor of massachusetts...      0\n",
       "16  mccain opposed a requirement that the governme...      1"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "def get_df(fiepath: str, label_map={ \"true\": 1, \"false\": 0 }) -> pd.DataFrame:\n",
    "    df = pd.read_csv(fiepath, sep='\\t', header=None)\n",
    "    data = pd.DataFrame()\n",
    "    df = df[df[1].isin(label_map.keys())]\n",
    "    data[\"text\"] = df[2].apply(preprocess)\n",
    "    data[\"label\"] = df[1].apply(lambda l: label_map[l])\n",
    "    return data\n",
    "\n",
    "train_df = get_df(\"data/liar_train.tsv\")\n",
    "test_df = get_df(\"data/liar_test.tsv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "98d4fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train_features = vectorizer.fit_transform(train_df[\"text\"])\n",
    "x_test_features = vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "cb00d7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       0.82      0.90      0.86      1995\n",
      "       false       0.86      0.76      0.81      1676\n",
      "\n",
      "    accuracy                           0.84      3671\n",
      "   macro avg       0.84      0.83      0.83      3671\n",
      "weighted avg       0.84      0.84      0.83      3671\n",
      "\n",
      "Test Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       0.64      0.72      0.68       249\n",
      "       false       0.61      0.52      0.56       208\n",
      "\n",
      "    accuracy                           0.63       457\n",
      "   macro avg       0.62      0.62      0.62       457\n",
      "weighted avg       0.63      0.63      0.62       457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_features,  train_df[\"label\"])\n",
    "\n",
    "train_pred = lr.predict(x_train_features)\n",
    "test_pred = lr.predict(x_test_features)\n",
    "\n",
    "print(\"Train Scores:\")\n",
    "print(classification_report(train_df[\"label\"], train_pred, target_names=[\"true\", \"false\"]))\n",
    "print(\"Test Scores:\")\n",
    "print(classification_report(test_df[\"label\"], test_pred, target_names=[\"true\", \"false\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "619dffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       1.00      1.00      1.00      1995\n",
      "       false       1.00      1.00      1.00      1676\n",
      "\n",
      "    accuracy                           1.00      3671\n",
      "   macro avg       1.00      1.00      1.00      3671\n",
      "weighted avg       1.00      1.00      1.00      3671\n",
      "\n",
      "Test Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       0.62      0.78      0.69       249\n",
      "       false       0.61      0.42      0.50       208\n",
      "\n",
      "    accuracy                           0.61       457\n",
      "   macro avg       0.61      0.60      0.59       457\n",
      "weighted avg       0.61      0.61      0.60       457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train_features,  train_df[\"label\"])\n",
    "\n",
    "train_pred = rfc.predict(x_train_features)\n",
    "test_pred = rfc.predict(x_test_features)\n",
    "\n",
    "print(\"Train Scores:\")\n",
    "print(classification_report(train_df[\"label\"], train_pred, target_names=[\"true\", \"false\"]))\n",
    "print(\"Test Scores:\")\n",
    "print(classification_report(test_df[\"label\"], test_pred, target_names=[\"true\", \"false\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "4f6fb01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "x_train_features = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "x_test_features = count_vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "8f77531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       1.00      0.78      0.88      1995\n",
      "       false       0.79      1.00      0.88      1676\n",
      "\n",
      "    accuracy                           0.88      3671\n",
      "   macro avg       0.90      0.89      0.88      3671\n",
      "weighted avg       0.90      0.88      0.88      3671\n",
      "\n",
      "Test Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       0.63      0.40      0.49       249\n",
      "       false       0.50      0.73      0.59       208\n",
      "\n",
      "    accuracy                           0.55       457\n",
      "   macro avg       0.57      0.56      0.54       457\n",
      "weighted avg       0.57      0.55      0.54       457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "lr = GaussianNB()\n",
    "lr.fit(x_train_features.toarray(),  train_df[\"label\"])\n",
    "\n",
    "train_pred = lr.predict(x_train_features.toarray())\n",
    "test_pred = lr.predict(x_test_features.toarray())\n",
    "\n",
    "print(\"Train Scores:\")\n",
    "print(classification_report(train_df[\"label\"], train_pred,target_names=[\"true\", \"false\"]))\n",
    "print(\"Test Scores:\")\n",
    "print(classification_report(test_df[\"label\"], test_pred, target_names=[\"true\", \"false\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69495be8",
   "metadata": {},
   "source": [
    "### CNN for fake news detection\n",
    "\n",
    "[“Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection](https://arxiv.org/pdf/1705.00648)\n",
    "\n",
    "[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "b34ca2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lapin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>building a wall on the usmexico border will ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wisconsin is on pace to double the number of l...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>says john mccain has done nothing to help the ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>suzanne bonamici supports a plan that will cut...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when asked by a reporter whether hes at the ce...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  building a wall on the usmexico border will ta...      0\n",
       "1  wisconsin is on pace to double the number of l...      4\n",
       "2  says john mccain has done nothing to help the ...      4\n",
       "3  suzanne bonamici supports a plan that will cut...      2\n",
       "4  when asked by a reporter whether hes at the ce...      5"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "multi_class = { \"true\": 0, \"mostly-true\": 1, \"half-true\": 2, \"barely-true\": 3, \"false\": 4, \"pants-fire\": 5 }\n",
    "\n",
    "train_df = get_df(\"data/liar_train.tsv\", label_map=multi_class)\n",
    "test_df = get_df(\"data/liar_test.tsv\", label_map=multi_class)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "a51e82af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 14214\n"
     ]
    }
   ],
   "source": [
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "vocab = { token for statement in pd.concat([train_df[\"text\"], test_df[\"text\"]]) for token in word_tokenize(statement) }\n",
    "vocab.add(PAD)\n",
    "vocab.add(UNK)\n",
    "\n",
    "vocab_idx = { word:idx for idx, word in enumerate(vocab) }\n",
    "\n",
    "print(\"Vocab size:\", len(vocab_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "e2beae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type Dim = 50 | 100 | 200 | 300\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "def get_embeddings(vocab: dict[str, int], dim: Dim = 100) -> nn.Embedding:\n",
    "    embedding_matrix = torch.zeros(len(vocab), dim)\n",
    "    with open(f'glove/glove.6B.{dim}d.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            tokens = line.split()\n",
    "            word = tokens[0]\n",
    "            idx = vocab.get(word)\n",
    "            if idx is not None:\n",
    "                embedding_matrix[idx] = torch.tensor([float(val) for val in tokens[1:]], dtype=torch.float32)\n",
    "\n",
    "    return nn.Embedding.from_pretrained(embedding_matrix)\n",
    "\n",
    "embedding_layer = get_embeddings(vocab_idx, dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "8601cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATEMENT_DIM = 32\n",
    "\n",
    "def text_batch_to_idx_tensor(text_batch: list[str], vocab_idx: dict[str, int], length=STATEMENT_DIM) -> torch.Tensor:\n",
    "    pad_idx = vocab_idx[PAD]\n",
    "    unk_idx = vocab_idx[UNK]\n",
    "    or_else = lambda this, that: this if this is not None else that\n",
    "\n",
    "    def pad_list(lst):\n",
    "        return lst[:length] + [pad_idx] * max(0, length - len(lst))\n",
    "\n",
    "    return torch.tensor(\n",
    "        [pad_list([or_else(vocab_idx.get(word), unk_idx) for word in word_tokenize(text)]) for text in text_batch]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "1d4f1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super().__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim1, self.dim2)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    embedding_layer,\n",
    "    Transpose(1, 2),\n",
    "    nn.Conv1d(in_channels=EMBEDDING_DIM, out_channels=64, kernel_size=5),\n",
    "    nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3),\n",
    "    nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3),\n",
    "    nn.MaxPool1d(kernel_size=3),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 8, 6),\n",
    "    nn.Softmax(1)\n",
    ")\n",
    "\n",
    "\n",
    "# data = text_batch_to_idx_tensor(train_df[\"text\"], vocab_idx)\n",
    "# print(model(data).shape)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "beb16246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6735, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6726, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6716, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6689, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6670, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6664, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6648, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6627, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6615, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6602, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6583, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6568, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6558, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6542, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6526, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6518, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6511, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6503, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6489, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6461, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6445, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6444, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6428, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6409, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6398, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6388, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6375, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6361, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6350, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6334, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6325, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6318, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6298, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6290, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6276, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6261, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6252, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6240, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6230, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6221, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6213, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6215, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6201, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6185, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6165, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6147, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6145, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6141, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6127, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6109, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6093, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6089, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6083, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6069, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6057, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6044, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6033, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6025, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6017, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6010, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6002, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5975, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5954, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5936, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5926, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5918, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5913, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5913, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5925, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5922, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5905, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5868, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5862, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5871, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5857, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5834, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5829, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5833, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5817, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5800, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5799, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5796, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5780, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5768, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5768, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5764, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5748, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5739, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5739, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5730, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5717, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5711, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5708, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5700, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5690, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5685, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5686, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "data = text_batch_to_idx_tensor(train_df[\"text\"], vocab_idx)\n",
    "labels = torch.tensor(train_df[\"label\"].tolist())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    outputs = model(data)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "8164c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       0.56      0.33      0.42      1676\n",
      " mostly-true       0.52      0.60      0.56      1962\n",
      "   half-true       0.49      0.63      0.55      2114\n",
      " barely-true       0.48      0.47      0.47      1654\n",
      "       false       0.46      0.60      0.52      1995\n",
      "  pants-fire       0.00      0.00      0.00       839\n",
      "\n",
      "    accuracy                           0.49     10240\n",
      "   macro avg       0.42      0.44      0.42     10240\n",
      "weighted avg       0.46      0.49      0.47     10240\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        true       0.25      0.14      0.18       208\n",
      " mostly-true       0.26      0.30      0.28       241\n",
      "   half-true       0.22      0.31      0.26       265\n",
      " barely-true       0.20      0.19      0.19       212\n",
      "       false       0.24      0.29      0.26       249\n",
      "  pants-fire       0.00      0.00      0.00        92\n",
      "\n",
      "    accuracy                           0.23      1267\n",
      "   macro avg       0.20      0.21      0.20      1267\n",
      "weighted avg       0.22      0.23      0.22      1267\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lapin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\lapin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\lapin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\lapin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\lapin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\lapin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = text_batch_to_idx_tensor(train_df[\"text\"], vocab_idx)\n",
    "    labels = torch.tensor(train_df[\"label\"].tolist())\n",
    "    outputs = model(data)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    print(classification_report(labels, preds, labels=[0,1,2,3,4,5], target_names=[\"true\", \"mostly-true\", \"half-true\", \"barely-true\", \"false\", \"pants-fire\"]))\n",
    "\n",
    "    data = text_batch_to_idx_tensor(test_df[\"text\"], vocab_idx)\n",
    "    labels = torch.tensor(test_df[\"label\"].tolist())\n",
    "    outputs = model(data)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    print(classification_report(labels, preds, labels=[0,1,2,3,4,5], target_names=[\"true\", \"mostly-true\", \"half-true\", \"barely-true\", \"false\", \"pants-fire\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
