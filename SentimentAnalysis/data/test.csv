ID,review
0,"This is an outstanding work. It is very well written and the story line flows very smoothly. Even for a non expert in the topic, I could understand the motivation, the flowchart of the methodology and the impressive results reached. It is also an important contribution for the modeling of physical systems using Neural Networks. 

Pros:
* The description of the data used and the methods contains enough details to have a clear understanding of what they did.
* They clearly state the limitations of their results and provide insights of why their methods don't reach sometimes the best performance.
* They provide an outlook of next steps to improve their results.

Cons:
* There is an overfitting problem that might make their results not so appealing.
* I felt that the authors missed to emphasize the novelty of their contribution."
1,"The paper is motivated by the time consuming nature of discovering high entropy alloys (HEAs) with high yield strength (YS). Based on the difficulty, the authors try to use ML tools to estimate the YS of alloys directly. To prepare the training dataset, the authors combine a large quantity of simulation data and scarce real data. The authors then propose a novel approach, termed Bi-RPT to deal with the domain gap between the simulation data and real data. Empirical results validate the effectiveness of the proposed method over several baselines.

### Strength
The paper is well-motivated and well-written, and the proposed methods are introduced in detail with derivations.

Both the problem setting and the proposed method are interesting and novel at least to the reviewer's concern

### Weaknesses
The reviewer believes the paper in broad falls within the few-shot learning domain which is a well-estabilished field with plenty of prior work , and since this is an ML conference, when proposing a method, one application may not be enough to demonstrate the universal applicability. Hence the reviewer would suggest comparing the proposed method with more prior work and conducting the experiments on more datasets beyond the YS of alloy."
2,"Paper Summary:
The proposed work addresses the Segment Anything In Medical Images on a Laptop challenge via an adapted nnUNet-based approach that incorporates the possibility of adding bounding box prompts. The authors propose using a binary channel as additional input to the model to help the model focus on the region of interest within the bounding box. Their strategy yields competitive results in most of the regarded image domains in the validation set and outperforms the baseline in the microscopy domain by a large margin.

Strengths of the paper:
- Building on top of a well-established baseline is a compelling idea
- The proposed idea of incorporating guidance signals via an additional channel is established and motivated. 
- The authors use various engineering approaches to increase the inference speed of the model, such as JIT or Openvino, which are valuable contributions.
- The authors explore the effect of pretraining on natural images, which is interesting. 

Weaknesses of the paper:
- While the nnUnet does excel in many different domains (e.g. CT/MRI) in which the network does produce convincing results as the authors have shown, its out-of-the-box performance which involves various preprocessing steps tailored towards the image characteristics of e.g. CT may be suboptimal for domains such as PET. It could be worth investigating this further or stating if certain modifications to the nnUnet pipeline have been made to address this. 
- The patch-based approach is not explained clearly. The authors point out that it increases the latency and propose some strategies to mitigate these effects, but the reason why it is useful to opt for a patch-based strategy is not explained.
- While the results are competitive, the approach seems to perform slightly worse than the MedSam baseline on most domains in the validation set

Further ideas
- While the motivation to go for a single model instead of set of expert models can be justified it raises questions about the performance difference. This would be in particular interesting to see if knowledge from one domain is useful to perform better in another domain. This has been ablated from the natural to medical images, but not among different medical imaging modalities.
- The way to incorporate the bounding box prompt could be ablated further (e.g. binary mask vs. distance transforms). A natural question to ask is, how would the approach perform if only the bounding box itself is fed into the model instead of ignoring predictions outside of the bounding box in a post-processing approach?"
3,"The paper applied LLMs to automate the generation of hospital discharge summaries from physician notes, and evaluated the performance with clinicians from different clinical perspectives. Public benchmark dataset MIMIC-III was used.
- Quality: the paper is technically sound and of high quality. Data processing and filtering criteria, such as only including the first occurrence of duplicate text, is considerable to avoid potential information leakage. 
- Clarity: the paper is well-structured, precise and concise in experiment settings, results interpretation and limitation.
- Originality: the paper illustrated a novel application of existing technologies to a challenging and meaningful clinical task. Though the idea might not be the first, the implementation and thorough evaluation with clinicians make the paper sufficiently novel. 
- Significance: the clinical task of discharge summary is a challenging and important problem to address using advanced ML techniques. Moreover, the evaluation criteria of different types of mistakes in particular with LLM applications could be a good reference for other similar clinical applications using LLMs.

Questions and feedback:
1. How many clinicians reviewed the same discharge summary? If there's more than 1 reviews per summary, how you aggregate the evaluation metrics, especially how to handle conflict opinions.
2. It will be better to compare the current approach with some baselines to further improve the technical soundess, e.g. fine-tune encoder-decoder models from supervised training."
4,"Strength:
- This work develops PPG foundation models using a decoder-only transformer
- loss function, embedding, and linear head are specifically designed to fit PPG applications.
- Results in Table 1 show promising results

Weakness:
- Table 1 is a bit hard to read. Different performance metrics (MAE, F1, false alarm rates) are included. Please consider separating them. 
- BP-SBP is not introduced. Also, why is 9.56 highlighted?
- In the conclusion section, it is claimed that the foundation model can be used for downstream tasks without further fine-tuning. I am not sure which experiments can support this claim."
5,"Strengths:
1. Innovative Approach: The integration of physics-guided diffusion models with discrete action representation provides a novel approach to predicting long-term human behaviors. This method enhances the realism and feasibility of generated trajectories by incorporating physical constraints.
2. Robust Framework: The paper presents a comprehensive framework that combines Hierarchical Action Quantization (HAQ), Action Diffusion Policy, and Reachability Guidance. This multi-component strategy allows for a nuanced modeling of human behaviors that are both diverse and physically plausible.
3. Extensive Evaluation: The authors provide a detailed evaluation of two publicly available datasets, demonstrating superior performance in long-term trajectory prediction compared to several baselines. The use of both common and novel metrics enriches the validation process.
4. Theoretical and Practical Contributions: The paper makes significant theoretical contributions with its method for integrating discrete action spaces and diffusion models. Practically, it offers a new tool for improving human-robot interaction in complex scenarios.


Weaknesses:
1. Dependency on Accurate Physical Modeling: The performance of the model heavily relies on the accuracy of the physical modeling of human dynamics. Misestimations in this aspect could lead to less reliable trajectory predictions.
2. Generalization Across Different Contexts: While the model performs well on the tested datasets, its ability to generalize across different environments or more dynamic scenarios isn't fully explored. The model might perform differently in settings not covered by the datasets used."
6,"Summary: This paper proposed a novel method for parameter estimation and uncertainty quantification which is then applied to meaningful problems in weather and climate modeling. 

Strengths and Weaknesses:

* Should add (ours) to the legend in Figure 1 to clearly indicate the author's proposed methods.

[+] The paper is well-motivated and clearly presented. It provides a clear description of the problem and the proposed solution. It is very well written, with comprehensive references to relevant material. 

[+] The uncertainty quantification is a nice benefit over the baseline approaches (Smagorinsky & no-parameterization schemes)

[-] It would be nice to have an overview figure or algorithm of the proposed framework to show the flow from data to initialization to model to prediction and what each step entails. 

[-] It would be nice to have a ground truth solution plot, either in the appendix or main text, so readers can better visualize the problem being solved. From my understanding, the author used the problem in Ross et. al. (2023). In that paper, Figure 1 gives a nice visual of the vorticity. This would help connect readers back to the application of the author's method to Earth's weather and climate predictions. 

Conclusion: The paper is concise and to the point. While there could be some improvement in the presentation of the problem and methodology, the discussion is still clear. I recommend accepting the paper and believe it is important to develop better methodologies for this class of problems after reading the paper. While the proposed method didn't excel in all metrics over all times, it shows that it is still a complex and difficult problem to solve, and this work makes a nice step in the right direction."
7,"This paper studies the PAC learning of halfspaces under a constant malicious noise rate. Specifically, for any sample (x,y) drawn from a given distribution, an adversary can replace it with an arbitrary pair with some probability. This paper presents an algorithm that requires 1/sqrt{d} margin and allows any constant noise level. The core idea of the algorithm is to use a linear program to reweight the queries, thereby minimizing the impact of corrupted samples. This approach builds on the framework established by Talwar 2020 with some new ingredients. 

Overall, this is a nice result. However, the contribution appears to be incremental, and the technical advancements are also not particularly interesting, as they largely build on previous work."
8,"In this work, the authors address a critical issue of evaluating the hallucination of LVLMs in a clinical context. As the authors assert, developing methods to assess hallucinations of these models in a quantitative and automated way is important. For this, the authors employ the CHAIR metric used in image captioning and propose a new domain knowledge hallucination metric. The authors present an initial evaluation of LLaVA-Med using the metric on the MedICAT dataset. 

Comments:

- The authors address an important issue regarding hallucination of LVLMs in a clinical context.
- The proposed metric seems reasonable, however, the fact that an additional LLM is used obtain ground truth object observations is questionable. LLMs themselves are not fully validated in terms of their performance, so a proper evaluation/validation study of this step is necessary
- For assessing object hallucination and domain knowledge, the authors utilize cosine similarity of embeddings with BioBERT. Again, although the method addresses scalability, careful validation of this method for assessment is needed.
- Minor: the figure seems to be assessing GPT-4V whereas the text is evaluating LLaVA-Med

Overall, the work addresses an important issue and presents a reasonable set of metrics for assessing hallucination. Although the study is preliminary, the work will garner relevant discussion, in particular regarding the usage of existing models (such as BioBERT and GPT-4) for assessing other LVLMs."
9,"This paper studies generalization bounds for learning models that minimize some loss function (such as the empirical loss) that depends on some random training data. The authors propose a deterministic PAC-Bayes view where the randomness only comes from the training data and from the distribution of the initial point to the algorithm, but the training algorithm is deterministic such as gradient descent or gradient flow. The authors prove an upper bound to the generalization error that holds with high probability. Notably, the upper bound is a function of the iterates of the training algorithm, so in principle it can be computed in practice along the algorithm execution. The authors provide some examples where their result can give a tighter upper bound in some regime compared to existing bounds. The authors also extend their results to show upper bound guarantees for other iterative algorithms, including stochastic gradient descent, momentum methods, and damped Hamiltonian methods.

To prove the results, the authors use the continuity equation for the gradient flow (or the change of variable formula for the algorithms) to track the evolution of the log density of the distributions of the iterates along the algorithm. The authors then combine this with standard concentration argument using Markov inequality to derive the high-probability bound for the generalization error. The use of the continuity equation is natural given the dynamics, and it is interesting to see how to use it to bound the generalization error. The technique of tracking the change in distributions is very flexible and can be extended to any iterative algorithms, as the authors demonstrated.

This paper is well-written and provides a good overview of the problem, techniques and results, and comparison with related literature. This paper makes a concrete contribution to the theory of generalization error by providing a new upper bound which is instance-specific and seems to be better in some settings. 

In Thm 4: Is s’ = s?"
10,"## Paper Summary
The goal of $\mathcal{H}$-consistency bounds are to relate the performance of a
classifier $h \in \mathcal{H}$ under a surrogate loss function to its
performance with respect to the target 0-1 loss. In particular, the goal is to
bound the 0-1 generalization error that is in excess of the best-in-class. This
is to be given as a function of the excess surrogate generalization error: 

$$\mathrm{error}_{01}(h) - \min_{h' \in \mathcal{H}} \mathrm{error}_{01}(h')  \leq \mathrm{function}\left(\mathrm{error}_{\mathrm{surrogate}}(h) - \min_{h' \in \mathcal{H}}\, \mathrm{error}_{\mathrm{surrogate}}(h')\right).$$

This paper provides tools that allow for more fine-grained bounds that achieve
rates that are not achievable by prior work. There are settings for which these
bounds are tight. This paper also applies these tools to show
$\mathcal{H}$-consistency bounds for various settings.

## Review
This paper addresses a very interesting question and seems to develop some
fairly useful and novel tools for $\mathcal{H}$-consistency. The main weakness
of this paper is its presentation of ideas, which are a little hard to follow.
While the writing itself is nice, it does not help ease the reader into the
technical details. For example, the tools are presented in its technical form
almost immediately with insufficient exposition/intuition (e.g. where did
$\alpha$ and $\beta$ come from? why is there a need for instance-dependence?).
After the tools are presented, they are applied for a seemingly disconnected
collection of settings (multiclass, noisy, and pairwise rankings). 

I think it would make the paper muh more accessible if the authors devoted more
time to motivating the new ""fundamental tools"". Perhaps a simple example to
demonstrate how these new techniques are able to achieve more optimal bounds. In
fact, it might even be worth just focusing on one of the settings in the main
body of the paper (probably the noisy setting), and helping the reader develop
the intuition for how the stronger bounds were derived. The proof of Theorem 1,
which is nice and short, could for example be even moved to the main body and
given more exposition.

Perhaps some of the following questions could be helpful for revising the
presentation.

1. I didn't understand the story about the faster rates. The authors write
   ""recent work by Mao et. al. (2024a) shows that for all smoothed surrogate
   losses in binary classification, $\Gamma(\epsilon)$ behaves as
   $\sqrt{\epsilon}$ near zero"". And then, later, ""we will show that under
   certain noise conditions in classification, the behavior of $\Gamma$ can
   outperform the typical square-root dependence, approaching near-linear
   behavior"". What does this mean? Is this saying that the earlier result was
   wrong because it is possible to achieve linear rates? Or, is it saying that
   the earlier analysis is not always tight? This comparison sounds interesting,
   but I did not understand what the specific differences are. Is it under more
   benign noise that faster rates are obtained? What about the new tools enable
   a tighter analysis?

2. Are there intuition behind $\alpha$, $\beta$, and $\gamma$? They really seem
   to appear out of nowhere to make things work out.

3. Minor thing: what is ""approximation error"" (e.g. page 4, first new
   paragraph)? It is not defined. Perhaps it is
   $\mathcal{E}_{\ell}^*(\mathcal{H})-\mathcal{E}_{\ell}^*(\mathcal{H}_{\mathrm{all}})$?

4. Are there other takeaways that should be emphasized from the results in the
   multiclass, noisy, ranking settings? They seem like they are a nice series of
   results, but they are a bit inscrutable to me; perhaps the lack of
   presentation is doing these results a disservice.

Overall, this paper lists many results, but essentially no proof sketches. And
so, the results seem believable, but it is fairly difficult for someone not
working closely in the area to appreciate them. The problem seems important and
the stated contributions, if correct, are good. I was not able to closely look
at the appendix and did not verify the proofs (and I didn't know how to do
high-level sanity checks for these results). I think it is well-worth to give
some thought to making the ideas more accessible. Overall, the writing style is
good and the general motivation is stated well.

I chose 6-weak accept because the technical results seem like good contributions if correct. However, my confidence is 2: while I would like to verify the proofs, there is too much overhead because there's not really an entry point for someone who doesn't already work on H-consistency."
11,"## Summary
This paper investigates the effectiveness of using LoRA fine-tuning compared to full-parameter fine-tuning LLMs for adaptation to the healthcare domain. Authors report results for both smaller 7B Llama-2 as well as larger 70B Llama-2 fine-tuned models. They also compare against close-source state-of-the-art models like GPT-4 and Med-PaLM-2. The findings and described methods are a useful reference for healthcare machine learning practitioners who want to fine-tune a general-domain LLM for health-care tasks.

## Pros
* Evaluation is comprehensive across multiple datasets
* Evaluation is carefully done with decontamination pipeline
* Compares one of the most popular parameter efficient fine-tuning technique, LoRA, against full fine-tuning and state-of-the art models (GPT-4 and Med-PaLM) in healthcare domain
* Models are publicly released and available
* Datasets are publicly released and available
* Manuscript is well written and easy to follow

## Cons
* Methods section describes that LoRA may be applied to only attention layers vs. all layers. PE-FT results in Table 1 are for LoRA applied to all layers. It would be nice to also show the performance for LoRA applied to only attention layers since authors have mentioned this is a common approach. However, this is more of a ""nice to have""."
12,"This paper provides exact, closed-form analysis for the error dynamics of mini-batch SGD in the context of least squares regression, in the challenging multi-epoch scenario with random reshuffling. The authors’ approach is able to provide analytical expressions for both training and generalization errors, which are generally challenging to derive and hence are notable technical contributions. In particular, this work provides population risk results under random reshuffling, a non-trivial setting that adds complexity to the analysis.

That said, the paper lacks a clear high-level message and main takeaways. The results are heavily technical, primarily consisting of closed-form formulas for error dynamics and population risk, but it is unclear what we fundamentally learn from these formulas. While the authors attempt to offer interpretations in various remarks (e.g., Remarks 4, 5, 6 and corresponding appendices), these interpretations remain rather technical and do not effectively address key aspects such as convergence rates or any generalizable properties of mini-batch SGD without replacement.

Moreover, the analysis is restricted to least-squares linear regression, without discussing the potential extension or relevance of the findings to broader settings, such as convex and/or smooth optimization. This limited scope further weakens the significance of the results, as it remains uncertain whether the conclusions drawn here hold in (even slightly) more general settings.

The most critical issue, however, is the lack of discussion and comparison to prior foundational work in the stochastic optimization literature. The paper mostly cites sources from statistics and deep learning but omits directly relevant papers in optimization, which significantly undermines the significance of its contributions. These include, for example:

1. “Why random reshuffling beats stochastic gradient descent” by Gurbuzbalaban, Ozdaglar, and Parrilo (2015)
2. “Without-replacement sampling for stochastic gradient methods” by Shamir (NeurIPS 2016)
3. “Random shuffling beats SGD after finite epochs” by HaoChen and Sra (ICML 2019)
4. “SGD without Replacement: Sharper Rates for General Smooth Convex Functions” by Nagaraj, Netrapalli, and Jain (ICML 2019)
5. “Closing the convergence gap of SGD without replacement” by Rajput, Gupta, and Papailiopoulos (ICML 2020)

Proper citation and comparison with these works (that apply more generally to convex, smooth objectives rather than only to least-squares regression) are essential for positioning the contribution of this paper and to appreciate its significance.

Overall, while the technical results in this paper are solid and seemingly sound, they lack a compelling overarching narrative, are limited in scope, and most crucially---omit essential context within relevant prior work that is necessary for a comprehensive understanding of their significance. I recommend that the authors refine the framing to emphasize the implications of their results and incorporate a more thorough discussion of related literature, before this work is considered for acceptance."
13,"This work studies the performance of majorities of ERM/proper learners in multiclass classification. Such learning algorithms have been shown to achieve better errors in the binary case. This work extends these results to the multiclass case via the standard reduction of multiclass to the binary case. The reduction introduces a dependence on the so called graph dimension of the concept class. The second question addressed by this work is whether such combinations of proper learners can be used to learn PAC learn an arbitrary learnable multiclass class. Here the result is negative. Showing that the dependence on the graph dimension is unavoidable. The result builds on a known construction and improves it by a log factor.
A significant plus is that the paper is written in a clear and detailed way making it accessible even to those who do not follow the area closely.

Overall this is a solid set of results that skillfully combines recent interesting developments in this area and enhances the understanding of multiclass PAC learning. A weaker side to work is that the setting of (multiclass) PAC model is rather detached from the motivation of multiclass ML being common in modern ML mentioned in the introduction. But that is not the fault of this particular work.

Minor comments:
p5: it looks like the function defines Plurality and not Majority
p.13 typo: ""idenity"""
14,"The paper proposes to learn structured (group) sparsity in MTL, i.e., it learns sparse shared features among multiple tasks. They analyzed the results of group sparsity in both single-task and multi-task settings on two widely-used Multi-Task Learning (MTL) datasets: NYU-v2 and CelebAMask HQ. On both datasets, which consist of three different computer vision tasks each, multi-task models with approximately 70% sparsity outperform their dense equivalents. I have following concerns:

1. The paper doesn't clearly explain how their approach bring benefit over many other sparse-learning works like ( https://arxiv.org/pdf/1911.05034.pdf https://arxiv.org/pdf/1705.04886.pdf). The novelty of the work is limited. 

2. In addition, the motivation of the work in the introduction is not strongly enlisted (eg. organization of parameters in a CNN, CNNs can develop redundant filter). It will be important to explain why/how channel-wise l1/l2 penalty to the shared (CNN) layer parameters can help in solve complex computer vision tasks (contribution 1). 

3. The paper lacks any comparable sparsity-induced MTL baselines in evaluation which is required to show the effectiveness. I am also curious to know how the authors calculated mean inference time in figure 4.

4. How do the authors regulate between Loss1, Loss2, ... Loss N of the proposed architecture in Figure 2. Seems very difficult to tune."
15,"**Summary:** The paper introduces a differentiable lagrangian fluid simulator based on smooth particle hydrodynamics (SPH), written in JAX. The authors extend the SPH code in LagrangeBench to be differentiable, demonstrating some applications of the said approach. 

**Strengths:**

1. The paper seems well-motivated and addresses the need for differentiable Lagrangian CFD solvers.
2. Extending the differentiability of SPH solvers is a novel contribution with examples demonstrating the usability of the software.
3. Incorporation of various features such as Riemann SPH, Transport velocity, Wall boundaries, and Thermal diffusion. 

**Areas of Improvement:**

1. The work can significantly benefit from a performance analysis with other available open-source SPH solvers, which questions the suitability for HPC applications.
2. Unsucessful evaluation of the original purpose of ""Solver-in-the-loop"" (SitL), i.e., spatial coarsening, concerns the applicability of methods to other use cases.
3. There has been a significant discussion about using continuous vs discrete adjoints [ma2021, kidger2022, nadarajah2000], which can have significant performance impacts. Evaluating the solvers and implementing the said adjoints could strengthen the technical aspects.

Moreover, I would be interested in more details of what specifics were done to make the solver differentiable, which can serve as a roadmap for other scientists implementing such programs in niche scientific disciplines.

[ma2021] Ma, Yingbo, et al. ""A comparison of automatic differentiation and continuous sensitivity analysis for derivatives of differential equation solutions."" 2021 IEEE High Performance Extreme Computing Conference (HPEC). IEEE, 2021. 
 
[kidger2022] Kidger, Patrick. ""On neural differential equations."" arXiv preprint arXiv:2202.02435 (2022).  

[nadarajah2000]: Nadarajah, Siva, and Antony Jameson. ""A comparison of the continuous and discrete adjoint approach to automatic aerodynamic optimization."" 38th Aerospace sciences meeting and exhibit. 2000."
16,"The paper is well organized, and the topic is suitable for this workshop. Therefore, I prefer to accept this paper."
17,"1.it's better to do experiments to give a proof for your methods
2.li's better to make comparison with sota methods."
18,"**Overview**

This paper proposes an efficient and theoretically motivated approach to simplify the technique of disentangling causal representations. 

**Strength**
* The key idea of approximating the difference of conditional probabilities with models' generalization abilities is intuitively reasonable and further carefully approved with corner case well discussed. 
* The empirical evaluation is solid as the experiments follow the most standard work, and the efficiency improvement of the proposed method is very pronounced.


**Question**
* Does the paper offer insights into the approximation error associated with using generalization abilities as a surrogate for the actual difference in conditional probabilities? Is there any provided intuition or established bounds?"
19,"The paper is exceptionally well-written, with only one minor typo detected. It provides a comprehensive explanation of the entire pipeline, enabling easy replication of the results. The paper is thorough, presenting a strong motivation for data-aware fine-tuning. The authors include enough quantitative and qualitative examples to demonstrate the performance of their methods on the validation data.

I have only one (optional) suggestion, that would contribute to the completeness of the manuscript. In Tables 4 and 5, the number of FLOPs and CO2 equivalents are missing and denoted as ""not tracked"". However, it is also possible to estimate these values from either a single forward pass (FLOPs) or from running one epoch of training (CO2 eq) and then extrapolate the CO2 for the whole training. 

Overall, I recommend the acceptance of this manuscript as it raises no major or minor concerns.

Minor comments:

Typo on page 3: ""on all available"" should be corrected to ""on all available data."""
20,"The work proposed an interesting work on using MCG signals to conduct individual identification. While several more steps needs to be considered as a foundation model

1.  ""Our system (shown in Appendices Figure ??) has two BellBloom OPMs as a gradiometer to sense the cardiac magnetic
field."" There is a reference issue.

2. More related work about MCG signal can be introduced to give readers a more comprehensive view of MCG.

3. In the introduction, the authors state that ECG quality can be influenced by physical activities etc, will MCG will also be influenced by those factors?

4. The most concerning issue is the model is tested on 5 subjects, it can hardly be called Foundation model."
21,"### Summary : 
This work suggests weakly correlated weights in the neural network Gaussian Process model enable learning useful sparse representations. Empirical evaluation demonstrates that weak correlations (via an intermediate convolutional layer) induce low-dimensional kernel Gram matrix in the sparse regime. Theoretical justification is also provided for the same observation which in turn predicts good generalization behaviour. Informally sparsity dissimilates the neural representations across layers while correlations counter this effect. For any sparsity level f, the authors derive an optimal correlation level that balances the tradeoff in generalization."
22,"# Summary

This paper explores the potential of artificial intelligence to replicate or surpass human creativity. It identifies advantages machines possess, such as unbounded effort, lack of emotional bias, and detachment from competition, and contrasts these with uniquely human traits like embodied intelligence, collaboration, and intrinsic motivation. The paper argues that instead of merely mimicking human creativity, AI should be designed to exploit its unique strengths for innovation rather than replication. The paper also briefly discusses ethical considerations, including the implications of machine creativity on ownership and societal roles.

# Strengths

The paper offers a compelling conceptual framework for understanding machine creativity through the lens of both cognitive science and computational advantages. It effectively highlights the strengths of machines in areas where humans are limited, such as scalability and freedom from emotional interference. The discussion around leveraging these machine-specific traits is technically grounded, particularly in ideas like memory graph representations for creative insights and reward function optimization for analogical reasoning. Additionally, its critique of human limitations, such as biases introduced by competition or self-doubt, is well-argued and relevant to AI system design.

# Weaknesses

While the paper presents a strong theoretical narrative, it lacks empirical evidence to validate its claims. The proposed ideas, such as “artificial dreams” or AI societies of mind, remain speculative without detailed implementation strategies. The discussion on creativity metrics is insufficient, failing to address how “Big-C” creativity (paradigm-shifting innovations) could be systematically identified or evaluated in machines. Furthermore, the ethical implications, while noted, are underdeveloped, leaving key questions of accountability and societal impact unresolved.

Overall, the paper provides valuable insights and raises thought-provoking questions about AI creativity. However, its theoretical nature and limited attention to practical methodologies and metrics reduce its overall technical contribution. Addressing these gaps could significantly enhance its impact."
23,"This paper studies the problem of Differentially Private Stochastic Convex Optimization. This is a well-studied problem for which optimal excess population risk rates have been known for 5 years. Its main contribution is in reducing the number of adaptive steps of the algorithm --called batch gradient complexity in the submission-- (and relatedly increasing batch sizes) for a single pass algorithm. Moreover, their privacy analysis does not directly rely on convexity, which makes it suitable for its use in nonconvex learning problems.

The key insights of this algorithm are in the use of the Nesterov acceleration in conjunction with (stochastic) variance reduction. These two ideas have not been systematically studied in this context (at least, not together) which already makes the analyses of the paper interesting. The paper also provides an improved  rate for the batch gradient complexity (of order $n^{1/4}$) under the additional assumption that the optimal solution is interior, which is based on a novel sensitivity analysis tied to the reduction of the norm of the gradient in this case.

I believe this paper contributes significantly on the theoretical and conceptual side. While the rates do not appear to be novel, there are good reasons to be interested in reducing the number of adaptive steps (e.g., it potentially reduces lags in distributed environments, and mitigates the impact of composition for privacy guarantees). I also believe that some of the techniques introduced in this work may be of independent interest. On the downside, I believe some claims in the paper are not very clear, and I would like to see a final version that is more careful in this respect.  

Minor comments:
1. The submission claims to obtain the optimal rates for DP-SCO, but the analysis introduces polylog(n) factors which are not present in the lower bounds. Moreover, the established rates hold with high probability, so these factors may be unavoidable. Can the authors clarify this? 
2. Still on this issue of polylogs, it is claimed in page 2 that ""even without assuming $x^+\in{\cal C}$ our work improves upon all prior works (at least by $polylog(n)$""). Is this claim in terms of the gradient complexity of excess risk. For the latter, it is very unclear to me that this is the case, for the reasons stated in the previous point.
3. In the same vein, I was expecting a high probability bound in the final result (theorem 10), but the excess risk is only stated in expectation.
4. In the context of DP-SCO, algorithms based on the stochastic Frank-Wolfe method use the stochastic recursive gradient estimators (Asi et al.2021a, Bassily et al. 2021). While the current submission uses this idea in a different context (stochastic acceleration), I believe this is a relevant comparison which is currently missing. 
5. The submission mentions that ""To the best of our knowledge, a direct potential-based analysis of stochastic accelerated gradient descent has not appeared in the literature before besider (Taylor and Bach,2019)."" On the one hand, such analyses exist (e.g., Cohen, Diakonikolas, Orecchia http://proceedings.mlr.press/v80/cohen18a/cohen18a.pdf). On the other hand, the analysis of Taylor and Bach is specific to randomization by subsampling, which is orthogonal to the stochastic i.i.d. analysis carried out in the submission."
24,"# Peer Review for the Manuscript: ""Evaluating Large Language Models for Race-Based Medical Content""

## General Evaluation

The paper presents an interesting study on the use of Large Language Models (LLMs) for identifying and evaluating race-based content in medical contexts. This topic is both timely and relevant, given the increasing reliance on LLMs across various sectors, including healthcare. The authors have made a commendable effort to highlight the challenges and nuances associated with race-based content in LLM outputs.

## Specific Feedback

### Strengths

1. **Relevance and Novelty:** The study addresses a critical gap in the current understanding and evaluation of LLMs in handling sensitive and crucial topics such as impact of racial stereotypes in medical advice.
2. **Methodological Approach:** The structured comparison across different LLMs provide a solid basis for the study's findings.

### Areas for Improvement

1. **Typos and Clarifications:** The paper mentions ""nine unique LLM-prompt combinations"" which should correctly be ""twelve unique combinations."" Attention to such details is crucial for the accuracy of the paper.
2. **Consideration of Skin Tone Variability:** The use of a more comprehensive skin tone classification, such as the Monk Skin Tone Scale ([Monk Skin Tone Scale](https://skintone.google/)), would enrich the study by providing a more nuanced understanding of race as it pertains to medical content. The current set of 13 prompts is limited, it is predominately representing black, white and some asian race. Recommend the authors to formulate a more diverse prompts by considering more examples of race-stereotype combinations.
3. **Benchmark Evaluation:** The paper states that there is a lack of methods to evaluate harmful content regarding race. This is not true, major players in Generative AI space like OpenAI, Meta, google all of them have released trust and safety scorecards and responsible AI covering bias and stereotypes is a big focus. However, existing benchmarks and datasets could be explored for their representation of race-related medical data. The absence of this exploration is a missed opportunity to contextualize the study's findings within the broader research landscape.
4. **Physician Backgrounds:** The background of physicians involved in the original research, particularly their awareness of bias and civil rights, is not detailed. This information is crucial for understanding the potential biases in the study's setup and interpretation.
5. **Statistical Measures:** A clearer explanation of the statistical measures used (Sensitivity, Specificity, NPV, PPV, F1) would make the paper accessible to a broader audience, including those not familiar with these terms.
6. **Methodology Suggestion:** Given the limitations of zero-shot prompting in niche domains like race-related medical data, exploring few-shot prompting or fine-tuning the models might yield more accurate results.

### Recommendations for Further Research

The authors are encouraged to explore the representation of race-related bias in publicly available datasets and benchmarks, such as those hosted on platforms like Hugging Face and Stanford's CRFM. Investigating these resources could provide insights into the current state of race representation in LLM training data and benchmarks. Furthermore, the authors should consider building and open-sourcing a dataset specifically for evaluating race-related content in medical advice. This contribution would significantly benefit the research community by providing a specialized resource for further studies.

## Some benchmarks for reference
- [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [Stanford CRFM HELM Lite](https://crfm.stanford.edu/helm/lite/latest/)
- [Hugging Face Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- [Artificial Analysis](https://artificialanalysis.ai/)
- [Martian Leaderboard](https://leaderboard.withmartian.com/)
- [Hugging Face Enterprise Scenarios Leaderboard](https://huggingface.co/spaces/PatronusAI/enterprise_scenarios_leaderboard)

## Conclusion

The paper ""Evaluating Large Language Models for Race-Based Medical Content"" contributes important insights into the evaluation of LLMs for sensitive content. With the recommended revisions and further exploration of the highlighted areas, this paper has the potential to significantly impact the field."
25,"The paper proposes a framework to perform complex segmentation tasks when instructed in language queries. Direct prompting or direct inference for several segmation models suffer under complex language instruction where the model has to perform grounded reasoning and identify relative attributes of objects with each other.
Deictic prompting proposes to use LLMs for generating reasoning programs which can then be grounded in the image by using scene graphs. With this the segmentation model can focus only on target areas.
Empirically the authors observe dramatically high improvement in performance as compared to baselines.

I have one weakness which is to improve readability of the paper by adding more examples and description of forward reasoning."
26,"**Summary**
The paper proposes a large language model-based foundation model for medical code prediction tasks. The proposed model is based on LLaMA-2-7B, with further fine-tuning on (1) the medical code and text definition pretext task; (2) the medical code prediction task. The model is tested on MIMIC-III and MIMIC-IV, with a comparison to RNN/CNN-based models and graph-based models.

**Strengths**
- Clear motivation for the ""concept memorization"" training and ""hierarchical contrastive learning"".
- I appreciate that the authors include a task formulation section, for readers to better understand the task and data format.
  - This part can be further improved if authors can clarify whether ``diagnosis prediction`` is a multi-label classification
- The technical part is easy to follow

**Weaknesses**
- The input sequence perturbation makes sense to me. However, authors do not explicitly mention whether the within-visit order of target visit matters or not. 
  - During training, will the sequence perturbation also apply to the target visit?
  - During evaluation, how the accuracy/precision/recall are calculated? (If target code order is ``996.74, 428.0, 414.1``, and model outputs ``414.1, 428.0, 996.74``)
- Authors mention that transformer-based LMs are widely used in the same task (Paragraph#3 in the Introduction Section). Is there a reason transformers are not included as compared baselines?

**Misc.**
- Missing reference in Paragraph#1 in the Introduction Section. 
  > 13,000 disease candidates in ICD-9 ``(?)``

Overall, the quality of this paper is great. I would be very interested in seeing authors' responses regarding my concerns."
27,"The authors introduced a novel framework that takes advantage of Tucker decomposition for polynomial networks to learn solutions of ODEs.

Major comments:
- Important references in the literature are currently missing, for example -- Kossaifi, Jean, Nikola Kovachki, Kamyar Azizzadenesheli, and Anima Anandkumar. ""Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs."" arXiv preprint arXiv:2310.00120 (2023). There, the authors already demonstrated the usage of Tucker decomposition for representation of network weights in neural operators. For the current manuscript, I feel like the usage of Tucker is not well motivated and the experiments are very low-dimensional and therefore not very convincing.
- It is very hard to see the uplift of TuckerNet in the ODE learning case study. Is RMSE really an effective measure? There are always some events that are better captured by other two networks and some other ones are better captured by TucketNet. I suggest authors either provide more metrics, or doing more analysis on these signals -- for example, does it make sense to look at the phase matching? See whether certain events are shifted away? There are 2 main factors in these signals --- amplitude and ""peak time"". I am afraid RMSE cannot speak for both, especially when the RMSE values for these three networks are very close.
- There are some strong claims in the manuscript and I do not think they are substantiated by experiments or references, e.g. ""Tucker formulated polynomials could be less likely to get stuck on worst local minima.""

Minor comments:
- What are ""parameter-constrained"" models?
- Several typos, e.g. ""the we"", ""CDP"""
28,"The reviewed work investigates the use of gradient boosting for improving the training of physics-informed neural networks. Instead of training a single neural network to fit the solutions, the authors propose using a sequence of neural networks, each of which trained on the residual of the approximation based on the previous networks. 

The authors find that this approach can significantly improve the accuracy of the resulting approximate solution on problems with multi-scale structure.

The paper is well-written and appears to achieve significant improvements in the accuracy of PINN training on multiscale problems - one of their current weaknesses. I therefore recommend acceptance."
29,"Summary

This paper considers the universal multiclass online learning under bandit feedback setting. This is a sequential setting where in each round, a learner receives an instance and subsequently chooses a label from a potentially unbounded number of labels, and subsequently the learner sees whether or not the chosen label was the correct label. There are two versions of this setting: the realisable setting, where there is a concept from a concept class that perfectly predicts labels given the instances, and the agnostic setting, where such a concept does not exist. The goal in the realisable setting is to design learning algorithms that make a finite number of mistakes as the number of rounds goes to infinity. The goal in the agnostic setting is to have small regret relative to the best concept in the class. The authors show that if the concept class has a finite littlestone tree, then both in the realisable and agnostic settings then the concept class is learnable. 

Strengths and weaknesses. 

Overly simplified, in the realisable setting, the question is as follows: given enough interaction with the environment, can we identify the concept that generates the true labels? Surprisingly, this work answers this question by showing that if the concept class has a finite littlestone tree, that this is indeed the case. The strategy to show that this is the case is remarkably straightforward, which I consider a plus. 

As written by the authors, this shows a stark contrast between multiclass online learning in the uniform framework and in the universal learning framework: in the uniform framework the concept class that is learnable even in the full information setting is considerably smaller. Even though the question that is asked in the uniform framework is considerably harder by definition I think that the difference as shown in this paper is very interesting. Specifically, for me this is the clearest demonstration that I have read about the differences between the uniform and the universal framework. 

The main downside about this work is how the paper is structured. Even though the results are introduced early on in the paper, the definition and goal of the setting are only given on page 9. This for me makes the paper unnecessarily hard to read, as I had to first wrestle through the introduction without really knowing what the setting was. Instead, if the setting was introduced early on in the paper, it would have been easier for me to parse the paper. It would have also been nice to state the goal in the uniform framework more clearly, as this would have made it easier for me to understand why there is such a big difference in what is learnable for the two settings."
30,"This work proposed an interesting framework for evaluating clinical LLMs. I personally would feel this is a more relevant evaluation framework for real world use cases than traditional benchmarks such as MedQA. The study demonstrated promising capabilities of the framework and revealed limitation of current LLMs in clinical reasoning (at least in the setting without domain-specific in-context learning). However, understandably due to the context length limitation of the non-traditional track, several important technical details are unclear to me. For example, what are the foundation models used in Grader-AI agent, and what's the outcome of expert evaluation?"
31,"This paper aims to mix up the Neural Collapse features to enhance the utility of the privacy-preserving data. The research topic of privacy preserving is crucial for the ML community. The main idea underlying NeuroMixGDP is instead of mixuping input features, the mixture of output features should make more sense, as the former may suffer from the broken 'Non-Approximate Collinearity (NAC)' condition. Their preliminary experiments verify NC indeed induces NAC and provide empirical evidence to support the main claim of this paper. Two technical contributions, Averaging Mixup and Hierarchical Sampling, were proposed to significantly boost the performance of NeuroMixGDP. Experimental results also verify the effectiveness of NeuroMixGDP.

Strengthness:

(1) The paper is well-motivated and supported by results. 

(2) I like the preliminary results provided in Figure 1 and Figure 2, which provides clear evidence and important insights to readers.

(3) The authors also provide results of Model Inversion Attack, making the paper more solid.

Weakness:

(1) Can the authors elaborate more the experimental settings of Figure 1 and Figure 2. I would like to see the similar trend on various settings and therefore, the finding in this paper is a general one that can be observed across various settings. 

(2) Are we solely extracting the output features of the last layer or all layers? I expect only the features from last layer are helpful here.

(3) What does epsilon in Figure 2 stand for? 

(4) I encourage the authors to explain the Poisson Sampling in the early part of the paper. 

(5) what is the formulation or definition of sensitivity mean here? what its relationship to DP noise?"
32,"### Summary

This paper proposes a foundational model for sleep-related tasks and shows its efficacy through two downstream tasks - (i) sleep apnea detection, and (ii) sleep stage detection. While it proposes a novel application of two interesting pre-training techniques, the experimentation does not truly evaluate the potential of the proposed foundational model. Overall, the paper is built on a promising idea and its presentation can be further improved through more rigorous experimentation.

### Strengths
- Novel application of pretraining tasks to align multiple modalities to create a sleep (time-series) foundational model
- Paper is written with clarity, explaining proposed methods and experiments well
- Promising results

### Weaknesses
- “Foundational Model” capabilities have not been evaluated: To claim that a new model is a foundational model, it must satisfy certain properties of foundational models (FM). It is the authors’ responsibility to then highlight the properties that have been tested and note limitations of the foundational model. For example, a common property found in foundational models is the ability to generalize reasonably well to an unseen dataset. 
- Weak baselines: Ideally, the model should be compared with other methods which have been trained for the proposed downstream tasks. Eg: [1] is a sleep apnea detection method which can be included as a baseline.

### Other Feedback (to extend this work):
- The authors should motivate whether a foundational model for sleep is in fact useful. Identifying the shortcomings of existing methods (which do not consider all modalities, for example) and clearly delineating the research questions that you would like to answer would be a good way to approach this.
- Authors considered separate encoders for the different modalities. Some recent papers on time-series foundational modeling have shown that a single time-series model can encode time-series of different #channels and frequencies [2, 3]. Consider using one of these methods as the base model for your proposed pre-training tasks. This could help better align the modalities, especially if one of the modalities is more sparse than others.
- Experiment on the effect of leaving out one modality - could motivate the need to jointly model multiple modalities.

[1] https://ieeexplore.ieee.org/abstract/document/8571271
[2] https://arxiv.org/pdf/2302.11939.pdf 
[3] https://arxiv.org/abs/2402.03885"
33,"The authors experimented with using LLM to detect whether LLM responses contain debunked race-based content. It's an important topic to explore and the results help us to better understand how current available LLMs may respond to race-based medical questions. 

13 questions were used to generate the responses from LLMs. It's unclear whether the set of the questions are representative enough for the study. 11 out of the 13 questions contained direct mentions of races in the question themselves, while 2 questions were fairly general. There is no comparison between the two to see if there is significant difference. Since direct mentions of the races may be more likely to generate race-based responses, the result responses set may have significant higher race-based responses compared to real life clinical uses cases, which may bias the evaluation results. 

The paper also didn't discuss on how the safe guard mechanism in proprietary and/or open source models may affect the model responses. In appendix, the author mentioned that MedPalm-2 simply rejects to answer some of the questions due to the content filter. This type of content filtering is a common practice and it may change constantly without any notice, especially for proprietary models, which puts a lot of uncertainty on the evaluation results."
34,"The paper emphasizes the potential of LLMs in extracting insights from unstructured medical data for survival analysis and risk estimation, proposing a comprehensive approach that incorporates embedding, fine-tuning, and prompting strategies. It also highlights the importance of cross-validation techniques in assessing the generalizability of these methods across different medical institutions.
The graphs seems not well orgnized, could you please double theck the graphs. Besides, the recommendation section seems fragmented, could it be reorganized to summarize it in the conclusion?"
35,"This paper explores the use and adaptation of large-scale language modeling (LLM) to the clinical domain. It builds on previous work and provides an in-depth analysis of existing clinical LLMs, focusing on domain adaptation approaches. The study compares various models according to their medical knowledge infusion strategies, including domain-specific adaptation and pre-training from scratch using a medical corpus. The study emphasizes the effectiveness of continuous pre-training and supervised fine-tuning in improving model performance on MedQA and PubMedQA medical benchmarks. The discussion raises important points about the selection of training data, the potential for retrieval enhancement generation, and the need for careful consideration of copyright issues. The discussion also touches on the utility of clinical LLM in downstream applications and the challenges that remain in bridging the gap between clinical needs and academic research.
While there are areas for further research and clarification, the study stimulates meaningful discussion on the potential and limitations of clinical LLMs, paving the way for future advancements."
36,"This paper introduces the EMT framework, an approach designed to assess the phenomenon of catastrophic forgetting resulting from the fine-tuning of multimodal large language models (MLLMs). The underlying concept involves treating MLLMs as image classifiers and subjecting them to varying degrees of fine-tuning.
The central finding of this study is the observation that moderate fine-tuning on one dataset confers benefits to datasets not utilized during the fine-tuning process, while excessive fine-tuning leads to catastrophic forgetting. This conclusion aligns seamlessly with logical expectations. When one heavily fine-tunes a model on a specific dataset, the anticipated outcome is an exceptionally high level of accuracy on that specific dataset. There exists no inherent mechanism within the fine-tuning process to incentivize the model to retain optimal performance on non-fine-tuned datasets.
In essence, it begs the question: Why should we anticipate any different outcome?

A few comments and questions for authors:
1. Regarding Section 4.2, titled ""Reasons for the Performance Degradation,"" it appears that this section delves into an investigation and speculation of the factors contributing to failed predictions. However, it lacks evidence establishing a causal link between these identified reasons and the observed decline in model performance. To enhance clarity, I suggest renaming this section to something like ""Analyzing Failure Modes of MLLMs.""
2. In my humble opinion, fine-tuning models with billions of parameters (e.g., 7B and 13B) on datasets such as CIFAR10 and MNIST may raise concerns about the reliability of the experimental setup. 
3. Have the authors explored techniques rooted in gradual unfreezing of layers, such as ULMFiT? These techniques are specifically designed to mitigate the issue of catastrophic forgetting during fine-tuning on downstream datasets. It would be insightful to know whether these methods were considered in the experimentation process.
4. In Section 3.1, would it be advantageous to employ two distinct classification heads for subsets of datasets with non-overlapping classes? The current approach, which adjusts the weights of classes from the pre-trained dataset based on their presence in fine-tuning datasets, may inadvertently encourage the classifier head to prioritize classes currently present in the dataset. This approach lacks a mechanism for preserving features learned for classes from the pre-trained dataset.
5. Could the authors provide clarification on the meaning of ""less potent than LLaMa""?
6. The authors mention the use of the OpenAI-API for evaluating results. It is worth noting that this choice may present issues related to the reproducibility of the paper's findings over time, and I would strongly suggest employing an additional (reproducible) method for evaluation purposes.
7. In Section 3.1, did the authors experiment with restarting the learning rate when transitioning from the pre-trained dataset to the fine-tuned dataset? Such a strategy can impact the model's adaptation to new data, and it would be interesting to know if it was explored in the study."
37,"This is a review of the manuscript, ""Applications of Fourier neural operators in the IFMIF-DONES accelerator,"" submitted to the ICLR 2024 Workshop on AI4DifferentialEquations In Science. The paper describes a Fourier neural operator (FNO) trained to predict the envelope parameters of neutron beams in the IFMIF-DONES facility. Further, a deep reinforcement learning (DRL) model is trained for optimization and/or control purposes. My review is summarized below in point form.

1. The FNO provides a nice speed-up, but it is also (presumably) trained on $\{s_x, s_y\}$ data as a function of $z$. Are $\{s_x, s_y\}$ data actually available as a (near-)continuous function of $z$? Or are individual particle trajectories somehow measured? What is the expected fidelity of such data, and how does the FNO's performance deteriorate in the context of realistic uncertainties? For instance, does parameterizing the beam envelope in terms of the standard deviation of the particle distribution lead to potential model errors? The authors should provide a more comprehensive phenomenological description of the physical system. They should answer such questions, and better motivate their chosen surrogate model form. Doing so will help situate readers from other fields.

2. Did the authors consider alternative operator network architectures? In many cases, a simple multilayer perceptron (MLP) will suffice. Especially in the present context, which features low-dimensional input and output spaces and a limited number of control parameters. A simpler network might perform equally well! The authors should test an MLP of similar depth, having the same number of trainable parameters as their FNOs, to assess whether the FNO configuration is beneficial, per se.

3. The authors use stochastic gradient descent to optimize $\{s_x(0), s_y(0), k_1, k_2, k_3\}$ to achieve a target output envelope, $\{s_x(L), s_y(L)\}$. As presented, this seems like a very simple optimization problem that could be solved using vanilla gradient descent. Why is it necessary to use the stochastic version?

4. Related to the first and third points, the need for a DRL controller is unclear. Is the controller required for real-time applications? And if $\{s_x, s_y\}$ data are available as a function of $z$ to train the model (i.e., in the absence of ``comprehensive knowledge regarding the [governing equations]''), why does the controller only have access to the data at $z = L$? Wouldn't it be feasible to reduce the number of requisite actions by feeding the controller a richer observation?

5. The authors should comment on the performance of their FNO outside the training envelope. (It can be difficult to operationalize the notion of extrapolation in many deep learning settings, of course, but the present demonstration is relatively low-dimensional.)"
38,"This paper studies the problem on subspace clustering. The main emphasis of the work are (1) hyperplane clustering, (2) using cost functions that enable robustness to outlier data, namely, L1 and Huber and their variations, (3) providing theoretical guarantees on various notions of the optimality of estimated hyperplanes ---  under strong conditions on data points and outliers.  The experiments in the main text include numerical results on small-scale synthetic data and CIFAR-10 features.

####################################################################
My evaluation:
This paper introduces interesting theoretical results that combine GPCA formulation of hyperspheres with robust L1 clustering cost (inspired by DPCP) and prove certain optimalities for estimated hyperspaces under strong geometric conditions. However, this does not qualify to be accepted due to the following issues: (a) The motivation of using ""only"" hyperspaces is not fully explained. What happens when we have subspaces of codimensions >2? (b) The writing the paper is poor : unscientific word and claims, the structure, explanation of the assumptions 5.1 is insufficient, ...  (c) The experiments includes small-scale data. What happens when we have no outliers or we have different variances for random additive noises? You can should move some of the ""no-outlier"" results in the appendix to the main text and investigate how this method compares in estimating the correct subspace with other methods (in the no-outlier scenario).

P.S. I assume that the proposed Theorems are correct as I did not read proofs in the Appendix.
####################################################################
I will summarize my main questions, comments, and suggestions as follows:

(1) Regarding the ""Geometric Quantities"": Suppose datapoints belong to subspace of codimension 2 (a subspace of the supposed hyperplane). Then, it is easy to show that c_{in, k , min} is 0 for all k \in [K]. In this case, we are not guaranteed to have {b^{*}_k} as coordinate-wise minimizers of (GPCA-l1) --- according to Theorem 4.3. The authors should explain what happens in this situation --- which is quite common in practice.

(2) Related to my comment (1), the proposed method formalizes the problem of subspace clustering using orthogonal complement representation of the hyperspaces, that is, vectors { b_k }. Therefore, once these vectors estimated, the algorithm returns the estimated hyperspaces. One can easily think of a situation where we want to estimate high-dimensional subspaces with codimensions > 1. What would be the appropriate modification of this algorithm to operate on those cases? One may think that you have to resort to estimating one-dimension at a time in the orthogonal complement space --- which would be greedy. It is important to clearly explain your approach from this viewpoint so that the reader can easily compare this algorithm with other methods.

(3) Assumption 5.1 might be standard but not realistic at all. Either the authors should provide their insight on why (or on which class of point sets) this assumption is true or clearly explain that this assumption puts a strong set of constraints on both inlier and outlier point sets -- which maybe even impossible to satisfy. Regarding (A3), please either prove the claim that for random points the eigenvectors of the weighted sum are distinct (with probability 1) or cite a reference.  

(4) (page 2, line 43) 	“KH-DPCP [27] integrates DPCP into the K-Hyperplanes framework. “ + (page 3, line 102) “Second, the underlying theory of why such an integration works well has thus far remained, to our knowledge, obscure. “  The algorithm KH-DPCP is not discussed in the main text. Given the fact that its objective is the most comparable to this problem  (robust hyperplane clustering), it is beneficial to dedicate a detailed remark on this algorithm.

In what follows, I give comments on issues that appear frequently in the paper (not limited to these examples).

(5) The specific choice of notations may seem irrelevant. But from the perspective of readers, authors should try to use “intuitive” notations. Two examples (among many): (a) In equation (GPCA-l2), indices in summation goes from j=1 to N. why not n=1 to N? (b) The definition of w^{t}_{j,k} in Algorithm 1: HARD-l is ""similar"" to that of d^{k}_j in subsection 4.1. (c) A good notation can simplify Equation (2) and make it more intuitive, namely, I - bb^{T} is an orthogonal projection matrix ...

(6) (page 1, line 24) “However, for subspaces of high relative dimensions (relative to D), sparsity and low-rankness break down, and so do these methods.” The word relative is repetitive. The sentence is broken. 

(7) (page 1, line 36)  “It is very simple and intuitive, but it is inaccurate and not robust to outliers, and it has limited theoretical guarantees (e.g., of convergence to true hyperplanes). “  Please do not use words that do not convey useful information, like “very”. Please also do not construct run on sentences ( … but … and  … and  … ). This hinders the readability of your paper. 

(8) (page 2, line 48) 	“we blend the GPCA and DPCP philosophies”  I’m not sure if “philosophies” is an appropriate word here. 

(9) (page 2, line 44) “In doing so, it inherits the one-shot ability of K-Hyperplanes and the robustness of DPCP to a certain extent. But it also compromises accuracy and comes with no theoretical guarantees”  
“to a certain extent” does not make sense to me. Could you please explain this algorithm, and compare it with your proposed algorithms? 

(10) (page 3, line 92) “The first idea of [27] has some  … ” Please do not use a numerical reference as an object in a sentence. You can use the authors’ names instead.

(11) (page 4, line 127) “How can we solve the harder problem (GPCA-l1) efficiently?“ I am not sure in what sense the problem is hard. Maybe instead you can emphasize on the fact that the l1 objective is nonsmooth?

(12) (page 4, line 132) Whether the true normal vectors are a global minimizer of (GPCA-l1)? I do not understand the meaning of this sentence. 

(13) (page 7, line 229) ""Intuitively, (A2) is more likely than (A1) to have a unique global minimizer because h is further locally quadratic (strongly convex)."" The term likely implies the existence of an underlying probability distribution. If there in none, this is an incorrect statement."
39,"This paper introduces an encephalitis query-document dataset along with an embedding model used for retrieval. The experimental results demonstrate the superiority of the embedding based model over traditional key-based search engines. 

**Pros:**

(1) The dataset and the model are valuable to the community. 

(2) The code is clear and easy to read. 

**Cons:**

(1) The experimental results only present some cases, instead of the performance on the whole dataset. It would be better to list a series of numbers in a table. For example, for the baselines, choose keyword-based search engines, as well as OpenAI’s text-embedding APIs. The metrics can be recall, etc. Also, the authors can explore whether the cheaper model developed by users (like the one proposed in this paper) can be on par with the OpenAI text-embedding API models. 

(2) The layout of this paper can be further improved. For instance, move the “Links” parts as the footnotes. For the case study paragraphs in the appendix, it would be better to wrap them with a blockquote. It would be more convenient to use latex templates to modify these things, compared with Word.

(3) The title is too long and can be shorten, such as “Enhancing Encephalitis Research Retrieval: Leveraging GPT-4 for Semantic Query-Document Alignment Beyond Keywords.”"
40,"Despite the success of GNN simulators, there are still significant challenges in accuracy and rollout error accumulation which prevent them from being employed more widely, so it's great to see papers trying to improve these.
This paper presents explores three options to improve accuracy, in particular in density of SPH simulation. Overall, I feel the solutions are a bit complex for what they do, and require quite a bit of tuning (finding alpha, beta coefficients, and the dataset-dependent kernel turning). But for workshop, such explorations seem adequate, and the paper does show improvements on an external benchmark which is good to see.

One reservation I have is on the GNS vs GNS-g results, which show a surprisingly large effect. I'm somehow very skeptical of this result; in the case of constant gravity as in Dambreak, the only difference between these methods is a constant bias factor, which should be trivial for a NN to learn.
If this result holds, this is a very interesting find and it would make sense to really drill down and investigate to _why_ this is; which could make a very interesting read as a paper. It however seems more likely that this is masking an issue with the baseline implementation; maybe related to output normalization or something similar. To that point, GNS in Fig.1 also seems very inaccurate compared to similar dambreak simulations shown in the GNS paper. So I'd encourage the authors to double check their baseline against reference implementations.

Smaller comments:
- Is the convolution in Sec. (D) really necessary? This seems quite complicated, what happens if you simply add gm(x), using the particles' current position? 
- I think eq (6) ends up being correct, but the reasoning is very weird. Really it's just a unit translation, GNS defines its timestep as 1, so you'll need to translate g to this system. This has nothing to do with the number of SPH substeps or the type of integration.
- Instead of SPH steps with zero velocity assumption, have you considered learning a relaxation kernel, or simply adding a density constraint as a loss term in the GNN?"
41,"This paper addresses uncertainty in PINNs through two main approaches:

1. conformalize the surrogate solution of the logistic growth ODE, ensuring that the empirical coverage aligns with theoretical expectations.
2. conformalize the inferred parameters, utilizing parameter estimates from multiple PINNs to establish valid coverage intervals for new datasets, and for unseen values of β.

Suggestions for Improvement:
1. Experimentation with more complex ODE/PDE models could enhance the method's validation.
2. The current study primarily relies on empirical evidence, lacking strong theoretical underpinnings."
42,"**Summary**:  
The paper proposes a self-expressive model for clustering data drawn from a union of nonlinear manifolds, extending methods working in the linear regime. 

**Pros**:
- The paper is, in general, easy to follow and clearly written. 
- The proposed method is a natural extension of a similar idea introduced for linear manifolds, and, within the provided in the paper empirical evaluation, is effective.


**Cons**:   
- The method seems to be grounded in intuition from linear case. In consequence, I am not sure whether it is theoretically guaranteed to work   
- The provided results could be complemented by a comment on the stability of the methods, as well as standard deviations.   
- The text ends abruptly   

(See the “Details” for more comments/reasoning behind the above pros and cons)

**Details**:

The paper is of good quality and clarity. I also consider the idea of extending the linear approach of self-expressive models to a nonlinear case by progressively linearizing the representations interesting. However, apart from the provided intuition given by the authors on page 2 I do not understand (or see) whether there any (even simplified) theoretical guarantees guiding this approach (not that I necessarily need to see ones, but would appreciate a comment on their existence in the text). I appreciate the provided evaluation with other approaches. However,I would like to have the blank spaces in Table 2 clarified (i.e. why some methods were not evaluated on some data). Additionally, I would also like to see a comment on the stability of the methods, preferably by adding information about deviation from the results reported in the Tables in the text. Finally, In general, I believe the paper is easy to follow and clearly written, however, it ends abruptly, without any remarks on the closing conclusions. I would like to see those issues addressed. Beyond the points above I do not see any major flaws in the paper."
43,This paper first demonstrated that prompt engineering can outperform fine-tuning in medical QA for open-source models. I really like this work and definitely think it's important as open-source models are more accessible for clinicians and have less privacy issues. Overall I think this is a good paper presenting important conclusions. My only concern is that this paper heavily relies on public benchmark and lacks the evaluation from the clinician.
44,"This study introduces a weighted loss function based on a gradient graph to improve fairness without needing demographic data. Theoretical results provide the intuition of the method, explaining how the correlation in gradients is related to unfairness. Empirical results show the effectiveness of proposed methods.

Strengths
- Theoretical derivation of method illustrates how it works well
- Empirically, the proposed method yields superior results to other methods"
45,"This paper provides an overview of GatorTron's use in existing literature. While the authors raise concerns about existing LLMs for medical applications, the paper does not delineate the methodological novelty and performance improvement of the presented model. A crucial ambiguity lies in the selection process of the cited studies, leaving readers uncertain whether it constitutes a systematic review of evidence. Overall, the paper offers some use cases of GatorTron within the literature."
46,"Instead of using a different Image encoder, this work tried to use an adapter at the end of each transformer.

For the mask decoder, SAM-HQ from Ref[2] is used.

The methods are explained mostly in details, including preprocessing, major framework and data sampling strategy.
But SAM-HQ still needs to be explained, as this is not commonly known.
How the adapter is trained, is not explained in details either.

The major drawback of this work, is the speed, which is shown to be substantially slower even than the baseline.

 BTW, the adapter is not improving a lot from baseline shown in the ablation study.

Section 4.2, 4.3 and 4.4 are too simple, which lacks the analysis on speed, and model performance."
47,"This work proposes a new method for data-driven parameter estimation. They use a neural network to estimate parameters and finite difference methods to compute derivatives and integrate the system forward in time for computing a loss. Parameter estimation is an important problem, particularly in the context of learning from experimental data. Several existing methods have been proposed for these types of problems, but all have different benefits and drawbacks. The novelty of this paper is to compute the loss with respect to the solution of the ODEs discretized using finite difference methods. The authors compare the performance of this method against PINNs on a cardiac electrophysiology dataset where they estimate the value of the diffusion tensor. The authors also explore the impact of the amount of training data on the performance of the model and show that it performs well, even with noise.
The method introduced in the paper is interesting and the results are promising, but the paper could use more details on what methods (both machine learning and finite difference) are implemented in the work and explore additional test cases, even very simple ones. 

Primary Concerns
1)	The paper includes no details on the specific architecture used or the time stepping used to solve the discretized ODEs.
2)	The method is applied to learn a single derivative in a set of ODEs, which contain no other derivatives. Applying this same method to other problems that have more complex expressions may lead to the accumulation of error using finite difference to evaluate the system and thus poor convergence. This may affect performance in certain systems referenced in the conclusion, such as the Burgers equations.
3)	The parameter fields estimated in this setup are quite discrete, I would be interested to see if the model struggles in estimating fields that vary continuously across the domain. This may simplify the task by removing sharp edges, but also may make the impact of estimating incorrectly less apparent during the integration.
4)	In this instance it appears that the cost of integrating the model is significant relative to computing and updating gradients. (My rough estimate is that the model trains 50x less epochs than PINN but trains for half the time, so each training step is 25 times more expensive.) This cost may further increase in systems that demand smaller time steps, require more complex numerics, or may decorrelate more slowly and thus require longer integrations. This is a worthwhile tradeoff in the example shown, but it is important to acknowledge that this cost may not always scale favorably."
48,"This paper proposes two potentially useful metrics, CHAIR and DVH, for evaluating medical LVLMs. Overall this is a good metric proposal, although given the use of chest X-rays, some comparison to other metrics like RadGraph and CheXBert would be useful here."
49,"# Summary
This paper introduces integral physics-informed neural networks (IPINNs), which differ from traditional PINNs by training a neural network to model the integral of the solution rather than the solution itself. This is in contrast to other PINN papers that attempt to incorporate integral conservation laws by numerically integrating the sub-regions [1] or modelling flux on interfaces between sub-regions [2].

This approach is targeted to hyperbolic PDEs, where traditional PINNs are known to break down because the solution is discontinuous near ""shock"" points. This paper shows that the IPINNs capture the solution profile better than traditional PINNS on two challenging problems (Inviscid Burgers' and Shallow Water Equations).

# Review
Overall, this work warrants acceptance to the workshop. The modification to PINNs to incorporate the integral form is elegant in its simplicity. More importantly, the authors demonstrate that their method is effective on challenging hyperbolic PDEs where PINNs are not.

## Originality
This is not the first work to integrate the integral form of conservation laws into PINNs. However, this work differs from previous approaches by having the neural network learn the integral of the solution over the state domain, rather than the solution itself. The original solution is then recovered by auto-differentiation of the neural network. To my knowledge, this seems original, but there may have been something I have missed in the deluge of PINNs-related literature.

## Significance
Unlike many PINNs-related papers, this work is targeted at difficult, hyperbolic PDEs that pose challenges for both PINNs and traditional numerical methods. By taking advantage of knowledge of the PDE (hyperbolic with conservation law), the performance of PINNs can be greatly improved. This illustrates that traditional PINNs are not a ""plug and play"" approach that can solve any PDE with the same framework.

## Quality
The method is relatively straightforward as written. The authors change the loss to be on the integral of $U$ rather than $U$ itself. However, this makes a large difference in the performance of the network in empirical settings (Figures 1 and 2). The authors pick challenging examples that are a failure mode of traditional PINNs, and aim to solve an important downstream task (shock detection).

While not necessarily required for a workshop paper, I would expect in a full paper to see an empirical comparison to the referenced competing ""conservative"" PINNS approaches such as cvPINNs [1] and cPINNs [2]. My hunch is that these other methods will break down in the hyperbolic setting, but it would be good to validate that hunch empirically.

## Clarity
The method and paper are clear and easy to follow. However, Figure 1 (left panel) and Figure 2 are quite difficult to read. It is difficult to visually distinguish the solutions. I suggest breaking out each competing method into a separate facet that is compared with just the reference (Clawpack).

Also, I have a minor stylistic comment. It is confusing to have the neural network be referred to as $N$ and $N_\theta$. The use of capital ""N"" is typically reserved for a number (e.g. number of samples). This is especially confusing when in Equation 8 the lowercase $n_d$ refers to the dimension size.

[1] Patel, Ravi G., Indu Manickam, Nathaniel A. Trask, Mitchell A. Wood, Myoungkyu Lee, Ignacio Tomas, and Eric C. Cyr. “Thermodynamically Consistent Physics-Informed Neural Networks for Hyperbolic Systems.” Journal of Computational Physics 449 (January 15, 2022): 110754. https://doi.org/10.1016/j.jcp.2021.110754.

[2] Jagtap, Ameya D., Ehsan Kharazmi, and George Em Karniadakis. “Conservative Physics-Informed Neural Networks on Discrete Domains for Conservation Laws: Applications to Forward and Inverse Problems.” Computer Methods in Applied Mechanics and Engineering 365 (June 15, 2020): 113028. https://doi.org/10.1016/j.cma.2020.113028."
50,"**Overall Evaluation**  
The primary innovation of this paper lies in bridging the gap between traditional object detection algorithms, which often lack adaptability to novel categories and introducing a weakly supervised class augmentation leveraging image-level labels. This approach underscores the paper's originality, and the empirical results demonstrate its efficacy.

*Pros*
1. The bottleneck of iFSD is effectively addressed through weak supervision, and the use of coarse-grained supervision by the authors is reasonable.  
2. The WS-iFSD pipeline is thoughtfully designed, aligning seamlessly with the motivation. Additionally, the potential uncertainty in pseudo-annotation is well mitigated by MCF.  
3. Overall, the paper is well-organized and easy to follow.  

*Cons*
1. The analysis of computational complexity is missing. Considering that employing Pseudo Bounding Boxes from ImageNet through Grad-CAM facilitates better class augmentation, it might also increase time complexity. The authors should delve into this aspect, comparing the added complexity against performance enhancements.  

2. The details for misclassification filtering are not comprehensive. One arising concern is whether the comparison of predicted labels against ground-truth labels was performed manually, which could be resource-intensive.  

3. Given that class augmentation is derived from ImageNet, the extent of class overlap could be pivotal. While the authors touch upon the impact of novel overlapping classes, merely adding more classes, whether overlapping or not, for the novel classes AP seems somewhat straightforward and naive. A more thorough examination of the balance between the introduction of new classes and its effect on the base classes AP would be persuasive.  

*Minor comments*  
(1) Line 5: hypernetworkis -> hypernetwork is  
(2) Line 156: hypernetworkand -> hypernetwork and"
51,"In this paper, the authors propose approximating a spatio-temporal PDE-based model of infection disease, based on tradition SI (Susceptible Infected) compartmental models typically used in epidemiology, with several data-driven / ML models.  Three experiments are designed, testing things like predictability, robustness and sensitivity to noise, and the proposed methods are evaluated using these experiments, on a test case which models the spread of disease within Germany.

The paper in question is very clear, concise and well-written.  I have not seen similar papers integrating PDE-based SI models with machine learning, and think there is some novelty here.  The experiments are well thought out and tackle a pseudo-realistic problem.  The information placed in the Appendix was appropriate for an appendix and nicely supplemented the main part of the paper.  Of the three ICLR papers I reviewed, this was by far the strongest.  

Below are a few questions/corrections I had while reading the paper.  I realize that there is a page limit, and likely this is why some of the details I ask about were not provided.

- Section 2: I assume you are working with a 2D model of Germany, correct?  I suggest to state this explicitly.  I think in this case x should be a vector not a scalar.
- One question that may come up is why the PDE-based SI model cannot be solved on its own for projections without ML / data-driven models, if it is only a 2D model not discretized by a very fine grid.  It might be good to clarify this.
- How much work was required to train / optimize the NN-based models?  
- Is there any insight into why the Graph Encoding approach does relatively well for task 2 but not for the other approaches?"
52,"Summary:
The authors propose and explore an approach to do joint inference over a hybrid (semi-parametric) partial differential equation. In a two-step approach, they first fit both the non-parametric and the parametric part jointly with gradient descent and use the result as a starting point for SG-HMC. One key aspect of the pipeline for the integration with numerical solvers is differentiable programming.

Pros:
- The approach is set up for realistic problems of dynamical systems for climate and weather predictions and as such highly relevant.
- The manuscript is well written and the authors do a great job in clearly explaining the different layers of their approach.
- The work shows a proof of concept highlighting the advantage of the approach over traditional and non-parametric methods.
- A discussion on ways forward to improve it or potential issues within semi-parametric modeling, such as equifinality/non-identifiability, are briefly mentioned.

Cons:
- A more thorough explanation of the results for the concrete problem and their consequences would be desirable (though I understand that it goes beyond what fits into a workshop paper of 4 pages). 

Open questions:
- It is interesting to me how this approach would compare to other scalable approaches of deep uncertainty quantification, such as deep ensembles or variational inference, both in performance and posterior distributions.
- As we are doing joint inference over the parameters and the non-parametric part, it would also be nice to show the posterior distributions over the $\delta$ and $U_1$ parameters. 
- Given that the data is synthetic, is there a specific reason why the deterministic estimator does not converge to the exact $\delta$ and $U_1$ value? Does it average out with multiple fitting runs, or is there a structural positive bias? Does it persist in the posterior distribution?
- What is the no-parameterization approach?
- The results show a clear advantage of the approach over traditional and non-parametric approaches up to roughly Time 3500h. After that, the Smagorinsky parametrization seems to take the lead. How can this be explained, and is there a need and/or a way to improve the long-term performance?"
53,"**Summary**:

The authors present comprehensive work towards building a Foundation Model for computational pathology. They motivate the need for an FM in pathology, providing a background of existing work in the field. They present three models based on the Visual Transformer Architecture combined with two SSL algorithms, DINO and MAE.  They have compiled a significant dataset for the pretraining of their FM using SSL and perform benchmarking on multiple downstream tasks. Their approach shows higher AUC for most tasks over the baselines they have used.

**Pros**:

- The models proposed by the authors showcase a clear superiority in performance to the baselines.
- The authors have collected an impressive amount of data pre-training their FM, alluding to their collected dataset being an order of magnitude larger than any other data collected in the field.
- The authors have done a good job of investigating the training behavior of their models, and have indicated potential next steps for extending their work, all of which I agree with. 
- I am glad the authors have discussed open-sourcing their model, as I view their data collection and FM as valuable contributions to the field of pathology.

**Considerations**:

- Can the authors provide some reasoning regarding the generally poor performance observed across the proposed models and baselines for Task 6: `Institution 2
lung cancer immunotherapy outcome prediction`? It appears that this dataset has the largest label imbalance across the different tasks the authors are testing for. Could that be the reason? 
- Authors use validation AUC to indicate performance, but AUC as a metric captures an aggregate performance of the model across different operational thresholds. When operationalizing an FM for a clinical setting, one often faces the dilemma of considering the ideal operational threshold of classification (especially in the binary case which is the case with a lot of the downstream tasks the authors test their models on). This is a minor nitpick, and maybe something the authors can show in supplementary material. However, I would be interested in the tradeoffs their FM makes on sensitivity vs specificity at a given threshold for the various downstream tasks.
- It is interesting to note that ViT-large with MAE performs worse in most cases than the two ViT models trained with DINO (in three cases, worse than the baseline). Can the authors comment on why they think this is? Did they explore ViT-large with DINO? If not, could they comment on why?
- The authors have provided multiple examples of SSL models pre-trained on pathology data, could they comment on why they didn't use some of those methods as baselines along with ResNet50?

**Quality**: 

The overall quality of the paper is good. 

**Originality**:

The authors pre-train their models on a very large corpus of pathology data, which they indicate is larger than any corpus of pathology data collected before. While the authors have used some off-the-shelf methods like DINO for their SSL strategy, the scale of the data they have pre-trained their models on encourages me to believe in the novelty of their SSL approach. This is further reflected by excellent performance in downstream tasks with their proposed approach. However, I am a little concerned with the lack of variety in their chosen baselines, I would like the authors to add some more baselines that use SSL as a pre-training strategy to firmly indicate the superiority of their SSL approach.

**Significance**:

The authors' contributions to the field of pathology with their FM and their collected corpus of data could potentially be very significant for the field of pathology. The authors have correctly identified a list of follow-up questions based on their approach which could further help assert the significance of their FM if they are answered. 

**Miscellaneous Comments**:

- Could the authors elaborate a little more about GMA, as this is not a method I am familiar with? My assumption was the spatial distribution of the tiles of a single slide would be necessary for the downstream prediction of the slide as a whole, since you do not have tile-level annotations.  Yet. the authors state in benchmark training that GMA does not consider the spatial distribution of the tiles in its prediction. I would appreciate it if the author clarified why GMA's property of not considering the spatial distribution of tiles works here."
54,"## Summary
---
This paper presents Deep Relational Dependency Networks (Deep-RDN), a framework that combines relational dependency networks and neural networks to integrate structured and unstructured data. It employs a decision tree for structured data and a neural network for unstructured inputs, incorporating domain knowledge through preference rules. Tested on ADE20k and RelKP, the model outperforms baselines in noisy, multimodal tasks.

## Strengths
---
- Novel combination of symbolic reasoning and deep learning with consistent improvements over baselines, especially in noisy scenarios.
- Comprehensive evaluation with clear metrics and integration of domain knowledge for enhanced interpretability.

## Suggestions for Improvement
---
- Dataset Choice: ADE20k is primarily unstructured and doesn’t align well with healthcare or other high-stakes applications. Medical datasets like MIMIC-IV or CheXpert, which combine structured records and unstructured imaging/text, would better validate the model's utility.

- Missed Citations: The paper overlooks related work in neurosymbolic reasoning and multimodal learning frameworks, such as approaches integrating neural networks with probabilistic graphical models (e.g., Learning using Privileged Information by Vapnik, hybrid neurosymbolic frameworks).

While innovative, addressing these gaps and testing on more relevant datasets would enhance the paper's alignment with its stated high-stakes application focus."
55,"**Summary of the proposed work:**

Conventional autoencoders typically use generic network architectures and lack structure or interpretability in the latent code representations. A recently proposed CTRL framework uses rate reduction to build generative models while imposing a Gaussian mixture model like structure in the latent codes.

The submitted work proposes CSC-CTRL in which the generation of natural images is modeled using a stacked concatenation of multiple convolutional sparse coding (CSC) layers and incorporated within the overall CTRL framework. The atoms in the convolutional dictionaries of the encoder and decoder are kept the same which allows for sample-wise alignment in the generated samples—an improvement over existing CTRL approach. 

The paper shows good image generation quality with interpretability in latent codes with respect to image classes in the dataset. There is additionally a higher stability to noise than vanilla CTRL owing to the incorporation of sparsity.

**Pros:**

- The results are nice and convincing with respect to the main ideas in the paper’s body.
- Barring a few minor typos, the paper is generally clearly written.

**Cons:**

I have the following questions/concerns that I would like the authors to clarify.

- The authors emphasize that the key goal is to show the feasibility of high quality image generation using CSC layers. While their results are interesting, there exist a few recent works which also use multi-scale CSC to generate high quality images, for example [1] and [2].  These focus on imaging inverse problems rather than autoencoding, though one could extend them to autoencoding by say tying the weights of their encoders and decoders.

There are indeed some differences between the proposed work and the 2 references, for example, [1] performs direct pixel level MSE loss minimization instead of distribution matching or CTRL. However, given similarities like CSC based high quality generation and interpretability in obtained dictionaries, a comparison (conceptual or experimental) would be helpful to better place the contributions of the submission with respect to literature.

- One of the advantages mentioned is that CSC-CTRL has reduced computational cost over prior models (line 48 in the paper). But wouldn’t the use of FISTA iterations in the encoder’s forward pass add extra overhead making it more expensive than a conventional autoencoder?   


[1] Liu, Tianlin, et al. ""Learning multiscale convolutional dictionaries for image reconstruction."" IEEE Transactions on Computational Imaging 8 (2022): 425-437

[2] Li, Minghan, et al. ""Video rain streak removal by multiscale convolutional sparse coding."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018."
56,"This work studies learning halfspaces in the presence of malicious noise, where an adversary can corrupt both instances and labels of training samples. 

$\textbf{Summary of relevant literature.}$ The work provides very detailed information about prior works, which are important to interpret the contribution of this work. The literature is summarized below.

Under the setup, the goal is often to characterize the maximum amount of noise (potentially as a function of relevant parameters such as accuracy parameter $\epsilon$ and dimension $d$) one can tolerate so that the error of the learning algorithm remains bounded from above by $\epsilon$. 

Without any further assumption, the theoretically optimal answer to the above is $\epsilon / (1 + \epsilon) = \Theta(\epsilon)$. 

A long line of work has focused on designing efficient algorithms under distributional assumptions.  In particular, the bound $\Omega(\epsilon)$ is shown to be attainable by polynomial time algorithms under isotropic log-concave distributions.

Another line of work instead imposes margin assumptions on the clean sample points, and obtains algorithms that could tolerate at most $\tilde \Omega(\epsilon \gamma)$ fraction of noise, where $\gamma$ is the margin size. Somewhat surprisingly, this bound has already surpassed the assumption-free info-theoretic bound of $\epsilon / (1 + \epsilon)$ when $\gamma$ is sufficiently large, showing that the margin assumption has fundamentally changed the nature of the problem information theoretically. 

$\textbf{Main Result.}$ Building on top of the prior work, this work shows that, under both the margin and a pancake-style distribution assumption (which is satisfied by mixture of log-concave distributions), the noise-tolerance rate can be improved all the way to $\Omega(1)$, a function independent of any other relevant parameters of the problem.

$\textbf{Technique.}$ The technique builds on top of the prior work of Talwar (2020), which analyzes hinge loss minimization under the same setup. In particular, the author observes that the main bottleneck of the prior work is that the adversary may construct samples that result in large linear sum norm — a quantity closely related to the subgradient norm of the hinge loss function, and hence also the estimation error.  Motivated by this, their algorithm uses the idea of soft outlier removal, developed in the context of designing a polynomial time algorithm with optimal error tolerance under distribution assumptions, to find weights to control the empirical variance of the sample sets. This turns out to be sufficient to control the linear sum norm of the sample, and they just need to perform weighted hinge loss minimization to conclude the algorithm.

$\textbf{Strengths.}$ The paper is very well-written. The algorithm proposed is natural and effective, but leads to very surprising results. Overall, I think this is a good contribution to the community.

$\textbf{Weakness}.$ The fact that the margin needs to scale poly-logartihmically with the accuracy parameter is a bit unsatisfying, and the authors leave it as future work whether such dependency can be improved. 

Question: It seems like the result is for homogeneous halfspace. But since the distribution assumption does not require the covariance to be of full-rank, does that mean one can also handle general halfspaces by simplifying embedding the bias as an extra coordinate?"
57,"Scope:
The manuscript introduced a “Neuro Symbolic AI” and “Graph Neural Network” to solve the challenges of deep learning including reasoning, semi-supervised learning, and long-tailed issue. They proposed a integrated hierarchical taxonomy of class labels in loss function of classifier which help in fine grain classification of text documents.

Strength:
1.	The manuscript introduced the method to integrate a well-designed hierarchical taxonomy into the learning algorithm of a neural network for guiding the learner to achieve significant results in classification problem.

2.	New loss function was developed to trace loss in computing both semantic loss and its gradient.

3.	The authors have written a good quality of paper and covering all the relevant details to address the understanding of reader. The concepts were explained in good way and limitations of existing supervised classification and semi supervised methods were discussed in details.

4.	Detailed experimental results were reported in ablation study.

5.	The experimental results were reported on two standard English datasets and one self created Chinese dataset including, English RCV1 and Amazon Product Review and Chinese In-house datasets. Two languages were used to evaluate the performance of system. 

Weakness:

1.	Architecture diagram of the proposed methodology is not clear. The author is suggested to redraw the architecture diagram highlighting the contribution of author in existing area of research.

2.	The result section is weak and rewrite the headings in results section to add more meaning. Author need to further elaborate their findings in better way. The ablation study need to be discussed and every table need to be explained.

3.	The related work section is  on weaker side and  author is suggested to update it with recent research paper to solve class imbalance problem as listed below:

Nida, N., Yousaf, M.H., Irtaza, A. and Velastin, S.A., 2022. Video augmentation technique for human action recognition using genetic algorithm. ETRI Journal, 44(2), pp.327-338.

Seo, K., Cho, H., Choi, D. and Park, J.D., 2022. Implicit Semantic Data Augmentation for Hand Pose Estimation. IEEE Access, 10, pp.84680-84688.

Škrlj, B., Martinc, M., Lavrač, N. and Pollak, S., 2021. autoBOT: evolving neuro-symbolic representations for explainable low resource text classification. Machine Learning, 110, pp.989-1028.

DeLong, L.N., Mir, R.F., Whyte, M., Ji, Z. and Fleuriot, J.D., 2023. Neurosymbolic ai for reasoning on graph structures: A survey. arXiv preprint arXiv:2302.07200.


Overall assessment:
 Marginally above acceptance threshold. Experimental result section need to be rewritten to add more clarity and cohesion."
58,"Contribution:
The authors made two contributions in their work: 1) clear definition of hallucination within the medical field, breaking it down into two types: Object Hallucination, which involves incorrect or fabricated details about objects, and Domain Knowledge Hallucination, which pertains to inaccuracies in medical knowledge or practices. 2) they developed a new tool called Med-HVL, designed specifically for the medical domain, to detect and evaluate these hallucinations.

Pros:
Its an interesting glimpse into the bias of LLM pretrained into a large amount of medical data. A nice extension of the work would be to systematically compare and benchmark those hallucinations across multiple dataset and models.

A few remarks:
- in the figure, gt caption and gt observation are the same. Is there a gt caption without irrelevant info that would not be the gt observation? it would be nice to have a different example in the figure
- ""Object"" in this context seems incorrect and can be confused with support device. Maybe a more suitable term would be anatomical structures?"
59,"Overall, the paper has good illustration and interpretation, the project has practical relevance, and an innovative approach is designed to model complex urban traffic scenarios, making it a positive candidate for acceptance.

### Quality
1. The paper demonstrates a comprehensive methodology for traffic simulation in urban intersections. It utilizes real-world trajectory data for training trajectory forecasting models, and the method and training process are well-constructed with clear steps in data collection, processing, and model application. 

2. The paper is also well organized with a clear interpretation of the result and case analysis. It is a good paper for reading.

### Clarity

1. The paper's figures and tables effectively support the textual content, providing clear visual summaries of the methodology and results. 
2. However, some sections could benefit from more detailed explanations to avoid ambiguity: such as in table 1. what does the `Wpts` mean? 
3. Some relevant work is suggested to include to provide a more comprehensive survey, such as for trajectory forecasting, another branch of method named Generative Adversarial Imitation Learning (GAIL):  
[1] Choi S, Kim J, Yeo H. TrajGAIL: Generating urban vehicle trajectories using generative adversarial imitation learning[J]. Transportation Research Part C: Emerging Technologies, 2021, 128: 103091.  
[2] Da L, Wei H. CrowdGAIL: A spatiotemporal aware method for agent navigation[J]. Electronic Research Archive, 2022, 31(2). 


### Originality
The paper is of good originality.

### Significance

The significance of this work lies in its potential impact on urban planning, traffic management, and autonomous vehicle systems: 
1. It helps in providing a more accurate and realistic simulation of traffic dynamics at intersections, 
2. This research could help in designing better traffic control systems
3. This paper may have a potential influence in improving the safety and efficiency of urban transportation."
60,"This paper provides algorithms for online portfolio selection problem.
Proposed algorithms achieves data-dependent regret upper bound that is logarithmic in the number of time periods $T$,
as well as problem-independent regret upper bound of $\sqrt{T}$.
These results are expanded to derive first- and second-order regret bounds,
by incorporating optimistic online learning framework with convex hint functions.


One of the novel technical contributions in their paper is the construction of a new adaptive surrogate objective function to derive the main results. In addition, the use of regularization through a logarithmic barrier function, combined with an adaptive learning rate schedule, can also be considered a technically innovative aspect.

The obtained results are appropriately compared with those from previous studies, as cited, and appear to indicate solid improvements. Furthermore, some techniques provided in this paper have the potential to be useful in future research, and the newly constructed framework is designed to be accessible and applicable for subsequent studies (e.g., as demonstrated in Appendix A). I believe these contributions will be of interest to the online learning community. Based on these considerations, I support the acceptance of this paper."
61,"Summary
-- the study generated discharge summaries using GPT-4 based on a MIMIC-III dataset (with secure Azure instance). The quality of the summaries was evaluated by clinicians.

Pros:
-- good use of secure Azure instance in accordance with Physionet DUA
-- rigorous clinician evaluation with clear labeling criteria (NHS harm definitions) and duplication for inter-rater agreement
-- good use of json format template with initial experimentation to adjust prompts. Good use of one-shot in-context example.
-- good reporting of inference time and costs, in addition to performance

Cons:
-- small dataset, n=53
-- low but realistic inter-rater agreement at 60%
-- could have used JSON-enforcing 
-- A better definition for the performance metrics would be an atomic-fact approach to evaluating the accuracy of statements in the summary. Metrics here could be fact precision and recall. Using the total number of sentences could inflate the denominator and thus deflate the ""FPs"" and ""FNs""
-- this is a relatively naive approach to discharge summary synthesis (dumping all the prior notes in-context). A more compelling study could have included a comparison to RAG-based approaches."
62,"***Summary***
The paper proposes and analyses a Metropolised version of the preconditioned Langevin algorithm, which can be viewed as a crude version of Riemannian Langevin. Rates are obtained under various concordance/convexity assumptions, which largely match those of zeroth order samplers.

***Strengths/Weaknesses***
I did not check all the details of the analysis, but broadly the results seem to be believable and quite analogous to those derived for the Dikin walk.

The paper does a good job of explaining the differences between the various notions of self-concordance which have since proliferated in the literature.

The new isoperimetric inequality (Lemma 17) is a solid contribution and will be useful for the existing literature.

The paper is forthright that its result is not better in asymptotic order than that of Dikin walk. While the analytic route to getting there was not trivial and is a worthy contribution in itself, it would be much better if the paper could at least speculate on what might be a plausible route to improving the rate estimates.

Furthermore, it would be helpful if the main text contained additional motivational information on the algorithm, as compared to the Dikin walk.

***Questions***
Here is a naive question: what is the expected rate of PLA without any Metropolis adjustment? Can one hope to improve the dimension dependence of this algorithm if an underdamped version is used instead? Finally, is there a proximal variant of this algorithm, which alternates some type of Gibbs sampling procedure?

The final remark about combining with Gaussian cooling should probably cite Kook and Vempala rather than Cousins and Vempala (which is only valid for the uniform case).

Page 5: $\mu$ in (WLD) is confusing for a deterministic function, since $\mu$ was used earlier for a measure

The equation for $\rho$ is also very small in (WLD) for some reason."
63,"This paper investigates using a neural ODE-LSTM with an attention mechanism to carry out the task of multi-touch attribution. They compare its performance to a transformer, temporal CNN and an attention LSTM, using two different datasets (auction-based advertising and marketing sign up data). They find varying performance between these models depending on the test metric and test dataset used, and discuss the limitations/advantages of each model.

In general, the paper seems to offer a novel approach for solving the MTA problem, and it appears to be competitive with the other benchmarks tested. Furthermore, the paper is concise, well-written, and appears robust in its ML workflow and tests. 

I see a few weaknesses that could be addressed:

-	Is there any justification why an ODE should be used to model this data, apart from the fact it is more amenable to irregular time-series? Do we expect customer behavior to be determined by an ODE? Given that the other models tested perform in the same ball-park, I am unsure if this data is really generated from an underlying ODE.

-	The authors mention other simpler MTA models in the introduction (Linear, Time Decay, U-Shaped models etc). If possible, it would be useful to compare against these as baseline approaches to see how much better the learned approaches perform

-	Section 2.1 – typo in the word ‘sequences’"
64,"Summary
-- the study generated discharge summaries using GPT-4 based on a MIMIC-III dataset (with secure Azure instance). The quality of the summaries was evaluated by clinicians.

Pros:
-- good use of secure Azure instance in accordance with Physionet DUA
-- rigorous clinician evaluation with clear labeling criteria (NHS harm definitions) and duplication for inter-rater agreement
-- good use of json format template with initial experimentation to adjust prompts. Good use of one-shot in-context example.
-- good reporting of inference time and costs, in addition to performance

Cons:
-- small dataset, n=53
-- low but realistic inter-rater agreement at 60%
-- could have used JSON-enforcing 
-- A better definition for the performance metrics would be an atomic-fact approach to evaluating the accuracy of statements in the summary. Metrics here could be fact precision and recall. Using the total number of sentences could inflate the denominator and thus deflate the ""FPs"" and ""FNs""
-- this is a relatively naive approach to discharge summary synthesis (dumping all the prior notes in-context). A more compelling study could have included a comparison to RAG-based approaches."
65,"This paper deals with the question of designing strategyproof learning algorithms in a regression or classification setting where an additional prediction on the optimal solution can be leveraged to improve the mechanism performance. The learning mechanism is required to be strategyproof, in that no agents can benefit from misreporting their data. Building on previously used strategyproof mechanisms, new mechanisms which incorporate the advice are proposed.  In line with the literature on algorithms with predictions, worst-case trade-offs between robustness and consistency are derived, and this trade-off is shown to be tight in both the regression and two-labellings classification setting. 

Strengths:
- The results of the paper are complete, tight, and cover multiple settings previously studied in strategy proof learning
- The results and setting are novel 
- The Lower-bound to show tightness in the median estimation setting is non-trivial

Weaknesses
- The writing in some parts could be improved, see comments
- I feel that the originality/contribution of those new results are limited, in that it is a standard mechanism + predictions problem

I have quickly read through most of the proofs for the median estimation setting, and they seem correct, besides some small omissions. 

Some comments on the writing: 
- The introduction and related work section should be more developed and better highlight why one should care about strategy-proof learning with predictions in particular. Right now it feels like a juxtaposition of a list of papers from strategy-proof learning and then papers on Mechanisms with advice. The question of combining the two is indeed natural from a theoretical standpoint, but it should still be better motivated; as an example the real-world application mentioned at the beginning is the same as the one in Dekel et al (2010) dealing with strategyproof regression.  
- In terms of related work, I think it should be better explained what were the methods in previous works, to compare and see what techniques were used in this paper. For instance, I don’t think that it was explicitly mentioned that the Project-and-fit algorithm is taken from Dekel et al (2010), although the results are indeed cited. Similarly a common technique in algorithms with predictions is to take the convex combination of the prediction and of the ‘standard’ algorithm and then analyze the performance. Here a similar method is used in that somewhat a convex combination of the true data-set and the predicted data set is used. 
- Some parts of the proofs or arguments are confusing and should be better explained. For instance in the first paragraph of the proof of Lemma 5, it is not clear to me what is the relationship between Observation 2 and the statement right after it. After reading the original strategyproof argument in Dekel et al I understand why, but on a first read it was quite unclear. Similarly in the paragraph below Theorem 16 the role of the $g_i$ was unclear. Is it correct that the $g_i$ are basically a way to get instances of agents data such that the order induced by the risk is consistent with a specific preference profile over labellings? 
- There are Claims, Observations, Lemma, but I don’t really understand the difference between them honestly. Why is Lemma 9 not a claim or an observation for instance? 

Questions:
- Can we say anything about the smoothness of the mechanism with respect to the predictions? That is to say, what is the best value of $\beta$ such that $R(M(S,\tilde{a}),S)< \beta R(\tilde{a},S)$?
- In Dekel et al group strategy-proofness was proven for any size of the data-sets. Here $\vert S_i \vert$ is required to be odd for all $i$. Is it a limitation of the proof, the mechanism, or the setting with advice? More precisely, are there examples which show that if $S_i$ is even then it cannot be group-strategy proof for any mechanisms? If not, can either the proof or the mechanism be fixed? 
- Do the results hold if $\mathbb{Y} \subset \mathbb{R}^2$? 
- Do any lower bound results similar to Theorem 7 hold if we consider randomized mechanisms for the median estimation problem? 
- Proof of Lemma 10: I don’t understand why the application of Corollary 25 immediately leads to the last inequality on top of page 21 to be greater than $1+\gamma$.
- Why can we assume in the proof of Claim 12 that $D’>b$? 

Small comments/typos:
- Consistency and robustness are mentioned in Theorem page 2, but are not defined at this point. Maybe a brief explanation in words should be added.
- Page 2: “of the advice error $\eta$.” -> “of the advice error.” 
- Page 3: “for the special case of classes” -> “for the special case of function classes” 
- Why not scale gamma in the regression setting so that it is always in $[0,1]$ to be consistent with the classification setting?
- In table 1, the $O(n)$ should be replaced by $\Omega(n)$, same in the Theorem 17. In table 1 and Theorem 15, the robustness is not unbounded: It is lower bounded by $\Omega(n)$.
- Lemma 2: Missing space before parenthesis 
- Theorem 6: missing link to the proof in the appendix 
- Last inequality of the proof of Lemma $2$ is only valid because $\vert y_{i,j} <a \vert \geq \vert y_{i,j} \geq a \vert$ as $a^*$ is the median and $a>a^*$. 
- Proof of Lemma 4 end of first paragraph: missing something like “And because R(a,S) is decreasing over $[b,a^*]$...”
- Claim 12 is proven before Claim 11
- Proof of Claim 11: M is used instead of D

---

I thank the authors for their reply. 

I am still between a 6 and 7. It would be better to have a smoothness guarantee rather than a consistency one, which is quite weak. Nonetheless due to the lower bound that seems non-trivial to obtain, and the fact that the results are quite complete (both median estimation and classification) I am going to change my score to a 7, provided that the authors do make the necessary writing improvements mentionned by another reviewer and I."
66,"*Summary.* The paper studies computationally efficient approximate reductions between a few distribution families with TV deficiency in the non-asymptotic regime. The goal is to construct a randomized mapping which transforms a sample from a distribution $u_\theta$ with unknown $\theta$ to a sample whose distribution is close to $\nu_\theta$. The computation efficiency is measured in terms of the number of samples generated from a reference distribution and the number of pointwise evaluations under $\nu_\theta$. The main result of the paper is the construction of reductions from distribution families including Laplace, uniform and Erlang location models to arbitrary high-dimensional product distribution under regularity conditions. The paper also describes applications of the reductions in regression and differential privacy (DP).

*Significance* The paper studies reductions between canonical distribution families, which is of interest to the statistics and learning theory community. I don't follow the line of work closely, but the introduced problem of approximate reduction between distribution is novel to the best of my knowledge. The explicit construction of a pure DP mechanism distribution that is close in TV to Gaussian mechanism might be of interest to the DP community.

The paper is nicely written. I didn't examine all the proofs closely but the technical results seem correct based on the discussions and sketches of proof ideas.
 
*Comments*
The main reason that I didn't give a higher score for the paper is about the general significance of the contribution. The results in the paper are specific to a set of source distributions. Can the authors comment on whether a signed kernel with small error can be constructed for general source distributions? 

Could the authors comment on how good the reductions are? For examples, is there a lower bound on the number of oracle calls to the pointwise evaluation on the target model?

While the paper list a few applications, the reduction approach doesn't seem to lead to new results in the described applications.

Corollary 8: It seems the accuracy here is even worse than the sum of the variance of the original Laplace distribution and the targeted Gaussian distribution. So the usefulness of the result is less clear.

==========Post rebuttal=====
Thanks for addressing my questions. My score is increased accordingly."
67,"The work addresses an important problem - the current lack of a meaningful comparison of methods for application of LLMs to process unstructured data for clinical survival analysis. It is very well-written, organized, and enjoyable to read. I am not well-versed in this subfield, but it appears to be reasonably comprehensive. It provides a framework for addressing the issues brought up in the review.

There is also room for improvement. The descriptions of many works lack details and are hard to draw conclusions from. At several points recommendations are given, but they are not very clear or specific. If the point is to highlight that prior work does not support clear recommendations, it would be clearer to state this, as the current structure makes it appear that the intent is for the authors to give methodological recommendations based on the reviewed work. A large part of the contribution is a common evaluation framework, which addresses the gaps in the field highlighted by the review; however, this is largely relegated to the appendix, not described in much detail, and not justified clearly in its design choices. I cannot view the current state of the project as the link is hidden for anonymity.

Also, I would point out that the strategy used for literature search seems brittle. For example, the search includes titles with either ""survival analysis"" or ""time-to-event"" AND ""medicine"" or ""healthcare"". Ironically, this submission itself meets neither of these criteria. This suggests both the need for a more robust search strategy and possibly the need for a more descriptive title for this submission. It would also be helpful if it was clearer from the title that this is a review. 

Questions and suggestions for the authors:
1. In section ""Fine-tuning: Adjusting LLMs for the task - Limitations"": are there any conclusions to be drawn from this? The findings seem contradictory.
2. Prompting: Querying in natural language - Strengths. Can you elaborate? The main point given is that these methods are the most novel, which is not a strength per se.
3. I found the following statement confusing: ""However, multiple risk scores are rarely evaluated due to the prohibitive cost of extracting the required covariates x_i, which are often present in patients’ unstructured health records."" What are the multiple risk scores being referenced here?
4. You encourage researchers to ""compare LLMs strategies on private sources using our implementation."" Can you clarify how this works? Is the data private, and if so, how do other researchers use and present it?
5. What is the current state of the GitHub? Can it be used yet? Can you provide any results for the methods that are implemented so far? I also understand that the intent is partially to use this conference to gather feedback for its improvement.

With some clarification on the nature of the provided evaluation framework, I think the paper meets the threshold for acceptance in its current state, but I hope the authors will take this feedback into account, as it could be a stronger paper without too much effort."
68,"The paper studies the following strategic learning problem. There are n agents, each agent $i$ holds a set of labeled data $\{(x_{ij},y_{ij})\}$. The agents report a possibly manipulated dataset (with labels being misreported) to the learner, and the learner learns a function $f$ in a given class that minimizes the overall loss $\sum \ell(f(x_{ij}),y_{ij})$. Each agent is strategic, and wants to minimize its own total loss.

The paper studies the strategic learning problem with advice. In addition to the labeled dataset, the learner also receives an advice function $\tilde{f}$. The paper shows that when the function class is constant functions or linear functions, there is a deterministic strategy-proof mechanism that is $(1+\gamma)$-consistent (which corresponds to the competitive ratio when the advice is a true minimizer) and $(1+4/\gamma)$-robust (which corresponds to the worst-case competitive ratio for arbitrary advice) for any $\gamma\leq 2$, and the parameters are tight. For general classification problems, when there are 2 labels, the previous constant function result generalizes. When there are multiple labels, large robustness parameter is needed for both deterministic and randomized mechanisms.

Evaluation: The paper generalizes the learning with advice type problems to the strategic learning setting, and provides tight consistency/robustness characterization of strategy-proof mechanisms with advice for constant functions and linear functions. For classification problems, the paper provides strong hardness results on robustness. Thus, the paper provides a clear and complete solution to a natural problem and is well above the bar of the conference.


Minor typo:
Page 4. “Ouput” -> “Output”"
69,"This paper proposes a framework for incremental few-shot object detection, which leverages a meta-learning approach with backbone and detection heads frozen, to generate class-specific codes, and it also introduces a weakly supervised approach for class augmentation technique with minimal requirement on image-level localization labels.

The novelty of this paper seems limited as the used meta-learning method, the freezing scheme and the class augmentation technique share much similarities with existing ones. The abstract states that ""it outperforms the state-of-the-art ONCE approach 13 on the MS COCO dataset"". However, ONCES [26] is published on CVPR 2020, which may not be viewed as a SOTA method."
70,"**Summary.**
The goal of the paper is to study whether plant diversity affects the thermal diffusivity of soil. The existence of this effect is supported by several physical studies. However, the current paper provides the first data-driven study on the magnitude of this effect. The current paper studies the effect using 20 sites from the Jena Experiment dataset. For each site, the thermal diffusivity coefficient is estimated by solving the heat diffusion equation using a physics-informed neural network (PINN). Then, the dependency between plant diversity and the thermal diffusivity coefficient is investigated via a linear mixed effects model with random intercepts. The results indicate a negative correlation between plant diversity and heat diffusivity.

**Discussion.** This paper constitutes a clear and straightforward application of PINNs in science, which closely fits the theme of the workshop. The presentation in the paper is very clear, and the figures are helpful for understanding the setup. The setup of the study is sound, and the paper provides important evidence supporting the existing work on the effects of plant diversity.

In my opinion, it would be helpful to discuss the following aspects in the camera-ready version of the paper.
1. Can other characteristics of the sites provide an explanation for the variability in the thermal diffusivity? For example, it is possible that soil conditions (based on the distance to the river) have an effect on both plant diversity and thermal diffusivity, leading to the observed correlations. How can we rule out this hypothesis?
2. What advantages do PINNs provide over the traditional inverse problem methods for the considered setting?

Minor suggestion: Use `\citep` to put some citations in parentheses."
71,"Clarity: 
The authors present a well written paper stating the current litterature and why their work is an advancement in the field of EEG pretrained models stating that their model achieves better performance and is able to generalize across different tasks via finetuning. 
Originality: The work combine many state of the art methods in the space of foundation models and apply them to a large scale foundation model trained on EEG data. A left out dataset reserved for validation that has no connnection with the TUH training dataset. 
Significance: The performance shows a clear advancement within this space compared to previous supervised and self-supervised models. 
Quality: the authors sometimes miss explanations on datasplitting and other common practices that need to be present to ensure. The fundamentals should be included in the script.

Major points
1.	Figure 1 should have a better explanation emphasising subfigures a and b 
2.	the finetuning paradigm is not clearly defined. You should consider explaining the way you constrained updating the weights during end-to-end fine-tuning if you did so to avoid misconceptions.
3.	In table 2 you present the AUROC and AUPRC of only a subset of the datasets you have avaliable and you show that with the model architecture you have created there is little difference between traning self-supervised and trainning supervised. I would want you to show the performance on the Neonate dataset to demonstrate the difference between supervised or self-supervised traning.
4.	There is very little disscussion of the results and interpretation of for example figure 3 showing the intrepetation of the naïve Bayes model. Here you seem to highlight the areas for seazures but you do not specify any output from the model. 
5.	You need to specify the datasets used for pre-training, fine-tuning, and testing to ensure that there is no lekage from training to testing 
Minor points
1.	In table 1, there is little reason to present all the EEGFormer variants as they have very similar performance. I would suggest that you proceed with only the EEGFormerl  for simplicity and just state the other tests in the text.
Pros 
•	The authors find high effectiveness in their model training a codebook to represent EEG with subsequent finetuning to solve interesting tasks such as: abnormal EEG detection, classifying EEG artifacts, classifying EEG slowing events, seizure detection, and neonatal seizures detection.
cons
•	Authors do not show the perfomance of finetuning styles for models on all datasets 
•	Auphors do not provide the way they split the training, test, and validation data giving no indication that the 
•	The authors presend very little discussion on their results, thus making the intrepetation of their results hard to understand out of the gate.
•	The embeddings seem to be highly dependent on large amounts of fine-tuning in order to perform well."
72,"The authors evaluated the performance of different fine-tuning strategies on medical QA tasks. A variety of medical QA datasets were used in the evaluation. Full parameter fine-tuning vs. LoRA fine-tuning were used and compared. Zero-shot performance were used to evaluate the model performance. 

Only two different size Llama-2 models were used as the base model. Some studies have shown that different base models may provide different level of improvement after fine-tuning, so it would be great to see the results from some other commonly available open source models such as mistral. 

The results were not new and mostly expected. Similar studies have been conducted and the results of this study were generally consistent with some previous findings. The authors did include more QA datasets for the evaluation, which made the results more comprehensive."
73,"This paper evaluates linguistic variation in speech data by age and gender using standard NLP tools: PoS using the Stanford Parser and the Penn Treebank tags adjusted for Singaporean English. The author goes on to derive a variety of linguistic features from this data, on which the analysis is performed. The results largely corroborate existing results related to the effect of age on linguistic variation -- particularly as it pertains to ""lexical concreteness."" The methodology appears to be sound, and the use of the Stanford Parser, in my opinion, constitutes foundation model usage and thus making it a relevant contribution to the workshop."
74,"Firstly, this paper explores the tuning of structure hyper-parameters of LiteMedSAM in detail to improve the efficiency without lowering the segmentation performance compared to the baseline model. Besides, the model can also get a relatively good performance on some external datasets.

However, I think the novelty is limited. The main changes in the model are structural parameters, with no new pre-processing or post-processing techniques added. Some latest efficient image encoder can be explored further to increase the novelty.

And also I have two questions:

1. I have noticed discrepancies in the results for the baseline model, specifically in DSC, NSD, and efficiency, compared to the leaderboard and the original template. Could the author provide more details on how these results were obtained?
 
2. The value of model FLOPs seems excessively high. There might be an error in the calculation code. Could the author check and clarify that?"
75,"The authors propose using an equivariant point cloud representation for latent variables in a neural PDE formulation. The idea is tested on 2D diffusion and 2D vorticity equations with rotationally symmetric forcing. 

The numerical results show improvement based on the baseline, but the baseline DINo is never cited or defined in text. The authors do not provide details on the ground truth solver used or how the initial conditions for vorticity equation was generated. They do not provide value for diffusivity D and do not state what the time steps used in evaluation are, making it impossible to judge the results. Finally, there are 2 typos in the statement of 2d NS equation: it is \mu \Delta v and not \nabla u = 0 but \nabla \cdot u = 0. 

In addition to lack of detail, the experiments are very contrived: what happens if forcing f is not rotationally symmetric, as it is most practical applications?"
76,"This paper derives generalization bounds for non-iid data that are sampled from a stationary mixing process. More specifically, the assumption on the samples Z_1,…,Z_t,… is that, for any time step t, the expectation of the gap between the true loss of any hypothesis w and the loss of w on the sample Z_t, conditioned on the past d samples, is at most phi_d (which is a function that generally decreases with d, with common choices having exponential or polynomial decay). The more common \beta-mixing assumption (which does not involve the loss function) implies this condition when the losses are bounded. 

The technique is an extension of the online-to-PAC conversion of Lugosi and Neu (2023): this work introduced an online learning game where in each step t, the online learner picks a distribution over hypotheses, the adversary picks the cost function of this step to be determined by the datapoint Z_t, the online learner incurs the average cost of their strategy and sees the datapoint Z_t.  Lugosi and Neu showed that the generalization error of a learning algorithm A can be decomposed into the average regret of an online learner as compared to the fixed strategy of playing the distribution produced by A in each step, and the total cost incurred by the online learner. Choosing different online learners can then give us different generalization bounds.

In this work, the authors show that changing the online learning game to introduce a delay d (i.e. the learner sees the Z_{t-d+1} datapoint and not Z_t) allows one to derive a similar decomposition of the generalization gap where the first term is the average regret of a d-delayed online learner and the second can be bounded by phi_d (notice that this implies that the generalization gap is bounded by these terms for all d simultaneously).

Choosing d to be roughly log(n) for exponentially decaying phi_d and instantiating the bound with standard online learners such as MW and FTLR gives us generalization bounds that hold for non-iid data, and match the known bounds for iid data with an extra multiplicative sqrt{log(n)} factor. Similar results are retrieved for polynomially decaying phi_d. 

The authors additionally consider an extension to loss functions that depend on the whole sequence of datapoints so far, which captures dynamical predictors, and show that if the losses satisfy some bounded-memory conditions, this case can be reduced to the previous one. 

Overall, the techniques do not depart significantly from Lugosi and Neu 2023, but I think that it is surprising that this general framework can handle non-iid data, for which most generalization bounds seem to follow tailored approaches. I think that this paper demonstrates the versatility of the online-to-PAC generalization approach and it would be interesting to the ALT community. 


Questions:
- To upper-bound the d-delayed online learning regret, the authors use a general transformation of the regret of a (non-delayed) standard online learner, e.g. MW. Are there online learners that are optimized for the delayed setting and can give us better bounds, perhaps shaving some log factors? Or is there a lower bound that implies that this is the best bound we can achieve anyway?
- Is there an inherent difficulty in achieving fast rates when the training error is zero, for the non-iid case? Or is it possible that one can use better online learners and similar tricks as in the online-to-PAC conversion for iid data that allows us to get fast rate convergence?

Typo:
Lemma 8: “is a concave in y [..]”"
77,"Summary:

The paper explores the use of concept learning for XAI in a GNN model trained to solve the 3-coloring problem. It identifies two key concepts, i.e. support and confidence, that are geometrically encoded within the GNN's embeddings, providing interpretable insights into the model's learning process. Although innovative, the study's narrow focus on these concepts restricts its exploration of dynamic embedding evolution, graph topology, and broader applications.

Major concerns:

- The two key concepts analyzed are the support of a vertex and the confidence in the coloring of a specific vertex, evaluated through the 2D PCA projection of node embeddings. This approach appears somewhat limited and may lead to a relatively incomplete exploration of the embedding space. Additional concepts or alternative projections could have been considered for a more comprehensive analysis. Could you please provide insights on this point?
- The paper does not address the different topologies of the graphs or how these topologies might influence the network's learning process. Could you elaborate on this aspect?
- The work discusses the interplay between the static graph structure and dynamic solution states but does not seem to examine how embeddings evolve over time. Additionally, the concepts developed in the study appear to offer a more static representation of the final output generated by the GNN when reaching a solution. A crucial area for further investigation would be to identify specific strategies within the GNN's message-passing framework that dynamically address and resolve color conflicts throughout the solution process. Please add some comments in this.
- Could you please comment on the expected robustness of the introduced concepts compared to typical heuristic methods used for the Graph Coloring Problem?"
78,"The paper proposes a simple yet effective method that leverages nuclear norm regularization to address the neural network's tendency to capture spurious correlations between labels and images instead of domain-invariant features. The authors provide a comprehensive set of experiments on synthetic and real datasets to showcase the effectiveness of their proposed method. Additionally, the inclusion of theoretical support in simpler settings improves the credibility of their approach.

## Pros and Cons
### Pros
The paper is clearly written and well-presented, all notations, and definitions are clearly conveyed, making it an enjoyable reading experience. Furthermore, most of the claims and conjectures mentioned in the paper are backed up using either empirical or theoretical results, enhancing the overall credibility of the research.

### Cons
The reviewer finds no major flaws in the paper. However, one aspect that raises some questions is the motivation presented in the paper: 'our main hypothesis is that environmental features have a lower correlation with the label than the invariant features.' While the theoretical analysis in Section 4 provides some support for this hypothesis, there is a lack of empirical evidence to reinforce this claim. Given that the setting in Section 4 is a simplistic setting, the reviewer believes that the intuition is not firmly established. Therefore, perhaps adding some experiments to empirically validate the motivation is useful. One starting point could be using the synthetic data introduced in Section 3.1 to visualize the magnitude of the matrix $A$ associated with environmental features and domain-invariant features, respectively, and see if the results meet the claims in the intuition.

## Questions

Q1: the notion $w$ is repeatedly used In line 259 when defining features and in line 284/285 to refer to the optimization variable (the entries of the classifier vector), which seems a bit confusing. The reviewer can understand that $w$ represents the strength of a corresponding feature, but why does it appear twice here?

Q2: In Proposition 3, the conclusion from the authors is that the OOD accuracy of ERM objective is ""much worse than random guessing"", while clearly, this is not the case in reality as we can observe from all the empirical results in the manuscript. Can the authors say more about the discrepancy between the theory and practice to explain this gap?

Q3: since the proposed method involves adding additional regularization on loss functions, the regularization strength becomes a tuning parameter. Do the authors conduct hyperparameter tuning for all tasks to find the suitable parameter?"
79,"## Paper summary

The task of predicting whether a reddit user who first starts posting on Anxiety subreddits will later also start posting on ADHD subreddits is used as a proxy for identifying people with anxiety who might also have ADHD. Classification performance of a fine-tuned RoBERTa model is presented which is shown to be better than keyword based baselines. Some explainability experiments are promised in the future.

### Strengths
1. Misdiagnosed comorbid ADHD is an important issue.
1. Data collection and processing is sound -- the 6 month gap in user postage history between their first post in ADHD subreddits and the data gathered from anxiety subreddits is a reasonable choice.

### Weaknesses
1. The authors have acknowledged the weaknesses and assumptions in the proxy task -- subreddit posting behavior is a very weak link to whether the user actually has a high risk of having comorbid ADHD. There seems to be no method for verifying the link between this proxy task and actual comorbid ADHD.
1. The practical benefits of how the proposed study can better enable diagnosis of comorbid ADHD in the future is not discussed.

### Feedback to authors
* In order to refine the ground truth for the proxy task, a Chat-GPT or equivalent LLM can be used to query whether the user believes that they have ADHD and/or anxiety from their posts alone. This may clean up the collected data significantly."
80,"The article presents a novel combinatorial model for non-linear dictionary learning and inference, focusing on an occlusion model for discrete objects in a 1D image. The authors introduce the concept of ""well-structuredness"" to ensure the uniqueness of components and demonstrate its sufficiency for learning and inference tasks. The work also explores adversarial robustness, providing insights into noise tolerance in combinatorial dictionary learning.

The article's strengths include its originality in proposing the combinatorial model and the well-structuredness property, its clear and concise presentation of the model and algorithms, and its significance in providing a theoretical framework for studying dictionary learning in non-linear settings. The authors provide comprehensive proofs for their claims and offer a reasonably detailed discussion of related work.

However, the work has some serious limitations. The authors acknowledge that their model is a simplification of real-world image generation and is currently restricted to one-dimensional images with quantized colors. The analysis also assumes specific conditions on object sizes and placement, which might not always hold in practice.
In addition, the authors do not discuss the tightness of the proposed bounds, particularly the exponential dependence on the number of objects in Theorem 15, nor do they provide lower bounds to support their findings. Including such discussion and providing lower bounds would significantly strengthen the results and offer a more complete understanding of the combinatorial dictionary learning problem."
81,"Quality and clarity:
- The writing is generally clear and easy to understand
- Methods seem to be clear and well-described for reproducibility

Originality and significance:
- Physician-guided LLM prompting seems original and highly relevant for this workshop
- Prompt examples in the appendix are quite interesting, as well as the performance breakdown per discharge summary field

Weaknesses:
- Figure 1: There is a black box around the cartoon doctor, please remove
- It would be interesting to see what the standard deviation would be under multiple generations of the same discharge summary, as the performance of chatgpt may change over time, but I understand the expensive nature of human-labeling
- 53 summaries were evaluated by 11 clinicians seems small. Also, do clinicians agree with each other in their analysis?"
82,"## Short Summary
The paper  introduces three methodological improvements to independently improve graph neural network based solvers for smoothed particle hydrodynamics. They introduce the problem of Smoothed particle hydrodynamics and introduce the improvements. They further present results on long roll outs of known datasets and conclude that their method is working. 

## Fit to Venue
I think the topic the paper addresses is relevant and interesting. It is a good fit for this venue. The problem is well explained and the motivation is well explained.

## Issues of the Paper
However, I have several issues with this paper. 

### Organization and Structure of the Paper
It is difficult to understand this paper if you only read the paper and do not read the appendix. This is because the authors stuck to the four page limit by moving integral parts of the paper, such as the explanation of their improvement into the appendix. (Appendix D). This makes the paper difficult to read. Further, it makes it difficult to link the methods to their results as some of the references to methods can be only understood after reading the appendices (8 pages).

The concluding remarks are quite general and it is unclear to me how the statements therein are conclusions of the experiments.

### Evaluation Method
More importantly, the representation of the results is strange. 
First, the authors claim that their results support that the methods presented help in long roll outs, but the metrics presented average across the whole time series instead of the final parts of the time series that would be much more relevant towards the claim.

The authors state that duet tho the test being carried out across only a small number of test trajectories the performance estimates are noisy. However, they proceed to afterwards state the results without any measure of uncertainty or variation between different trajectories. I think the informativeness of the results would profit massively from providing rages or uncertainties.

The authors present three improvements (p,g and v) that, to my understanding, can be applied individually. The authors then go on to present some of these improvements on some test sets. On no test sets did the authors present all eight combinations. Additionally, all presented results match or outperform the baseline. This, in combination with the statement ""The viscosity term is shown only for [...] because it didn't improve the performance on the other datasets."" could generate the impression that the authors, also for the other methods, do not present their full results but cherry-picked results to erroneously conclude that the method would improve performance consistently. I strongly recommend that the authors avoid the possibility of this impression by explaining the selection of test runs extensively or by redoing the test runs that would complete the experiments.

## Summary
In summary I think the paper is difficult to understand in the current state as relevant information is in the appendix. Further, the choice and presentation of the evaluation method seems sub optimal and is missing critical information. While I think that the method might be interesting to this community whether or not it improves performance. I think the paper does not really stick to the four page limit and the evaluation is not suited to determine whether the method does or does not improve performance. 


## Misc.
I think V in (2) is refferenced as U in the text"
83,"This paper improves the efficiency of training hp-VPINNs. This is achieved by vectorizing computations over the hp-VPINN’s cells when computing the hp-VPINN loss function, allowing tensorized GPU operations to be used. The authors claim that this approach is orders of magnitude faster to train than PINNs and standard hp-VPINNs.

In general, reducing the training time of hp-VPINNs is important for them to be useful in realistic applications, and so the subject of this paper is well motivated. It does appear that the authors achieve considerable speedups compared to PINNs and standard hp-VPINNs.

There are some weaknesses that should be addressed:

-	This paper lacks a mathematical description of hp-VPINNs, their trial/test functions, and their loss function - such a description is essential to include. Furthermore, it is unclear whether the proposed approach uses a global trial function (like the original hp-VPINN paper) or local trial functions as well as local test functions – this should be clarified -  and the authors do not mentioned which test functions are used – this is essential to include.

-	It would be useful to have a mathematical explanation on how the bilinear transform allows vectorization, in the context of the hp-VPINN loss function.

-	The authors claim “the existing hp-VPINNs code scales exponentially as the number of residual points increases, whereas the time required by our framework remains largely constant”. This sounds unrealistic to me – what is the actual computational complexity of evaluating the hp-VPINN loss function and how does it scale with the number of residual points? This would be more convincing if stated mathematically (big O notation). Furthermore, I am unsure why the proposed approach is faster in terms of time/epoch than PINNs, because PINNs already use tensorized operations and are evaluated using the same number of residual points – more explanation would be useful.

-	In places the paper needs more proof-reading; please check for typos (e.g. Figure 4 I am not sure the subplot labels should be h-refinement and p-refinement, and in the introduction “where most implementations loop calculate the”)

In summary, the motivation is sound, but the paper lacks some essential implementation descriptions which limit its reliability in my opinion."
84,"This paper explores three methods for generating ensemble predictions with FNOs: 1) input perturbation, 2) training for multiple outputs via a statistical loss function, and 3) using Laplace approximation for Fourier layers for getting ensemble of weights. Empirical studies are conducted on a simple one-dimensional PDE dataset.

Pros
1. This paper is well organized. The outline of this paper is clear and easy to follow.
2. The empirical studies clearly demonstrate the advantages and the disadvantages of different ensemble methods.
3. The proposed FNO-FL is capable of providing a full probability measure on weights for uncertainty quantification, instead of approximation from generations.

Cons
1. ""short run"" and ""long run"" is defined by the validation RMSE. However, since there is no information about the convergence, it is hard to judge if the ""short run"" is too far from convergence and hence less meaningful. Directly comparing the convergence speed can be a good alternative.
2. The validation RMSE are 0.4 and 0.07 for ""short run"" and ""long run"" respectively. However, the training RMSE are around 0.3 and 0.11, and the test EMSE are around 0.27 and 0.1, which show different trends compared with the validation RMSE. The dataset split may cause severe distribution mismatch among training, validation and test sets."
85,"# Review to 17
Strength:
- This paper studies the problem of generating medical data while preserving privacy. This is an interesting and important problem.
- The idea of combining Bayesian optimization and privacy-preserving generative models to generate informative and differentially private data is novel.

Weakness:
- The optimization specification and pseudo-code is a bit hard to follow. Here are some questions/problems: 
-- definition of $K$
-- I assume $\mathcal{X}_i$ is the $i$-th dimension of the input space. Is my understanding correct? If yes, please define the notation.
-- When sampling from $[\alpha_i, \alpha_i+\beta_i]$, I assume a uniform distribution is used. Is this correct? If yes, please specify.
- The empirical results are not so impressive. 
-- It seems that the data augmentation, both standard and PriSHA, can only improve the AUC marginal (85.30%);
-- When DPGAN is selected, the benefit of including Bayesian optimization, which inevitably increases computation complexity, is not so significant (only 2 out of 8).

Problems
- The benchmark with 88.87% AUC is claimed to be obtained using $D_{AB'}$. Is this distribution $D_{AB}$?
- Could you provide some details of the used dataset? For instance, how many samples are included? What is the ratio of different ethnical groups?"
86,"This is an interesting demonstration of an application of foundation language models cost-effectively fine-tuned to a clinical task and performing impressively, especially compared to language models without fine-tuning.

However, it is hard to follow for someone like myself with machine learning but not medical expertise. For instance, I don't know what TNM or triple-negative mean, and my lack of familiarity with medical reports make the problem and the description of data preparation, which are important to understand the implications of the results, hard to understand. In the results, it is unclear what the difference is between the sample count indicated in the model column vs the samples column. If the former is the number of samples used for training, then the claim that low sample count is sufficient for training seems unsupported, as the accuracy is substantially higher with greater sample count. Also, if possible, it would be helpful if the results included some model trained only on the cancer data so we can tell how much is gained by starting with a foundation model. Further, what are the practical implications of these results? What would be the impact of deploying this model in particular?

A small discussion of prior work in fine-tuning foundation models for clinical tasks and the gap that this work fills could help contextualize this work and understand its contribution. Much of the last page is speculative, which is, I think, less valuable than further supporting the experimental study - the main contribution of this paper - with more details as mentioned above."
87,"This work is well written though it could have presented more information on the approach. And could have done a better comparison against other similar approaches to show any sort of improvement over competing approaches. The question why is this approach better than the current state of the art was never quite answered. Just because you can do something doesn't mean you should do it. 

Neural Radiance Fields or other modern representations could also be mentioned for comparison, since point clouds have been improved upon. 

I don't know what their page count limit was, but the paper could be extended to six or eight pages to show the USD representation and the final neuro-symbolic representation of a single processed image, to further highlight what specifically is gained by taking this approach.  Also the reduction in space could be discussed, i.e., how much space is required by the original point cloud representation versus how much space is required by the final neuro-symbolic / USD representation. 

What was glaringly absent from this discussion was the amount of time taken to process a single image. No timing information was presented at all, which is concerning.  

The fact that a library of object images needs to be pre-constructed as well is also concerning. It is not mentioned where this library comes from. 

For the [near/far] future, it would be nice to generate the matched object library directly from the scene itself. When the object library becomes enormous, how costly will it be to match extant known objects against a new scene that is being processed in real-time, (24 frames per second).   Obviously the matching should be sub-second. 

Despite these shortcomings of the current paper, I think that this investigation is certainly noteworthy. 

The paper should be revised to address the shortcomings mentioned above."
88,"This paper introduces a first-order sampling method termed the Metropolis-adjusted Preconditioned Langevin Algorithm (MAPLA) for approximate sampling from a target distribution $p(x) \propto e^{-f(x)}$, where the support is a proper convex subset $\mathcal{K}$ in $\mathbb{R}^{d}$. MAPLA extends the Metropolis-adjusted Langevin Algorithm by incorporating a preconditioner matrix $G$ into the Langevin dynamics update, followed by a Metropolis-Hastings acceptance/rejection step. The authors derive a mixing time bound under standard conditions on $f$ and $G$.
However, my primary concern with this paper is its lack of novelty. The results closely mirror those of Srinivasan et al. (2024), who analyzed the Metropolis-adjusted Mirror Langevin Algorithm (MAMLA) and provided mixing time bounds under standard assumptions on $
f$ and the mirror map. While MAPLA offers a different perspective by using a preconditioning matrix $G$, the paper does not adequately address critical questions such as how to choose $G$, when MAPLA should be preferred over other methods, or the algorithm’s practical advantages.

Moreover, the proof techniques employed here largely follow those of Srinivasan et al. (2024) without improving upon the existing results. Although the paper discusses two additional cases involving specific choices of $G$ and $f$, these cases are not sufficient to demonstrate substantial novelty. The general results remain comparable to those of MAMLA, differing primarily in how smoothness and strong convexity are defined—in terms of the mirror map for MAMLA versus the preconditioner matrix $G$ for MAPLA.

Given the limited contribution and the lack of a clear argument for the advantages of MAPLA over existing approaches, I cannot recommend acceptance at this time."
89,"This paper presents an interesting and impactful approach to improving drug recommendation systems and reducing drug drug interactions.

I would be happy to raise my score if the weaknesses and questions are addressed. 

Strengths: 
1.  The paper is well-organized and was very enjoyable to read.
2. The idea appears to be novel and methods are thoroughly explained
3. Figure 2 is a clinically relevant and easy to interpret example that demonstrates the utility and transparency of this approach to assist healthcare workers. It would be nice to see a similar example of how DDI are assessed for the predicted medications.

Weaknesses:
1. Potential harm of drug-drug interactions and expected benefits of drug recommendation systems are not discussed. A couple sentences would suffice. For example, statistics regarding prescription errors and effects would help emphasis the importance of this research area. 
2. Paper does not explain novel contributions compared to prior work, particularly longitudinal drug recommendation methods that consider the effect of DDIs. I am assuming it is the use of cross-attention, transformer architectures, and a cycle-embedding module for healthcare but this should be explained as gaps in prior work. New loss function could also be referenced as a contribution. 
3. Results demonstrate impressive gains over baselines but do not include confidence intervals. Including averages across multiple runs or random seeds is important to convey significance of results. If there is a reason this is not feasible, then that should be mentioned.
4. Diagram is fairly clear but duplicate inputs and labels, such as for 'drug_embd' and 'sym_embd', in the drug transformer and symptom transformer is confusing and makes diagram more difficult to parse than it need be. I would recommend reworking the figure to reference common inputs once outside of the modules. Also there are two colors for drug_embd used (blue and pink). One color should be used for the same input. The acronyms and colors should be explained in the image description.
5. DDI rate is not explained concretely. This is a very important term in pharmacology so more information regarding how the DDI detector is crucial.
6. More information regarding the classification task is needed. For example, use of the precision metric implies some score threshold is set to convert probabilistic outputs. 

Questions:
1. How is DDI rate measured? The author mentions that ""A lower DDI often implies the set of drug combinations should be as small as possible"". Is set refer to a specific combination or number? Drug drug interaction rate should measure the extent to which drugs interact and not the number of drugs. Assuming the number of drugs given is a proxy for drug interaction rate is not a valid assumption. Can the author elaborate on their assessment of DDI and provide references if possible?

Other:
1. It seems that predicted drug predictions learn from historical data/prescriptions. It would be interesting to see extensions of this work that also consider patient outcomes in the prediction. The model may suggest alternative medications not administered, although these predictions would need to be verified by medical professionals."
90,"The paper introduces GUARD, a diffusion model which is trained on short PDE trajectories and aims to forecast longer trajectories and perform data assimilation given sparse spatial data. 

Pros 
- The approach and comparisons are interesting, experiments are well-described. 
- The results presented seem promising (especially for forecasting) and should be investigated further.


Quality: The quality of the paper is good. \
Clarity: While the paper is written clearly it sometimes lacks in logic as crucial parts are omitted/in the appendix. \
Originality: The basis of GUARD is AR (Ho et al.) and the distinction to this work remains unclear. Due to a lack of conclusion it is also not clear what exactly the authors claim to be new.  \
Significance of this work: The results presented are promising. However, to determine the significance a more elaborate benchmark would be needed. 

Major issues
- The paper lacks a discussion and conclusion. Overall the background section and description of the setup take up most space of the paper and sections on the experiments performed and their results remain very short. Therefore, many things seem to be omitted and the appendix is referenced often. 

- The experiments performed for data assimilation and forecasting are not comparable to each other. It remains unclear why for data assimilation only the two approaches AAO and AR are compared while for forecasting GUARD is compared to an amortized model. Also, the amortized model is not described and there's a lack of further benchmarks.


Minor issues 
- Math equations are difficult to read when inline. 
- Error metrics: Why are some evaluations performed using MSE and others using RMSD? 2a vs 2c. 
- Runtime comparison of GUARD and amortized model?
- Conditioned on less states the amortized model performs better than GUARD (Figure 2). Why? Could this be a reason to use the amortized model in some cases and GUARD in others?"
91,"This study introduces a weighted loss function based on a gradient graph to improve fairness without needing demographic data. Theoretical results provide the intuition of the method, explaining how the correlation in gradients is related to unfairness. Empirical results show the effectiveness of proposed methods.

Strengths
- Theoretical derivation of method illustrates how it works well
- Empirically, the proposed method yields superior results to other methods"
92,"This paper studies the question of learnability of stochastic bandits for the PAC learning framework. The noiseless setting was studied in earlier work [Hanneke and Yang 2023] and showed that learnability can be undecidable in that case. Formally, a function class $\mathcal F$ from arms to rewards is learnable (for the noiseless case) if for any tolerance, there is a finite horizon $T$ and some algorithm that finds a near-optimal arm with high probability using $T$ arm queries, for any reward function within the function class $\mathcal F$. 
In the main stochastic case they consider, the algorithm should find a near-optimal arm (up to $\alpha$) for any stochastic rewards within $[0,1]$ such that their mean reward function is within the function class $\mathcal F$ (realizable). They provide a characterization of learnability for stochastic bandits in terms of quantities $\gamma_{\mathcal F,\alpha}$ and characterize the range of possible optimal rates depending on this quantity (basically between $\log 1/\gamma_{\mathcal F,\alpha}$ and $1/\gamma_{\mathcal F,\alpha}$. They also provide an alternative characterization in terms of a variant of DEC.
Last, they show that this quantity also characterizes learnability when rewards may be unbounded but are supposed to have a bounded variance. 

The question is definitely of interest and closes an open question from [Hanneke and Yang 2023]. In particular, while in the noiseless learnability is undecidable, for the noisy case there is a very simple characterization, which (almost disappointingly) essentially states that a good algorithm is the following: given a good prior on arms (existence of which is guaranteed by the characterization), simply test random samples of arms according to this prior and output the best arm observed. 
The writing of the paper (english and clarity) could be largely improved, however, specifically starting from section 3. The ideas are quite simple but this made reading some of the proofs impossible. See below for some more detailed comments/questions. I am also confused about the utility of Section 5 which gives a DEC-variant as characterization. I understand the goal of relating the results to the DEC literature but I am not sure which insights this variant provide on the problem compared to the first characterization. It is a lot more complex, requires a significant amount notations and definitions (their presentation is also not the clearest), and hides the simple structure that was revealed by the first characterization (that there is a single good prior on arms).

Minor and detailed comments:

- p4 second paragraph of the proof of Thm 3, sentence starting with ""We construct a distribution..."" has grammatical issues.

- proof of Thm 4. The writing is quite confusing. From what is written, I imagine that we have fixed $\gamma\in\{1/i,i\in \mathbb N\}$ and I follow the proof replacing all occurrences of $\gamma_{\mathcal F,\alpha}$ with this $\alpha$ (it does not make sense to define the tree in terms of $\gamma_{\mathcal F,\alpha}$ while we didn't even define $\mathcal F$ yet). The actual link with the quantity $\gamma_{\mathcal F,\alpha}$ is not given within the proof. I imagine the argument is simply that the optimal prior $p$ on arms is the uniform on all arms in the leaf buckets, which is $N/\gamma$. In that case, I get $\gamma_{\mathcal F,\alpha} = \gamma/N$. This changes the parametrization of the example a little bit (although the final result is the same).

- p6 ""The query of the adaptive"" -> ""The query of an adaptive""? Similarly for the whole paragraph.

- p6 Proof of Thm5 ""for non-adptive algorithm"" -> add ""s""

- p7 Proof of Thm 5, what is the point of the extra $1/10$ in the last paragraph of the proof? Shouldn't this be $1-2/5$ the complementary event of the one on which $N\geq 2$?

- The whole proof of Thm 7 is very hard to read. In addition to grammatical mistakes, a few notations are not defined when used. What is $F$? What is $n$? Is it not the number of rounds $T$? What is $G'$ in the last paragraph?

- Why do we need to use Thm1 in Corollary 8 if the statement is about the unbounded reward case?

- Algorithm 3: line in the middle of the exploitation phase. When defining $\gamma$, this uses $f^\star$, but we do not have access to it, correct? Next line, what is $p$? Is it $\hat p?$

- p9 proof of Thm11. What is $\mathcal H$?"
93,"This work proposed an interesting framework for evaluating clinical LLMs. I personally would feel this is a more relevant evaluation framework for real world use cases than traditional benchmarks such as MedQA. The study demonstrated promising capabilities of the framework and revealed limitation of current LLMs in clinical reasoning (at least in the setting without domain-specific in-context learning). However, understandably due to the context length limitation of the non-traditional track, several important technical details are unclear to me. For example, what are the foundation models used in Grader-AI agent, and what's the outcome of expert evaluation?"
94,"The paper proposes a novel generative DL approach to reduce structural vibrations in plate-like structures using a guided diffusion design optimization framework. By integrating a denoising diffusion generative model with a surrogate model for vibration prediction, the authors propose a method that generates design patterns with significantly reduced vibration energy. 

Pros:
- The application of guided diffusion models for structural design optimization in traditional engineering is innovative. 

Cons:
- While the audience may be interested, the paper's contribution may not be directly relevant to the workshop topic. It seems that the differential equations are only used to generate datasets, rather than directly guiding or benefiting from the model.

Questions/Comments:
- May need to justify the beading patterns (i.e., up to 3 lines and 2 ellipses) in the dataset and the applicability of the approach to other scenarios (e.g., possibly more complex patterns in realistic).
- The methodological steps involved in combining these models for design optimization are not sufficiently detailed to be easily understood by a reader unfamiliar with these techniques. For example, why are ""score-based and denoising-diffusion based"" generative models used? How alpha and beta were determined and adjusted in the guidance?
-  The advantages and limitations of the method are not clear, as there is no detailed analysis comparing the proposed approach with traditional design optimization techniques.
-  At least pseudocode or a more detailed description of the algorithm within the paper would allow for a better preliminary evaluation."
95,"The paper proposes using conformal prediction in the context of physics-informed neural networks (PINNs) for uncertainty quantification.

It is clearly written and well structured, and the results seem novel. 

My biggest concern / question is regarding the computational cost of the method: The first experiment is made on the very simple logistic ODE, but PINNs are typically used in the context of PDEs. Is there something holding back the method from being applied to more complicated problems? This is not explicitly discussed in the paper, but it would be helpful so that readers can properly assess the utility of the proposed method. Additionally, it is good to see that the method can also be used for uncertainty quantification in parameter inference, but to the best of my understanding the proposed algorihtm does not seem very practival, as it requires many simulations, and training many different PINNs for the individual tasks; but this is acknowledged in the conclusion.

Overall, I think the paper is a good contribution to the topic of uncertainty quantification for PINNs and I therefore recommend its acceptance."
96,"This work provides the nearly optimal regret upper bound $O(d\sqrt{n}log(n))$ for the (unmodified) randomised exploration algorithms (including linear TS). More specifically, their algorithm does not rely on the optimism framework, which introduces the challenge in the analysis process. To overcome this problem, this work carefully analyzes the confidence width during the algorithm process and shows that pure randomised exploration is enough to control the error terms.
Pros:
1. This work provides the first nearly optimal upper bound for the (unmodified) randomised exploration algorithms, which is meaningful.
2. The intuition of why randomised exploration algorithms work (Figure 1) is clear.

Cons:
1. Though the smooth and strongly convex assumption is stronger than previous work, I think it is meaningful to analyze the linear TS algorithm. However, it would be better to discuss the role of these two assumptions. For the strongly convex assumption, it seems to be used in Lemma 5. For the smooth assumption, it seems to be used in Eq. (2)."
97,"The paper introduces innovative prompting techniques and demonstrates their effectiveness in improving the performance of the general language model Yi-34B over a clinically fine-tuned LLM, Meditron 70B, across three out of four medical Q&A benchmarks: MedQA (4 Options), MedMCQA, PubMedQA, and MMLU - Medical.

**Pros**

1) **Clarity of paper**: The paper is well-written and easy to follow. Figure 1. showing LLM performance with time is good. 

2) **Good improvements and ablations**: The results with different prompting techniques are well demonstrated in Figure 2 and Table A1.


**Cons**

1) **Unavailability of training dataset for KNN FS CoT (minor)**: Since this requires access to train set samples, it is a costly process, and training data won't always be available for many models. I do understand the point of open source LLMs which the paper advocates. But consider medical domain datasets that are sensitive to release. Hence the authors are not able to show results of Meditron 70B on kNN FS CoT. This is to be discussed in limitations.

2) **Experiment baselines**: The exclusive focus on Meditron 70B as a baseline leaves the comparison somewhat narrow. Including at least one additional medical LLM baseline would offer a broader perspective on the presented techniques' relative performance.

3) **Missing Proper References**: The paper lacks references for the various prompting methods (CoT, KNN prompting, etc.) utilized in the study. These missing references raises following two questions:

4) **Novelty**: : It is unclear whether the use of KNN with training samples is a novel contribution of this paper or if it has been previously used.

5) **How are similar questions selected for kNN FS CoT?**
I didn't find how similar questions to test examples are chosen via KNN in the paper. It is done in which space (embedding space of model/input space). Please explain it properly.

6) **Limitations of kNN FS CoT and other prompting strategies:**  The potential for prompting strategies, such as kNN FS CoT, to overemphasize reasoning patterns from similar training samples needs further discussion. Highlighting this and other limitations of the proposed methods would provide a more balanced and comprehensive view of their applicability.

7) The clarity of Figure 2. can be improved with the usage of other shades of colors than green for more clarity.

The paper presents evidence that innovative prompting strategies can enhance the performance of open LLMs compared to fine-tuned counterparts in the medical domain. However, addressing the aforementioned concerns would strengthen the paper."
98,"## Summary 
Flat Minima has long been hypothesized as an indicator for good generalization. Recent empirical results (such as sharpness-aware-minimization) observe that learning algorithms that encourage flatness indeed benefit from better generalization properties. This article investigates the formal link between flatness and generalization. The core contribution of this article is a generalization bound via PAC-Bayes that directly involves flatness terms (via the norm of loss gradients). This article identifies qualitative properties of the posterior that enable fast $\mathcal{O}(\frac{1}{m})$ “transitory” fast-rate generalization bounds (Theorem 6) which indicates that the number of samples needed to guarantee a certain $\epsilon(Q,S_m)$ upper bound on the test error can be fast. Here the $\epsilon(Q, S_m)$ depends on certain properties of the posterior, the data distribution and the empirical risk and in general can be bounded away from zero due to the dependence on loss gradients. In this way, the main contribution of this article is a sufficient condition on the learnt posterior (that depends on the data distribution) that then carries the implication : flatness => good approximate generalization. To obtain an overall test error of $\epsilon$ (rather than the approximate generalization as above) one still needs $\mathcal{O}(\frac{1}{\sqrt{m}})$ samples as noted by the authors.

## Strengths 
1. Theoretical frameworks that allow the characterization of multiple phases of generalization are insightful and further our understanding beyond uniform convergence. 
2. The presentation and the results are intuitive. 


## Weakness
1. Some of the theoretical results require heavy assumptions on the data. These assumptions have classical flavor but the resulting bounds still depend on the expectation over the data. It is unclear if a learning algorithm can guarantee minimal expected sharpness w.r.t parameters on the data distribution as opposed to just minimal sharpness w.r.t parameters at the observed data points. While it is indeed true that algorithms such as Sharpness aware minimization can output posteriors with minimal norm of gradients, such a property is usually only on the training data. Indeed, the extension of such flatness phenomena from the training data to the wider data distribution requires further justification just like the generalization of low empirical risk to low population risk! In Nagarajan et. al and Muthukumar et. al. (see list of references below) for example attempt to generalize flatness from training data to flatness in test data in the context of classification by incurring additional terms in the generalization bound (just like Theorem 8 but with lesser assumptions in a specific setting). Hence, the dependence on expected sharpness over the data distribution in Theorem 6 is a weakness. On a related note, Theorem 6 and Corollary 7 are not empirical - i.e. they cannot be computed or estimated in practice as they depend on assumed properties of the data distributions, the hypothesis class or combinations there-of. Still the spirit of the results developed by the authors indicate the broad links between flatness and generalization. The authors note similar implications in the discussion above Theorem 8 which attempt to alleviate this drawback using additional assumptions on the sensitivity of loss gradients. 
2. Much of the result depends on the KL divergence which requires absolute continuity between Q and P. Theorem 14 circumvents this requirement by employing Wasserstein divergence instead. However the assumptions in Theorem 14 appear quite strong - a favorable conditioning of the prior and the data distribution (via $sigma^2$) is assumed and further the generalization gap is assumed to be G-gradient Lipschitz. It is unclear to this reviewer whether such an assumption is reasonable, I urge the authors to supplement their discussion with an example if possible. Further, the bound on the generalization gap depends on the expected norm of the gradient of the generalization gap!  The impact of such a result is unclear. For e.g, is assuming that the generalization gap is Lipschitz significantly stronger than assuming that their gradients are Lipschitz? If not, then I can assume that the generalization gap is itself Lipshcitz and use any predictor $h’$ for which $R_D(h’)$ is known to directly obtain a bound on risk $R_D(h)$ since $|R_D(h)-R_S(h)| \leq |R_D(h') - R_S(h') + L|h-h’|$. Please let me know if I have missed a trivial complication that validates the context of Theorem 14. 
3. At several points authors make claims on novelty regarding the usage of gradient norms in generalization theory. A direct quote “We highlight that the gradient term in Theorem 6 is derived with respect to a predictor $h\in \mathcal{H}$ and not $z\in \mathcal{Z}$ which is, to our knowledge, novel in PAC-Bayes” 
In my humble opinion, these claims are a slight over-representation of the contribution. In particular, there is a long-standing tradition of developing bounds on the generalization error based on sensitivity measurements i.e., Lipschitz constants all of which are themselves bounds on the norms of the gradient w.r.t parameters across the data domain. In particular, I note that Nagarjan et.al (2019), Banerjee, et.al. (2020) and Muthukumar et. al. (2023) all satisfy the following constraint (1) Use PAC-Bayes as the proof-vehicle, and (2) Show bounds that rely on gradient norms or Lipschitz constants. In each reference, flatness (or sharpness) arises when one derandomizes the PAC-bayes bound from the posterior to a bound holding on the mean predictor $mu_Q$. Such a bound naturally relies on (a) the concentration of the posterior $Q$ around the mean $\mu_Q$, and (2) the sensitivity of loss $\ell$ upon perturbations to the mean predictor. In my opinion these results are comparable to the developed results and this article can benefit from contextualizing them. Banerjee et. al. (2020) in particular appear to link generalization with the norm of the loss Hessian w.r.t parameters. Additionally Bartlett et. al. (2017) and Wei et. al. (2019) bound genearlization error using Lipshcitz constants albeit via a different popular proof vehicle - Rademacher complexity. Wei. et. al. (2019) in particular account for the Jacobian norms of the predictors for general computational graphs.  All the above references utilize sensitivity w.r.t perturbations in the weights directly or indirectly and thus the claim of novelty of using gradients w.r.t. parameters should be amended. 
4. The authors claim that their analysis focuses on loss gradients w.r.t parameters rather than loss gradients $\nabla_z \ell(h,z)$ w.r.t data as $\nabla_z \ell(h,z)$ cannot be optimized since data is fixed. This statement requires more context or nuance. In particular, there is much to be gained from learning predictors that are flat within neighborhoods around the data points, i.e. low norm of gradient w.r.t data. Indeed the field of adversarial robustness seeks such predictors and often predictors that are robust to input perturbations generalize better (subject to additional technical nuances). Thus, I think, this particular statement should perhaps be amended to contextualize this work appropriately and indicate the subtleties of seeking flatness w.r.t perturbations in the data domain vs flatness w.r.t perturbations in the parameter domain. 
5. The experimental design is incomplete in my opinion. The authors estimate the expectations over the posterior with a single random sample and the expectation over the data with a mini-batch of 512 examples. I think the bar for experimental evidence should be higher, especially for smaller datasets such as MNIST. For e.g. Ujvary et. al (2023) conduct a Hamilton Monte-Carlo sampling to estimate the same posterior. I would suggest that the authors refine the experimental evidence.





## Technical clarifications
1. Quadratic self-boundedness (Assumption 5) is noted as a relaxation of boundedness, can the authors supplement the article with an example where QSB materially improves the analysis, i.e. either (1) A task where using bounds on the loss function or the expected-variance assumption results in a worse bound? Or (2) A task where QSB is necessary because boundedness/expected variances is too strong a requirement. 
2. “Theorem 8 not only introduces the first PAC-Bayesian bound involving gradient terms, but it can be transformed into a generalisation metric” , what is a generalization metric and how is it different to just a bound on the generalization error?
3. “Note in this case that, while the KL divergence has an explicit formulation, it requires calculating the exponential moment which is costly in practice. On the contrary, we only need to estimate a second-order moment over the Gibbs posterior” It is unclear why the latter is easier, could the authors expand on this?
4. As a reader, I struggled to see the benefit result (2) in Theorem 11 provides over the basic result of (1) which as I understand is the PAC-Bayes bound for the Gibbs Posterior + bound KL divergence of Gibbs Posterior from Lemma 10. In particular (1) already involves the gradient of the empirical risk w.r.t parameters, what is gained from the additional assumptions in (2)?




## References
1. Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. (NeurIPS, 2017)
2. Vaishnavh Nagarajan and Zico Kolter. Deterministic PAC-bayesian generalization bounds for deep networks via generalizing noise-resilience. (ICLR 2019)
3. Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz augmentation. (NeurIPS, 2019)
4. Arindam Banerjee, Tiancong Chen, and Yingxue Zhou. De-randomized PAC-Bayes Margin Bounds: Applications to Non-convex and Non-smooth Predictors. (Arxiv 2020)
5. Ramchandran Muthukumar, and Jeremias Sulam.. Sparsity-aware generalization theory for deep neural networks. (COLT 2023).
6. Szilvia Ujváry, Gergely Flamich, Vincent Fortuin, José Miguel Hernández Lobato Estimating optimal PAC-Bayes bounds with Hamiltonian Monte Carlo. (NeurIPS 2023)"
99,"**Summary**

The paper explores approaches to minimize the inconsistencies in the LLM generated texts without fine-tuning. Authors discuss some approaches for representing semantic and lexical constraints and how to enforce these constraints during text generation. Experiments on CommonGen dataset shows the benefit of combining prompt engineering and constrained decoding techniques for generating text in LLMs that meet provided requirements

**Strengths**

1. Interesting discussion on enforcing semantic and lexical contraints at various stages of text generation without requiring the computationally expensive fine-tuning
2. Insightful ablation studies on CommonGen dataset

**Weaknesses**

1. The contributions of the paper are not expressed well in Sections 2, 3 and 4
2. The experiments conducted in Section 5 do not cover the ideas discussed in previous sections especially semantic constraints or repairing output sequences
3. Proposed high-level declaratives constraints (prompt engineering) and combining that with decoding techniques is not technically novel"
100,"## Summary
This paper examines whether Math-LLMs are able to perform consistently on different math problems with similar solutions. The research is conducted on three well-known math-LLMs: Qwen-Math, Mathstral, and DeepSeek-Math. The testbed is a set of probability word problems. These problems are annotated with formal representations. The author conduct experiments to explore whether Math-LLMs could treat similar problems similarly and capture mathematical semantics in these problems.

## Pros
- Regarding the research question. It is known that LLMs are black-box models. This paper provides a new way to uncover the inner mechanism of LLMs by questioning whether they behave consistently on semantically similar problems.

## Cons
- The organization of this paper could be further improved.
    1. Typo: in subsection{Models}: Qwe2.5-Math --> Qwen2.5-Math
    2. The experiment procedure is only depicted in Figure 1. However, the reviewer believe it is important to elaborate on the details of the conduction. 
    3. The authors should further explain why we can use the embedding similarity to measure semantic similarity.

The reviewer would support its acceptance to the workshop if the author could properly address Cons 2 and Cons 3"
101,The paper is in the direction of creating foundation models for PDEs. I have no comment as the paper is nicely written and has great results.
102,"**Pros:**

- **Advancement in LLM Evaluation:** CRAFT-MD represents an advancement in Large Language Model (LLM) evaluation, moving beyond static exams to capture the complexity of real-world clinical conversations. This is particularly relevant as LLMs are increasingly being explored for healthcare applications.

- **Scalability and Ethical Considerations:** The use of AI agents for simulations offers scalability, ethical considerations, and control over conversation flow, providing a more controlled environment for evaluation.

- **Comprehensive Evaluation:** CRAFT-MD assesses various aspects, including history gathering, information synthesis, and diagnosis under different conditions, offering a comprehensive evaluation framework.

**Cons:**

- **Comparison with RAG-based Models:** Given the evolving landscape of clinical LLMs, it is suggested to explore the applicability and effectiveness of CRAFT-MD when used against models based on Retrieval-Augmented Generation (RAG). Including a comparative analysis with these models would offer valuable insights into the framework's performance relative to different approaches in the field.

- **Limited Generalizability:** The study focuses specifically on  two LLM models, potentially limiting its generalizability. Further exploration across a wider range of LLMs would strengthen the paper's applicability to broader contexts.

- **Need for Balanced Perspective:** While highlighting LLM limitations is valuable, a more balanced perspective could be achieved by also exploring potential strengths and areas for improvement."
103,"* Summary

This paper addresses amodal instance segmentation (AIS), which aims to recover wholes from occluded parts in images. AIS methods typically leverage shape priors for segmentation. However, prior works often rely on shape priors learned from a limited number of unlabeled masks, which are prone to overfitting and neglect class information. To address this, the paper proposes using text-to-image (T2I) diffusion models trained on a large-scale dataset with semantics. The proposed method infers class and condition it into the T2I diffusion to obtain shape priors, which are used to generate final AIS masks. This approach outperforms prior works not using diffusion priors.


* Strength

The 3D or shape prior abilities of T2I diffusion models have been reported in many research papers. Therefore, it is reasonable to explore how to properly utilize the information for the AIS problem.


* Weakness

The baseline methods are outdated; all of them are from before 2022. There are a bunch of works that utilize diffusion priors for the AIS problem [1, 2, 3]. The paper should compare with those approaches both methodologically and empirically.

[1] Ozguroglu et al. pix2gestalt: Amodal Segmentation by Synthesizing Wholes. CVPR 2024.\
[2] Zhan et al. Amodal Ground Truth and Completion in the Wild. CVPR 2024.\
[3] Xu et al. Amodal Completion via Progressive Mixed Context Diffusion. CVPR 2024."
104,"Summary:

There is empirical evidence that flat minima lead to better generalization in deep learning. This is often invoked to support Bayesian methods in machine learning. In particular, it was argued that flat minima will lead to tighter PAC-Bayes bounds [Dziugaite & Roy, 2017]. The objective of this paper is to go beyond heuristics, and to write PAC-Bayes bound that depend explicitly of the flatness of the minimum of the empirical risk. This is an excellent objective in my opinion. However, I'm not completely convinced by the outcome. After a very reasonable result (Theorem 6), the authors make some weird / approximative / misleading statements and loose space with unnecessary developments. On the other hand, they don't discuss key issues regarding how Theorem 6 can be useful in practice.

Major problems:

- page 5, Theorem 6: the result is nice and might lead to very exciting developments. But for the reader who wants to use these bounds, the first thing that should be discussed is: given a posterior $Q$, is it possible to check the conditions $Poinc(c_P)$ and $QSB(\ell,C)$?

- page 5: I find the discussion on ""transitory fast-rates PAC-Bayes bounds"" is misleading. The standard PAC-Bayes bound is in
$$ \sqrt{KL/m} \leq \inf_{u} \left[\frac{KL}{2u} + \frac{u}{2m}\right]. $$
Thus, by putting $u=2 m\epsilon$, we have
$$ \sqrt{KL/m} \leq \inf_{u} \frac{KL}{2\epsilon m} + \epsilon . $$
That is, all PAC-Bayes bounds are ""transitory fast-rates"". It's almost like a synonym for ""standard bound"". The difficulty for fast rates is actually to remove $\epsilon$. It is also weird that the authors cite [Alquier et al., 2016] as a reference for the $1/\sqrt{m}$ rate, which is simply the minimax rate (at least without additional assumptions on the loss) and can be found in all PAC-Bayes (and PAC) bounds.

- page 5: the discussion ""high-probability bounds with fast rates, a paradox?"" is in the same vein. First, I doubt that $\sigma$ is the variance of the dataset in Grunwald's statement. I think it is rather the variance of $\ell(h^*,z)$. And of course, it is known that if $\ell(h^*,z)=0$, convergence is faster than $1/\sqrt{m}$, so there is no contradition between fast rates and Grunwald's lower bounds. The whole discussion is useless anyway, as your ""transitory fast-rates"" bounds are actually ""standard rates bounds"" and do not contradict Grunwald's result.

- page 7: Theorem 8, there the empirical risk is multiplied by $2$. In the noiseless case, where $R_{\mathcal{S}m}(\hat{h})=0$, this is of course fine. But in the noisy case, this makes this bound very poor, because even when the sample size tends to infinity, you have no chance to recover the optimal prediction risk. It's OK to mutipliy $R_{\mathcal{S}m}(\hat{h})$ by a quantity larger than $1$, as long as this quantity tends to $1$ for large sample size. As it is stated, your result does even satisfy your definition of ""transitory fast rates"".

Frtextbfom the proof, this is due to the choice $\lambda=1/C_1$. It is necessary to allow $\lambda\rightarrow 0$ to get a non-trivial bound.

This problem is recurrent: Corollary 9 page 8, Theorem 11 (i) page 9.

- page 8: ""... Gaussian distributions, but this class of distribution is restrictive. Following the approach of Catoni (2007), we go beyond the Gaussian distributions to focus on Gibbs posteriors"". Gaussian distributions are feasible in practice, which is why [Dziugaite & Roy, 2017] used them for deep learning. Gibbs distributions might lead to feasible MCMC algorithms in simple models, but I doubt they are feasible in really deep networks. Of course, I believe it is interesting to study Gibbs posteriors as theoretical objects, but it's not fair to pretend that they are less limited than Gaussian. Actually, this leads to the question: can we apply Theorem 6 to Gaussian distributions, or are Gibbs distributions the only case where you can check the assumptions? This is one of the key that will make your results useful in practice. Thus, it should be discussed.

- page 10, Theorem 14: the term $W_2^2(Q,P)$ has no reason to go to zero, unless $Q\rightarrow P$? Or am I missing something? It seems to me you are adding a constant term to a PAC-Bayes bound, that makes it irrelevant.

Example: assume you have two classifiers $\mathcal{H}=\{h_1,h_2\}$, one of them is always right, say $h_1$, and the other one is always wrong, $h_2=1-h_1$. In practice we don't know which one is always right, so we take $P$ as uniform (a reasonable choice), and $Q$ is the point mass on the ERM, which will turn out to be $h_1$. Standard PAC-Bayes bound will give $R_{\mathcal{D}}(Q) \lesssim \sqrt{\log(2)/m} $. Fast rates-bounds will tell you $R_{\mathcal{D}}(Q) \lesssim \log(2)/m $. However, your bound will only guarantee $R_{\mathcal{D}}(Q) \lesssim constant = W_2^2(Q,P) = 1/2 d^2(h_1,h_2)$ where $d$ is the metric on your hypothesis space. This bound is not even able to prove the consistency of the ERM for 2 hypothesis in the noiseless case.

Is it really worth stating such a result?

- page 19: $(x_1+\dots+x_m)^2 \leq 2 (x_1^2 + \dots x_m^2)$ is wrong in general (take $x_1=\dots=x_m=1$ to see it). The best you can do is $(x_1+\dots+x_m)^2 \leq m (x_1^2 + \dots x_m^2)$. Thus, something in the proof of Corollary 19 seems wrong.

- page 20: ""these bounds are relevant for practitioners as they require little computational time"". Sampling from the Gibbs posterior is a computationaly difficult problem. Do you have a proof that you can sample from it using little computational time?

Minor comments:

- page 1: ""there is no formal definition of flatness"". The view in [Dziugaite & Roy, 2017] is that the flatness of the minimum at $\hat{h}$ can be measured by how large we can make the variance of a Gaussian $Q$ centered at $\hat{h}$ while keeping $R_{\mathcal{S}m}(\hat{h}) ~ E_{\sim Q}[R_{\mathcal{S}m}(h)]$. This can be turned into a quantitative definitions in a straightforward way, and it is a very practical notion. Catoni [Catoni, 2007], page 6-7, explicits a PAC-Bayes bound for classification in terms of the set $S=\{h: R_{\mathcal{S}m}(h)=R_{\mathcal{S}m}(\hat{h})\}$. When this set is large, the minimum is exactly flat on a large area, and the generalization bound depends on the mass of this set under the prior. For more general losses, $S$ can be replaced by $\{h: R_{\mathcal{S}m}(h) < R_{\mathcal{S}m}(\hat{h}) + \delta \}$ as in Def. 4.3 in [Alquier, 2024], it is a very classical condition that this set is large enough under the prior (""prior mass condition""). All these conditions are attempts to formalize flatness and should be discussed.

- page 4: Assumption 5 is not classical, so more discussion would be welcome. Can you check this assumtion in practice? Also, if you replace $\ell(h,z)$ by $\ell(h,z)-\ell(h^*,z)$ where $h^*$ is the optimal predictor, Assumption 5 is implied by the classical Bernstein condition, which also implies fast rates (bounds in 1/m instead of 1/sqrt(m)). Is Bernstein condition satisfied under Assumption 5? If so, all you results should be stated with fast-rates PAC-Bayes bounds.

- page 9, Theorem 11, (i): ""and for all $Q\in\mathcal{H}$"" seems useful as the following equation is not stated for any $Q$ but for the Gibbs posterior $Q_{\mathcal{S}_m}^\gamma$ only.

- page 9, Theorem 11, (ii): I appreciate the effort to keep the factor $1/(1-\lambda C/2)$ instead of $2$. However, the term $\|\ell_2\|_{\infty}$ in the bound is not defined (typo?).

Conclusion:

In my opinion, the authors should remove the discussions on ""transitory fast-rate"" bounds, the useless Wasserstein bound. They should instead discuss the application of their Theorem 6 beyond the theoretical objects (Gibbs distributions) to Gaussian distributions, or other feasible methods. With such results, this will be a nice contribution.

***********
Post-rebuttal update:

cf. the discussion below. All problems were fixed. If the authors can implement all the discussions mentioned in the discussion, I believe the paper is a reasonable contribution for COLT."
105,"Pros:
1. Applying the machine learning method to the material science is an interesting and important area. 
2. A new benchmark for refractory HEA yield strength prediction is schedule to be public, which may inspire the investigation of new ML technology in the material science field.
3. The bi-level framework has a solid theoretical foundation.
4. This paper is well-written.

Cons:
1. the motivation of incorporation of sparsity is unclear. Although the authors claim that the use of sparse regularizers enhances the robustness and transferability, there is no related previous works in the line 198-202 to support such claim. Moreover, the incorporation of sparsity/pruning reduce the power of the networks, why it will be helpful for the performance in turn? Is this because the networks are easy to overfit on the simulated data? 

In general, this paper is well-written and logically structured. The datasets may fill a missing gap in the material science domain and the sparsity regularizer with bi-level optimization is novel and solid. Therefore, I think it is qualified for the acceptance."
106,"This work investigates the dynamics of mini-batch gradient descent with random reshuffling for well-specified least squares regression. The main contribution is to identify a sample cross-covariance matrix which quantifies the impact of random reshuffling in the convergence and the error dynamics compared to full-batch and other replacement-based schemes. The paper discusses different limits where this sample cross-covariance matrix - given by a cumbersome non-commutative polynomial of the batches sample-covariance matrices - simplifies, such as small step-sizes, large sample size and the proportional regime. The main conclusion is that sampling without replacement can have a significant impact in the covergence and generalization error in the regime of constant step-size.

Overall, this is a solid theoretical paper with, to the best of my knowledge, novel results for SGD without replacement. The manuscript is well written, though dense and technical. Of course, the setting (well-specified least-squares) is limited in scope, but it is a fair start given the limited literature in this direction.

**Questions**:
- One of the interesting points in the paper is the explicit characterization of the learning rate dependence in the trajectory and in particular in the limit. I miss, however, a discussion on the implications of this dependence in terms of the implicit bias of the solution. For instance, how does the performance depends on the learning rate? Is it the dependence monotonic? Is there any benefit of tuning the learning rate in this setting? Is it comparable to adding a penalty, for instance?

- In the introduction:

> *Most prior theoretical works also analyze gradient descent with infinitesimal learning rates, i.e. gradient flow (Advani et al., 2020; Ali et al., 2020). In this work, we analyze the dynamics of gradient descent with arbitrary learning rates.*

In this level of generality, this sentence is misleading. There is a very extensive body of literature about SGD with constant learning rate, starting as early as 1986 [1] to nowadays, see e.g. [2-7] for a few. These works span different settings (one-pass, two-layer neural networks, etc.), but are closely related to the discussion here.

- On the related works, page 4:

> ""From a dynamical perspective, Paquette et al. (2021); Lee et al. (2022); Paquette et al. (2022) show that the trajectories of SGD for ridge regression with finite step sizes and high-dimensional random data concentrate on a deterministic function determined by a Volterra equation, assuming the batch sizes are vanishingly small as a fraction of the sample size.""

These works show that low-dimensional functionals of the SGD trajectories (such as the risk) concentrate to a deterministic function solving a Volterra equation. Alternatively, this can be seen as convergence of the trajectory (in a weak sense) to an effective stochastic process in the high-dimensional limit (e.g. Theorem 6 in Paquette et al. (2022)). It is worth mentioning that the results in these works for streaming SGD are concurrent to [2, 4, 6, 7], who derived equivalent concentration results in the constant step-size, high-dimensional regime a larger class of problems (such as two-layer neural networks) that include least-squares as a particular case.

- Related to the above, one of the observations in the line of work [2, 4, 6,7] is that for streaming SGD, taking a constant step size in the high-dimensional regime can change the fixed point of the dynamics, usually for the worst (e.g. by estabilizing an unstable fixed point or shifting the global minimum). In the least-squares case, where there is only a single global minimum of the population risk, this is just a manifestation that the process converges to a stationary distribution with finite variance (hence the excess population risk is bounded away from zero). How does this compare with the random reshuffling case studied here? In particular, from Corollary 9, are there cases where a finite learning rate leads to a better performance than zero learning rate?     


**References**:
- [1] Georg Ch. Pflug. Stochastic minimization with constant step-size: asymptotic laws. SIAM Journal on Control and Optimization, 24(4):655–666, 1986.

- [2] David Saad, Sara Solla. Dynamics of On-Line Gradient Descent Learning for Multilayer Neural Networks. Part of Advances in Neural Information Processing Systems 8 (NIPS 1995)

- [3] Dieuleveut, Aymeric, and Francis Bach. Nonparametric stochastic approximation with large step-sizes. Ann. Statist. 44(4): 1363-1399 (August 2016).

- [4] Sebastian Goldt, Madhu Advani, Andrew M. Saxe, Florent Krzakala, Lenka Zdeborová. Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup.  Part of Advances in Neural Information Processing Systems 32 (NeurIPS 2019).

- [5] Aymeric Dieuleveut, Alain Durmus and Francis Bach. Bridging the gap between constant step size stochastic gradient descent and Markov chains. The Annals of Statistics, 48 (3): 1348 – 1382, June 2020.

- [6] Gerard Ben Arous, Reza Gheissari, Aukosh Jagannath. High-dimensional limit theorems for SGD: Effective dynamics and critical scaling.  Part of Advances in Neural Information Processing Systems 35 (NeurIPS 2022).

- [7] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, Bruno Loureiro. From high-dimensional & mean-field dynamics to dimensionless ODEs: A unifying approach to SGD in two-layers networks. Proceedings of Thirty Sixth Conference on Learning Theory, PMLR 195:1199-1227, 2023."
107,"This work is well written and it is a significant state of the art contribution. It is well supported with up-to-date literature on the topic and the results look very impressive in terms of the magnitude of the errors. 
I feel that more details should have been included in the explanation of the methods to be a more appealing work.

Pros: 
* The authors clearly highlight the novelty of their contribution and the accuracy of the methods proposed. 
* The methods achieve data efficiency and low computational costs.
* It is a nice example of how a model can be trained when having low amounts of data avoiding overfitting.
* The magnitude of the errors achieved after the training is impressive.

Cons:
* I felt the lack of details in the methods. It would have been nice to add more explanations/reasoning about the selection of the input data and the split of the data into training/validation/testing samples.
* It seems that they did not conduct a hyperparameter tuning. I wonder if the results would have changed significantly with different hyperparameters.
* More context about the physical meaning of the parameters in the methods to inform the machine learning-based model is also missing. For instance, the ranges shown in Table 1 Section D2 are not properly explained. I feel more discussion about this topic is needed because they mention in the abstract the promising venues of coupling machine learning methods and physical domain-specific insights."
108,"the paper has a good abstract, introduction and a clear graphy for their method of ViT+LoRA. But there are many parts missing in the following parts including  postprocessing,Quantitative results on validation set,Qualitative results on validation set."
109,"Paper summary: This paper focuses on the problem of sufficient dimension reduction (SDR). The model consists of random object response matrix X in a metric space and a predictor vector Y in a Euclidean space; and SDR assumes that they are statistically independent of each other given an unknown linear function of response matrix X, namely, \beta^T X. The number of features in X exceeds the sample size. The goal is to identify a low-dimensional and sparse representation of the dimension reduction subspace (DSR) --- which is related to \beta. They construct a multitask regression model with synthetic responses (computed from X and Y) and achieve sparse estimation by leveraging the minimax concave penalty. Their algorithms avoid inverting the ill-conditioned covariance matrix. The optimization problems are nonconvex. So they develop and implement a double approximation shrinkage-thresholding algorithm that combines a linear approximation to the penalty term and a quadratic approximation to the loss function. The show the utility of their algorithms on manifold-valued synthetic data and one real-world dataset.
 

My Evaluation: The problem statement is interesting. However, the authors should reorganize subsection 2.1 to more clearly explain it; perhaps they can dedicate a latex environment under ""Problem 1"" and formalize the problem. What are the measurements?, what is the goal? and what are the assumptions? One of the main ideas to arrive at the current solution is ""conditional mean independence"" which is not explained.  Furthermore, it is not clear how one arrives at equation 4 from equation (3). I left a comment below about this. Every step of the derivation should be clearly explained (as well as the required assumptions). Overall, I think this is a nice problem statement. The proposed solutions are neat and, for the most part, clearly explained (section 3). The experiments are limited but overall they showcase the performance of their approach compared to the LASSO-based methods on synthetic data (using small sample sizes). I'm willing in to increase my score if the authors address all the comments in this review.


Questions, Comments, and Suggestions:

(1) In the introduction, the problem of ""Fréchet SDR"" is not motivated nor introduced. 

(2) Equation (1) : What is the orthogonality with respect to? Is there a probability distribution assumption? If so, please make it clear. That way, you can better discuss terms like “regression information”.

(3) (page 1, line 38) “The matrix \beta satisfying (1) is not identifiable.” Up to what? I assume it is upto a post-multiplication by an orthogonal (or an invertible?) matrix . Please make it clear. 

(4) (page 1, line 39) “However, DRS is not unique.“ Is this generally true? If so, please dedicate a remark and explain why this the case.  My understanding is that it should depend on the specific model, no? For example, a simple linear model Y = \beta^{T} X + n where n is an independent AWGN. Is DRS unique in this example? 

(5)  (page 2, line 81) “The double approximation technique relies less on the initial values and provides explicit expressions at each iteration,  … “ What does it mean that this technique relies less on the initial values? Do you mean it has a global solution? Or maybe it is not sensitive to the initial values? If so, why or in which cases? 

(6) (page 3, line 91) ""To detect the conditional mean independence, ... "" Could you please explain what this means? This seems very important because it is the main reason for using WIRE kernel matrix to identify DSR.

(7) (Proposition 1) . (a) What is ""metric space of negative type""? (b) This proposition puts a constraint on both the distribution of X and the underlying metric space. Am I correct? If so, could you please give a couple of examples in which the linearity assumption is true. (c) Please reference the proposition inside its environment. 

(8) (Section 2.2) The derived conclusion of \eta \prop 1/n X^T X \beta is true but not immediately obvious. I spent some time deriving it for myself. I suggest that you do the following: (a) Dedicate a definition for the synthetic response variables -- computed from Y and X   (b) Start with the fact that \Sigma Span( \beta ) = Span( \Lambda ) (under the assumptions) and claim that \mathbb{Y} (defined earlier) can be written as \mathbb{Y} = X \beta where \beta is a vector in DSR. Then this can be generalized for when \beta is a matrix. Then, motivate the use of equation 4. The current derivation/explanation is very confusing.  

(9) (Line 2 in Algorithm 1). Could you please explain how X^*_j = \lambda / w_j \mathbb{X}_j is derived? This is not obvious to me. A remark on the main idea in adapting LLA to the sparse setting would benefit the readers.  Also, please make sure that your clearly define terms like \mathbb{X}_j, X_j, w_j, e_j, \beta_i, b_{i, k-1},  .... These notations are very confusing. I suggest for rows and columns use e_i^T X , X e_j ( not X_j, ... ) and for iterations use superscript with parentheses like X^{(k)}. Also indices should varying from lower case letters to uppercase ones, like \sum_{n = 1 }^{N}  not \sum_{j=1}^{p} ... 

(10) (Algorithm 2) What is O_{i,k+1}?

(11) (Page 6, line 210)  ""[24] discovered a disparity a ... "" (a) Please do not use numbered references as nouns (everywhere it applies). (b) Please explain why eigenvalues of \Lambda (instead of \Simga^{-1} \Lambda) are important? And also, why do we need the adjustment terms?

(12) The one-step LLA_G algorithm seem to provide very good numerical results (Table 2 and example 3 in Figures 1 and 2 ) compared to DASTA methods. Any comments on the computation complexity of LLA vs DASTA? 

(13) (page 1, line 62) ""Most of the existing sparse SDR methods employed Lasso or group-Lasso penalty, both of which are convex and lead to biased estimation."" What is the source of this biased estimation? And how does the nonconvex MCP penalty remedy that?"
110,"This paper proposes and analyzes a new MCMC scheme for sampling from Gibbs distributions supported on a convex set (the authors refer to this as the constrained setting). The target distribution $\Pi$ has a density $\pi(x)$ proportional to $\exp(-f(x)){\bf 1}_{\cal K}$, where $f$ is the potential function and ${\cal K}$ is the convex constraint set. The scheme uses a preconditioned Langevin algorithm to generate proposal moves, then applies the Metropolis-Hastings filter. This results in an unbiased scheme (its steady-state distribution is equal to $\Pi$). The local geometry of ${\cal K}$ is encoded in a Riemannian metric tensor, which in turn determines the preconditioning. Under a variety of regularity assumptions on the potential, the metric tensor, and the constraint set, the authors obtain precise upper bounds on the total variation mixing time of their scheme from a suitable warm-start initialization. The discussion of prior art is thorough, including a detailed discussion of computational complexity per iteration as compared with the recent Dikin Walk scheme of Kook and Vempala (in turn inspired by the work of Narayanan on uniform sampling from convex polytopes), as well as a discussion of the relation between their proposed scheme and metropolis-adjusted mirror Langevin algorithm.


Overall, this is a high-quality contribution to the literature on iterative stochastic algorithms for sampling. The authors provide good motivation in terms of applications and the discussion of relevant challenges. The comparison with existing approaches is informative, highlighting the relative advantages and disadvantages. Overall, the paper is relatively easy to follow, although the plethora of definitions of self-concordance is somewhat hard to parse. The authors do a decent job motivating these definitions and explain where one can reap the benefits of stronger (more restrictive) notions of self-concordance. The discussion of the cases where the new definitions coincide with existing ones is also helpful. Given the authors' goal to present a reasonably complete picture, I can't fault them for not including any proofs in the 12 pages of the main submission. The overall strategy underlying the proof is a conductance-based analysis along the lines of Lovász and Simonovits, which is more or less standard as far as Metropolis-Hastings MCMC goes. The main innovations here are in the definitions and in the tradeoffs between performance and complexity; some new isoperimetric inequalities are also established along the way. While I have not checked all the proofs minutely, I have verified the major steps, and am fairly certain the analysis is correct."
111,"This paper studies multiclass learning problems with a specific focus on understanding how unlabelled data helps with \emph{proper learning}. The authors study the ``distribution fixed'' PAC model where the learning algorithm is given access to unlabelled data via a full description of the marginal distribution $\mathcal{D}$ on the instance space $\mathcal{X}$ (and hence can generate as many i.i.d. as desired.) This model still retains a worst case perspective by evaluating on the hardest $\mathcal{D}$ and hardest target function $f \in \mathcal{F}$. As a first result, the authors show that the sample complexity of this setting and the standard PAC setting are essentially equivalent up to a logarithmic factor in the failure probability, demonstrating that unlabelled data cannot help much for the worst distribution. The authors then go on to show that in the distribution fixed setting, an optimal learning strategy (up to a constant factor of 2) is achieved by a \emph{randomized proper} learning strategy whenever the instance space $\mathcal{X}$ and label space $\mathcal{Y}$ are finite. The authors then consider the standard multiclass PAC setting and show several negative results regarding characterizing proper learnability by proving that: 1) proper learning can be logically undecidable 2) that it is not monotone property of function classes (wrt to addition/removal of functions) and 3) it is not a local property.

I generally find the results in the paper to be quite interesting. In particular, the result which demonstrates that there exists a randomized proper learner that is optimal in expectation up to a factor of 2 is interesting because this result is somewhat in contrast to an implication of known result in the standard multiclass PAC setting (considered with finite $\mathcal{X}$ and $\mathcal{Y}$) that says any proper learner suffers constant expected error though the comparison is not perfect since the upper bound in this paper allows for randomization. I am a bit confused though why the authors refer to these as randomized proper strategies (and in fact drop the randomized part in many places in the paper) since they are mixtures of functions in the class which can be much more expressive than the functions themselves. This can give off the false impression that these learners are truly proper especially when the \emph{randomized} qualifier is dropped. It would also be interesting to see a discussion regarding the specific case of standard binary classification where we known simpler optimal algorithms (up to constants) based on taking majority votes of ERMs (which to be fair is not the same as randomized proper) and whether it is possible to prove similar results here. Finally, while I find the final set of results around undecidability to be interesting, I feel that they are less of a contribution since they are a very straight forward consequence of existing results."
112,"This work investigates the generalization properties of gradient descent (GD) and stochastic gradient descent (SGD) in high-dimensional stochastic convex optimization problems, specifically examining how the sample complexity scales with the problem dimension. The authors provide an in-depth analysis of the dimension dependence in generalization, delivering two notable lower bounds on the sample complexity for both full-batch GD and one-pass SGD.

For full-batch GD, they construct a learning scenario with problem dimension $𝑑=𝑂(𝑛^2)$, showing that even when tuned to optimize the empirical risk, GD can yield an approximate empirical risk minimizer with Ω(1) excess population risk. This result implies a lower bound of Ω(𝑑) on the number of training examples required for GD to reach a non-trivial generalization error, addressing open questions raised by previous work (Feldman, 2016; Amir, Koren, and Livni, 2021). This finding underscores that non-trivial dependence on the problem dimension is unavoidable, highlighting a significant challenge in achieving effective generalization in high-dimensional settings.

Furthermore, for one-pass SGD, the authors apply a similar technique to demonstrate a Ω(d) lower bound for the sample complexity needed to achieve a non-trivial empirical error, even though SGD achieves optimal test performance. This result provides an exponential improvement in dimension dependence over previous bounds (Koren et al., 2022) and resolves an open question from that work.

Overall, this paper advances the understanding of dimension-dependent generalization bounds for gradient methods in stochastic convex optimization, offering theoretical contributions that clarify the limitations and challenges of GD and SGD in high-dimensional settings. The results could impact how these optimization techniques are applied to large-scale learning problems, guiding future research into more dimension-aware gradient-based methods. I went through the proof to the best of my abilities within the time-limit provided and they seem correct to me. 

Hence, I recommend acceptance."
113,"Summary of Contributions:

The work provides a transformer based method to create a “prescription assistant” by training on longitudinal EMRs. The authors propose CycleTrans to learn good representations which surpasses performance achieved by prior works.


Strengths:
1. The results are excellent when compared to prior works.
2. The block diagram is good representative of the method described in the paper. The visualization in the final page is also quite impressive.
3. Hyper-parameters and training details are provided in the paper.
4. The EMD loss strategy to increase precision is a good idea.


Weaknesses:
1. The biggest concern is that, the work authors propose might not a foundation model in the traditional sense. In this work, the generalised pre-training phase and domain specific fine-tuning phase wasn’t clear from the manuscript.
2. The dataset MIMIC-III is quite small for training a foundation model. Maybe MIMIC-IV even some synthetic datasets like Synthea might have helped the model learn good quality representations in the pre-training phase.
3. Experimental results have been demonstrated on just a single dataset (MIMIC-III). The generalizabilty of the method cannot be verified unless other benchmark datasets are also used."
114,"The paper explores the efficacy of traditional segmentation methods such as thresholding, Meijering filters, and Geodesic Distance Transforms in comparison to the more recent MedSAM model, specifically for scribble-based interactive segmentation of medical images. Focusing on modalities like fundus, OCT, PET, and microscopy, the study highlights the potential advantages of simpler models over foundation models in terms of segmentation accuracy and computational efficiency. The paper's methodological approach, examining each imaging modality individually, provides a clear understanding of where traditional methods can outshine more complex models. However, the use of different preprocessing methods for each modality seems to contradict the overarching goal of developing a generalizable foundation model, potentially limiting its broad applicability."
115,"Personally, I have found the exposition of this paper quite hard to follow. I do not think it is clearly outlined in the abstract and introduction how the authors leverage insights from the NC phenomenon to improve the utility of differentially private mixup. With that said, here is a summary of my reading.

Summary:

Previously, Zhang et al. proposed RW-Mix, a data augmentation technique aimed at improving the utility of (presumably) ML algorithms. However, when noise is introduced in a differentially private context, the algorithm's performance is compromised, resulting in diminished utility (whether that is for data publishing or for further use). To address this issue, this paper introduces two main algorithms: NeuroMixGDP and NeuroMixGDP-HS. The former is a differentially private version of Avg-Mix, while the latter is a differentially private adaptation of NeuroMixGDP, incorporating hierarchical sampling.

Review:

Overall, I find the idea in this paper to be novel: using insights from label collapse to propose a new algorithm in differentially private data publishing. Furthermore, the results appear to be very promising — the proposed method seems to significantly outperform existing methods, as reported in Table 2. The main issue I have is with the clarity of the paper, which I outline below:
- It is not made clear why Poisson sampling suffers from the Label Collapse issue other than being an empirical result. Is there an intuition for this, and does this apply only to Poisson sampling or more generally? If it's specific to Poisson sampling, can we employ different sampling techniques to avoid label collapse?
- Is the linear model an appropriate choice for studying and characterizing the sweet spot of the mixup degree? Were there other models that could have been considered, and if so, what are their advantages and limitations? The authors do not clearly explain why they opted for this specific model.
- I believe that Section 5, which covers the membership attack, could provide more detail and further motivation on why this is a valuable case study.
- To my knowledge, saying that an algorithm is “\mu-GDP” is not customary. In the relevant literature, a Gaussian differentially private algorithm is generally referred to as (\epsilon, \delta)-DP.

I would be more than willing to raise my score if these questions can be well addressed. As suggested by my confidence score, there may have been some parts that I may have misunderstood."
116,"In this work, the authors develop and present preliminary results regarding an identification system based on magnetocardiography. The method for using MCG is an interesting non-invasive technique and significant effort has been shown in developing the system. However, there are several comments to be made regarding the work in the context of the symposium:

- First, the limited sample subjects does question the robustness of the results. Evaluation over a larger cohort would strengthen the conclusions of the paper
- The modality of MCG is novel, but given the size of the apparatus shown, it is questionable where such a system would be practical and needed.
- In addition, although the authors claim that MCG-based identification would be more robust and reliable, it is uncertain whether this is true given a lack of comparisons to other mentioned methods, such as facial features or ECG.
- The authors utilize a CNN with features extracted from a wavelet transform for the identification, it’s unclear that such a method would be scalable to large populations that would be necessary for practical deployment.
- Moreover, there doesn’t seem to be any significant innovation related to MCG or ML for healthcare in general.

Although the method of MCG is novel and interesting, further results and evaluation to other existing approaches is recommended"
117,"Summary Of The Paper

This paper studies active learning (AL) for medical image classification. The authors introduce an efficient AL framework based on self-supervised learning models. This paper also introduces a patch augmentation method to enhance the feature representation quality and reduce redundancy, ultimately preventing overfitting during AL.


Main Review

- Strength
1) The paper is written in a clear manner. The authors provide a comprehensive background and give clear justification and problem setting for their proposed work.

2) In addition, the proposed method is supported by multiple datasets, and the authors have conducted extensive ablation studies, including several versions of augmentations, which is good to justify the proposed model.

- Weakness
1) With extensive experiments and analysis, the paper has demonstrated its proposed strategy on several dataset, however, the intuition of the proposed method is not thoroughly discussed. I recommend the authors should elaborate more on the design of their label-irrelevant patches augmentations.

2) In order to demonstrate the effectiveness of the proposed method, downstream tasks such as detection on AL (e.g., https://github.com/yuantn/MI-AOD) could be explored. I think the proposed method may not be practical without other tasks.

3) Have the authors investigated and addressed potential domain discrepancies between the pretrained ViTs, which are trained on natural images, and their fine-tuned adaptor with medical images (specific domain)?

4) I'm not entirely certain if Table 2 provides an apples-to-apples comparison. In one case, the entire VIT backbone is frozen, and only the adapter is fine-tuned. However, in the other case, Resnet18-50 is trained end-to-end. What would happen if I were to take a pretrained Resnet18 and then add an adapter for fine-tuning?

5) I suggest the authors to provide a comprehensive survey on relevant works such as augmentation strategy on representation learning (including self-supervised learning), For example: [A] Kügelgen et al, Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style, [B] Yeh et. al., SAGA: Self-Augmentation with Guided Attention for Representation Learning

Summary Of The Review

Overall, without discussing the intuition of the proposed method in details, I am not sure if the novelty that authors mention in the paper is reliable. Also, I think we need to see valid elaboration and the intuition of the proposed method, and scale up to other tasks. Combined with the weaknesses I mentioned above, I vote for 5. I would like to see authors response to consider raising the rating."
118,"Summary:

This paper introduces the notion of ""full swap regret,"" a new form of swap regret where the learner competes against all swap functions mapping a bounded convex $d$-dimensional action set $K$ to itself. 
The paper proposes algorithms for minimizing full swap regret under varying conditions of convexity and smoothness in the loss functions. Key results include $O(T^{d/d+2})$ full swap regret bound for strongly convex and smooth loss functions (it also provides full swap regret bound for other conditions of convexity and smoothness in the loss functions.), and applications to $\ell_2$ calibration error minimization. For $\ell_2$ calibration error, they provide an efficient online forecasting algorithm that achieves $O(T^{1/3})$ calibration error for $d=1$.

The paper also defines a discretized version of $\ell_2$ calibration error, achieving an error bound of $O(\max(\sqrt{\varepsilon T}, T^{1/3}))$ for $d=1$. They achieve this result with the full swap regret approach by using a piecewise linearization of the loss function.

Pros:

(1) The concept of full swap regret extends existing swap regret frameworks to continuous convex action sets, broadening the applicability of swap regret minimization to new domains. 

(2) The application of full swap regret to $\ell_2$ calibration, particularly in achieving new bounds for discretized calibration, demonstrates the practical utility of the proposed framework. Both the $O(T^{1/3})$ error bound for $\ell_2$ calibration and $O(\max(\sqrt{\varepsilon T}, T^{1/3}))$ bound for discretized $\ell_2$ calibration for $d=1$ can not be naively obtained from previous methods.

(2) The authors provide a rigorous theoretical framework, establishing regret bounds for different convexity and smoothness settings, which are well-suited to high-dimensional, continuous action spaces. Their method uses the $\varepsilon$-net and $\varepsilon$-triangulation of the convex set, the piecewise linearization of the loss function, and the optimal external regret algorithm for nearly-strongly-convex loss, which is very interesting. 

Minors:

(1) Page 7, Line 7: typo in the update for the estimate $x_t$."
119,"The paper addresses data-dependent regret bounds for Online Portfolio Selection, moving beyond standard worst-case regret. For the standard online scenario, the authors propose the LB-AdaCurv ONS algorithm, which achieves O(min(nR log(T),\sqrt{nT log(T)})) static regret (with \epsilon set to 0 or 1), where R is an unknown data-dependent parameter. The additional computational complexity per round is O(n^3). 
In the second scenario, where the algorithm has a prior belief about the loss function (or future market trend) before each round, the authors propose the OUE-LB-FTRL algorithm. This algorithm achieves O(n log(T)) static regret when priors are accurate and O(\sqrt{nT log(T)}) static regret if the priors are arbitrary (worst case). Additionally, they introduce a meta-algorithm, Bob-OPS, which achieves O(log(T)) regret against an expected utility investor and O(n log(T)) static regret, both when the prior belief is accurate. Furthermore, they demonstrate that their algorithms can achieve dynamic regret for various dynamic scenarios.

Weaknesses:
Since it is a very specific problem (OPS) I think experiments are important here. 
You write that the additional computational complexity of  OUE-LB-FTRL is O(n^3), however, it finds the minimization over the expectation of some distribution. How it can do it? Is it for specific distribution?

minor:
What is the \Psi presenting in the log-barrier regularizer?
I am unfamiliar with your names for the First-order/Second-order/Gradual Variation regret bounds. I know they are types of dynamic regret (or part of it). So, if those names/definitions are known it’s fine, otherwise it’s better to use known names/definitions.
Typos (not all):
In Alg.1 when you calculate M you write q\in\Delta. There is no q. You probably mean w. Also, the definition of the log-barrier regulaizer has typos.
I mentioned a few.

My wonders:
In LB-AdaCurv ONS there are two regularizers, both of which push to the equal element vector (all 1/n). what is the intuition for this? As we can see LB-AdaCurv FTAL has only one regularizer (\epsilon = 0).
As I understand, there is a connection between LT^* and VT. Thus, if it's true, you are supposed to be able to achieve the same Gradual Variation regret of the OUE-LB-FTRL algorithm by using the LB-AdaCurv ONS algorithm, but you don't do it. What is the reason is it not possible?

In conclusion, the paper presents new results for the OPS problem, which is a very specific problem. In my opinion, the proposed problem, results, and algorithms are of interest to the community, but I doubt that the work is significant enough to be published at the conference. Additionally, I mentioned a number of misunderstandings I had in the paper. I would appreciate the response of the authors.

#####
After reviewing the appendix, I concur that the work is broader than my initial claim. Thus, I revised my score to 6."
120,"This paper explains an approach to convert tabular data into textual descriptions that can be parsed by a language model for comparison against a specific medical definition.

Strengths:
1. The paper presents an interesting idea to supplement LM models with clinical definitions to inform model predictions. 
2. Multiple experiments were performed. Results were averaged and standard deviations were included.

Weaknesses:
1. Paper contributions are misleading. Paper says, ""approach transforms multifaceted pre-operative and intra-operative tabular data into coherent text-based descriptions, enabling the use of advanced Language Models (LM) for data interpretation."" This seems to imply that a model is used to output text descriptions from static data but really these descriptions are developed using medical experts. Also the conclusions say, ""The key contribution of our study is to increase the predictive performance of medical data with heavy class im- balances in disease prediction"". Class imbalance is not discussed as an objective in the introduction and it unclear how the method was designed for this purpose in mind.
2. More explanation of the RBF framework is needed. What is it? How does your method differ/expand on this approach?
3. Paper contributions as explained do not seem to present a novel methodology. The text and definition are developed by medical experts and then just parsed through an existing LM where the output embeddings are compared using cosine similarity. It seems that the main contribution is the approach to group sentence inputs to a downstream classifier for prediction rather than directly using an LM model fine-tuned for binary classification. Perhaps the embedding approach for grouped sentences is novel? More explanation and a perhaps refocused message is needed.
4. Based on confidence intervals the results do significantly improve over baselines in terms of precision and recall. 
5. Explanation of baseline LM training is vague. 
6. How was the relevance threshold selected? This should be a hyperparameter that is tuned based on a validation set and not manually selected based on model performance. More explanation is needed here.

Questions:
1. Is the definition of MINS provided to the other baseline LMs? If not, it would be difficult to compare how this approach compares to other baseline LMs and assess whether the downstream classifier is necessary.
2. What is the purpose of the different inputs channels? Is it to create embeddings over a set of grouped sentences rather than individual sentences? Did you experiment with just inputing the embedding over the entire patient description? This would be similar to using an LM binary classifier baseline.

Other:
1. I would recommend emphasizing the value of this approach to adapt to new/changing definitions easy without needing to relabel the data and retrain a model. That is a very valuable approach in healthcare."
121,"This paper lays out a framework for ODE and SDE distributional sampling using PINNs. It presents the loss terms for different evolution models, and compares PINN to path integral samplers. This is an interesting idea, it's great to see PINNs being used in domains outside of physical modeling. This isn't quite my area, so I can't speak to whether there's similar existing work though.
The benchmarks are rather low-dimensional and simple, but that's fine for an exploration in the context of a workshop."
122,"Summary

This paper proposes architectural variants of transformers, pretraining, and fine-tuning methods for PPG domain.

Strengths

- Experiments are extensively conducted in PPG domain.

Weakness

- Lack of comparison with other transformer architectures for timeseries data
- Writing could be improved. For example, the difference between linear prediction head and attention-based prediction head is unclear, and it’s hard to identify SOTA algorithms in experiments."
123,"This paper evaluates the usage of in-domain pretraining for medical segmentation tasks as opposed to the use of a generalist foundation model for segmentation (SAM), and proposes a new PEFT technique: LoRaMedNet. The authors show that the in-domain pretrained model, MedSAM, does not consistently outperform SAM when fine-tuning on medical segmentation data, which brings the usefulness of MedSAM into question. Additionally, their proposed LoRaMedNet outperforms standard PEFT techniques for fine-tuning on medical data. The technique combines standard LoRA with an additional convolutional head. The set of comparisons also seem solid as a workshop contribution, as they compare to a variety of existing fine-tuning baselines on their target dataset -- the Automated Cardiac Diagnosis Challenge (ACDC). This paper seems quite relevant for the workshop and is well-written."
124,"The goal of this work is to obtain linearized representations and cluster data that is drawn from a union of low dimensional manifolds. Existing methods either have difficulties with less/noisy samples or use deep models that lack interpretability.

To achieve this task using interpretable deep models, the authors propose an approach called DELVE that takes inspiration from linear subspace clustering methods. The key idea used by DELVE is that a data point on a non-linear manifold can be approximated using a linear combination of points in its local neighborhood on that same manifold. This is used to build a sequence of ‘self-expressive layers’ that progressively linearize the input manifold. The parameters of each layer are sequentially optimized in an unsupervised manner by using a ‘self-expressive loss’.

**Strengths:**

1. DELVE achieves good clustering performance, comparable to existing deep models. The model can also scale well to large datasets since it does not necessarily require backpropagation through all the layers. Additionally, from Fig. 2, the obtained representations visually appear meaningful.

2. I like the white-box characteristic of DELVE. The role played by each layer is interpretable and one can play with different hyper-parameters and observe performance changes from the model that are mathematically explainable.

3. The paper is generally well written.

I have a few questions/concerns regarding DELVE that I would like the authors to clarify.

1. A key premise for DELVE to succeed is the ability to linearly approximate a data point using other points in its local neighborhood on the manifold. How would DELVE perform in comparison to an autoencoder if the ambient dimension/image size increases (or available data points from the manifold are farther apart from each other)? How do the obtained representations in such case look like? Are they still meaningful?

One of the limitations of conventional approaches like locally linear embeddings is their difficulty to deal with less samples. I wonder if DELVE would have a similar performance decline (if any) or its model depth could somehow help with that.

2. Once an autoencoder is trained, it can perform a quick forward pass to yield representations for a new data point. On the other hand, it would be quite inefficient if DELVE had to run the entire optimization program every time one needed representation for a new data point. How quickly can DELVE do this in comparison to a conventional autoencoder? Is there an efficient method that DELVE can utilize?"
125,"The paper applies a novel strategy for solving high-dimensional Poison equations based on the neural walk-on-spheres technique. The approach and the results are interesting and significant, as I see. Also, I should admire the authors for staying loyal to the unanimous sharing of information, even their code. The paper looks interesting and novel to me. Also, the authors carefully explained the details of the work in most of the sections. They, then, compared the results to other approaches. I have below minor comments:
-	It would help clarify the approach if authors also mention the challenges of the method or cases in which it will have problems. 
-	Is it possible to utilize neural WoS for inverse problems?
-	As far as I know, PINNs performance is really a function of choosing the right collocation points, and also a suitable training strategy. It would be better if the configurations that have been used for PINNs were discussed more carefully in appendix E (and maybe for other methods)."
126,"#Strength
1) This paper first proposes a new algorithm for DP stochastic convex optimization, namely Accelerated-DP-SRGD, based on a careful adaptation of Nesterov’s accelerated stochastic gradient descent.
2) The proposed algorithm can achieve optimal DP-SCO errors for $O(1)$-smooth convex losses with $\sqrt{n}$ batch gradient steps, which is better than the prior works in this term.
3) This paper analyzes the cases where the unconstrained population risk minimizer $\mathbf{x}^{\dagger}$ is and is not within the constraint set $\mathcal{C}$. When it is within $\mathcal{C}$, the batch gradient steps can be further reduced to $n^{1/4}$.
#Weakness
1) The structure of the paper is not well-organized. The introduction section is too long, i.e., over 7 pages.
2) The comparison with related work is limited, which fails to clearly explain the challenges in reducing batch gradient complexity.
#Suggestions
1) It would be better if you introduce the definitions of $L$-Lipschitz and $M$-smooth functions before presenting the problem definition in page 2.
2) In line 6 of the second paragraph on page 3, ""number of"" is written twice.
3) There's quite a bit of blank space on page 22.
4) It seems that the second ""$=$"" on page 30 should be ""$\leq$"".

#Questions
1) A claim in this paper is that Accelerated-DP-SRGD beats the prior SoTA in terms of gradient complexity and batch gradient steps. In the non-private setting, can Nesterov's accelerated SGD achieve this effect, or is this improvement specific to the differential privacy setting? 
2) In Section 1.1, you mention using DP matrix factorization to approximate the SRGs, but I couldn't find where this technique is used. Could you point it out?
3) What is the difference between the assumption that $\mathbf{x}^\dagger \in \mathcal{C}$ and the assumption of the realizable regime?
4) In general, when bounding the $\ell_2$-sensitivity, an algorithm without perturbation should be used. Why in the proof of Lemma 11, $\|\mathbf{b}_{t-1}\|_2$ is used when bounding $\\|\\mathbf{x}\_t-\\mathbf{x}\_{t-1}\\|$?
5) In the last equation of the first paragraph on page 24, why does $\sqrt{\frac{\sqrt{n}T}{c_{DP}B}}=\sqrt{c_{DP}}$ hold?
6) In Corollary 9, is the $polylog(n,1\/\\delta)$ term omitted? If so, I think $\widetilde{O}(\cdot)$ should be used."
127,"In this paper, the authors propose a pipeline for modeling and simulating traffic at street intersections. The trajectory models are trained using real-world tracking data. When simulating an agent, a coarse trajectory is first sampled from a GMM and then refined by an LSTM.

Strengths:

The authors expend significant effort in preparing real-world data for the data-driven method, including five years of data for object detection and tracking, and 30 days for trajectory forecasting.

The GMM + LSTM model is sensible, where the GMM can generate diverse coarse trajectories, while the LSTM is capable of modeling nonlinear interactions.

Based on the existing data and the methodological backbone, several interesting follow-up works are proposed in the future work section, such as collision avoidance and traffic light control.

Weaknesses:

The diversity of the testing scenarios could be improved. Currently, only one intersection is showcased, while traffic patterns could vary significantly in different places.

The presentation of this work could be enhanced if the authors provided some videos.

Summary:

This paper presents a data-driven traffic modeling and simulation pipeline based on their collected data. The entire pipeline appears reasonable with promising future work. The project could be made more comprehensive with a better variety of tasks and data sources."
128,"This paper proposes an innovative approach to predicting the dynamic knee moment using wearable sensors, AI, and ML algorithms. The author provides a detailed description of the model structure, training process, and explanatory analysis facilitated by the XAI tool. Here are some review comments:

Methodology: It is strongly recommended that the author explains the physical meanings of each symbol and letter in the formulas on the right side of the second page, as well as the distinctions and connections between the second and third lines of the formulas. Additionally, is the sample size of only 24 participants a bit small?

Results and Discussion: The paper's explanatory analysis of model predictions, particularly using the XAI tool, is very interesting. It is suggested that the author delves deeper into analyzing the model's performance, limitations, and future directions. For instance, a discussion on the model's adaptability to different types of patients or varying environmental conditions would be valuable.

Figures: Figures 3, and 4, along with their respective explanations, are crucial for readers to understand the research results. However, it is advised that the author provides more detailed explanations in the captions and legends of the figures to ensure readers accurately comprehend the information presented in the charts.

Overall, this paper presents a promising study demonstrating how the integration of wearable technology and AI/ML algorithms can predict the dynamic knee moment. Through explanatory analysis and detailed discussions on model performance, the paper has the potential to further strengthen its contributions and practical applications."
129,"Congratulations to the authors in getting this research to this stage. I enjoyed reading this nicely drafted paper. Draft starts with a short, concise and excellent Introduction and defines the problem very well. 

The title and abstract focus on the proposal of a new activation function for geometric algebra however I fail to grasp the novelty in the paper. The approach of combining two existing components to formulate the GA-RELU activation function and its subsequent application to a new domain is both commendable and indicative of the paper’s creative efforts to advance the field. The application is definitely new and hence makes the case for this paper. However, upon careful review,  the novelty of the concept of GA-RELU prompts further questions. The description and promotion of complex cardioid  activation under a new name of GA-RELU may overstate its uniqueness. 

- Quality 
  - The written quality of paper is good. Draft nicely explains the problem and solution. The paper use the modified complex cardioid  activation for the GA and demonstrate the improvement with this approach and 
  - Draft clearly associates the physical meaning associated with mathematical equations used and derived. 

- Clarity 
  - The text is clear and describe the problem and solution with all necessary mathematical details. 

- Originality
  - The concept of modifying the complex cardioid  activation is original and has not be proposed in the filed earlier. 
  - Moreover, the application of GA-RELU for Geometric Algebra Networks is new and its application to the 2D NS PDE. 

- Significance 
  - The proposed research mark an significant improvement for Geometric Algebra Networks, however the experimentation uses on one example. 
  - Due to limited experimental results it is hard to judge its usability and applications in different problems in GA."
130,"I strongly recommend that this article should not be accepted at this stage, or that the author should revise it before resubmitting. My reasons are unrelated to the technical content of the paper and are as follows:

1. The author list appears to be inconsistent. OpenReview and the table at the end of the paper indicate there should be two authors, but I only see one name on the paper.

2. There are numerous writing errors, such as:

  2.1 In the first sentence, ""Deep learning-basedmodels have shown great promise inmedical image segmentation due to their ability to learn intricate image features,"" there should be spaces between the words.

  2.2 The spacing between paragraphs within the same section is too large, as seen on the second page.

  2.3 There are spelling errors, such as:

        In the Introduction, terms like ""LitedMedSM"" and ""MoboleSAM"" are incorrect.

        In the description of Fig1, the phrase ""...finally the two Decoder results Mask1 and Msk2 are weighted and fused to train the model"" 
        contains ""Msk2"" instead of ""Mask2."" Additionally, Mask1 and Mask2 are not labeled in Fig1.

2.4  There is misuse of punctuation, for example: ""Different from the original structure, I added a new Decoder with a structure and 
       Scribble-Guided Mask Dcoder. , and inherit the corresponding weight."" The punctuation "". ,"" is incorrect.

2.5  There are terminology errors, such as: ""The network was optimized by the AdamW37 optimizer with an initial learning rate of 5e-4 and 
       a weight decay of 0.01."" I do not understand what ""AdamW37"" means. I assume 37 is a citation, but there are only 18 references in 
       the paper.

The above are just a few examples; there are many such errors throughout the paper, making it difficult to read. I believe that the author has not taken a serious attitude towards to the writing of this paper, so I hope the author can revise it before resubmitting."
131,"## Overall clarity
### Pros
Clear language expression. Well-placed references and citations. Pertinent literature review and related work. A handy appendix that further details the data curation and evaluation steps.

### Cons
Minor grammatical and punctuation mistakes.

## Overall originality
### Pros
The work is original on two fronts: it proposes two new datasets and a new learning algorithm for better handling reasoning in large language models (LLMs).

### Cons
The work could have benefitted from an architectural diagram of the fine-tuning and innovation applied at the learning algorithmic level.

## Overall quality
### Pros
Clear and detailed evaluation results and ablation studies. The dataset collection process is explained in depth as well. Result analysis of the performance and the novelty of the technique with respect to LLMs judging their generations is also highlighted.

### Cons
More a bonus than a critique: it would greatly benefit the work to be able to shed light on a comparable performance with other famous LLM generations besides LLaMA (e.g., ChatGPT, Falcon, etc.)

## Overall conclusion
An excellent contribution in reasoning for large language models."
132,"The paper comprehensively analyzes each modality of the dataset and decides on the method to use. It reviews failure cases of MedSAM and complements the method with classical approaches such as k-means or thresholding. For the 3D dataset, slice interpolation is incorporated to increase the efficiency.  Using the classical approaches with the supervision of the instance-level bounding boxes, it achieves good performance and efficiency.
I have minor constructive comments: 
1. Highlighting the best metrics in the tables would be useful.
2. Commenting on how the coloring coding was done on images with multiple instances would be helpful."
133,"I congratulate the authors on getting this research to this stage. 

The research is good quality and clearly describe the problem and propose the solution. The work presented is known to be original (in my knowledge) and marks an significant improvement to use IPINNs for hyperbolic conservation laws. 

The text is written in clear manner and easily understandable. There are some minor inconsistencies in the text , eg. neural net and nets are used interchangeably, Line 4 use the word ""such"" and rephrasing is needed here. This is not an exhaustive list and therefore I request authors to do a quick proof reading."
134,"Summary of Contributions:

The work provides a transformer based method to create a “prescription assistant” by training on longitudinal EMRs. The authors propose CycleTrans to learn good representations which surpasses performance achieved by prior works.


Strengths:
1. The results are excellent when compared to prior works.
2. The block diagram is good representative of the method described in the paper. The visualization in the final page is also quite impressive.
3. Hyper-parameters and training details are provided in the paper.
4. The EMD loss strategy to increase precision is a good idea.


Weaknesses:
1. The biggest concern is that, the work authors propose might not a foundation model in the traditional sense. In this work, the generalised pre-training phase and domain specific fine-tuning phase wasn’t clear from the manuscript.
2. The dataset MIMIC-III is quite small for training a foundation model. Maybe MIMIC-IV even some synthetic datasets like Synthea might have helped the model learn good quality representations in the pre-training phase.
3. Experimental results have been demonstrated on just a single dataset (MIMIC-III). The generalizabilty of the method cannot be verified unless other benchmark datasets are also used."
135,"The paper's main problem is the writing and clarity. The paper contains many spelling mistakes, grammar mistakes, miscapitalization, use of first person, and text leftover from the template. In addition there are formatting errors, e.g. as the description of SAM in the Proposed Method Section belongs in the Background Section. Figure 1 looks extremely similar to Figure 2. The authors describe the MedSAM contest in the background, but solve the Scribble MedSAM contest in the Proposed Method. The author introduces various medical imaging tasks (e.g. fundusscopic) out of the blue. As such it makes reading extremely difficult, and should not be published until the mistakes are fixed. 

The paper contains reasonable detail. For example the author does not specify how they choose the modalities upon which to apply the multi-expert fusion.  

The paper is very novel, it introduces a unique multi-task expert design. However it does not attempt to provide any computational speed improvements. Furthermore it only just surpasses the baseline in dice score, and equalizes it in normalized surface density. 

Its analysis and discussion of results is very rudimentary."
136,"It’s interesting that you used nnUNet to achieve such good accuracy. 
However, there are a few things that I would like to see improved:

1. Standardize the format according to the template provided by the Challenge.
2. The section '2.4 Training vs. Inference' seems to lack details on the strategy for training the model.
3. The details on the fine-tuning dataset and model are insufficient.
4. Providing sources for the various datasets used in the challenge can greatly improve the clarity and reproducibility of your research."
137,"The authors have presented a very neat study on learning stochastic dynamical equations from data. They have provided the mathematical framework and implementation details for real world data in a detailed manner. 

In section 4.2 the authors seems to have misplaced the Figure number. Please rectify it. The figures in section 4.1 and section 4.2 should be clearly identified. Currently it is misguided. The drift function used in the section 4.2 seems to be inconsistent with section heading of deep neural networks. I request the authors to look into that and avoid such discrepancies. 

The results section should be rewritten in a clear manner avoiding any misinterpretations."
138,"The paper proposes two automatic metrics for evaluating LVLMs hallucination degree in the medical domain.

## Pros
- hallucination evaluation in the medical domain is an important research question. And the authors motivate it well.
- the proposed object hallucination and ""domain knowledge hallucination"" are relevant to the medical domain

## Cons
- The proposed ""domain knowledge hallucination"" indeed only focuses on the diagnosis. Other medical concepts such as procedures, medications, medical conditions are not included. To my understanding, it is more appropriate to use the term ""diagnosis hallucination"".
- The automatic evaluation is great for scaling. Meanwhile, it would be interesting to know whether these model-based metrics are really reliable (i.e. the model-derived evaluations themselves do not contain hallucination). I suggest adding some human evaluation to see (1) whether LLM-based NER is reliable; (2) whether cosine-similarity and threshold are reasonable.
- Only LLaVA-Med is evaluated, which weakens the argument presented.
- Clarification needed:
  - how to combine cosine similarity and the ICD-10-based distance?
  - On the second round inference, would the ground truth diagnosis be leaked to the LVLM in the enhanced prompt? 

## Misc.
- Figure 2 is presented, but never mentioned in the text.

Overall, the proposed metrics are variants of existing metrics, with a focus on the medical domain. 
The clarity can be improved to make the manuscript stronger. Certain human evaluations and LVLM evaluations are needed to thoroughly validate the technical designs."
139,"The paper presents a PAC learner that is both statistically optimal and computationally efficient, specifically designed for the realizable PAC setting. The learner is improper and utilizes empirical risk minimization (ERM) as an oracle. The authors' approach leverages the ERM by reducing the required number of calls and optimizing the sample size for each call, achieving a computational complexity that is nearly linear in the sample size. This is a notable improvement in efficiency compared to previous works, which required sample sizes that scaled with the sample size, rather than the VC dimension.

The authors build upon foundational works by Hanneke (2016), Larsen and Ritzert (2022), and Larsen (2023). Whereas prior methods like bagging or boosting among multiple ERMs were computationally heavier, this paper introduces a modified AdaBoost strategy. By making smaller, linearly scaled queries relative to the VC dimension, and achieving lower complexity in both training and inference, the proposed method stands out in efficiency. The paper provides a thorough summary of prior works, which helps readers understand the novel contributions of this approach in the broader context of efficient PAC learning.

Overall, the paper is well-written and well-organized. In particular, the first 13 pages effectively introduce the necessary background and previous literature, helping to situate the authors' contributions. This result aligns well with the core interests of the ALT conference, emphasizing both theoretical soundness and practical efficiency in learning algorithms.

However, one area where the paper may fall short is in introducing entirely new techniques. While the results are valuable, they build on existing methods rather than proposing a fundamentally new approach.

Typos and Minor Issues:

- Page 4: ""Hanneke continue"" should be ""continues.""

- Throughout the paper, the expectation symbol is frequently used instead of the probability symbol in over 10 places.

- Page 7, line 3: The wrong font is used for \( B \); it should be \( \mathcal{B} \).

- Page 8, second paragraph from the bottom, line 2: ""S"" should be replaced with \( h_i \)."
140,"Authors propose NeuralMD which is a ML surrogate model, multi-grained symmetric differential equation model for learning protein-ligand binding dynamics."
141,"This paper concerns online nonparametric regression with general convex losses, where the comparator class of functions is Lipschitz. The authors employs an adaptive boosting algorithm in a specialized class of trees, termed ""chaining trees"", where the parameter updates can be computed efficiently due to a sparse structure of the gradients. By applying recent advances in adaptive online learning, the paper achieves several significant results:

1) The authors present a new and general online gradient boosting method, which can be performed efficiently within the class of chaining trees (in fact, the algorithm is defined more generally on some class of functions \mathcal{W})
2) The algorithm achieves minimax regret over the Hölder class of functions and adapts to unknown regularity constants (L or \alpha).
3) The authors also propose a meta-algorithm that competes with oracle-pruned trees, yielding optimal locally adaptive regret that scales with the local regularity of the function class and adapts to loss curvature (e.g., exp-concavity).

Overall, I liked the paper and I believe its theoretical contributions are substantial. The presentation is generally clear and the technical parts seem sound, up to what I was able to verify (only part of the proofs in the appendix). However, the paper is quite dense in math at some parts and it would benefit from more intuitive explanations. For instance, while the introduction of the chaining trees (Definition 1) was very clear, I was missing some discussion in the construction of locally adaptive boosting in Section 3.3.

Nevertheless, this paper is the first to present an efficient algorithm achieving locally adaptive regret relative to any Hölder class, without requiring knowledge of the local regularities or the loss curvature (while adapting to both). I would like to see the paper presented at the conference and so I recommend it for acceptance.

Some more remarks:

1) The online gradient boosting resembles an adaptive version of online gradient descent in online convex optimization (with parameter vector \theta). The analysis includes an additional step to bound the approximation error between the target Hölder function and the closest (in terms of cumulative loss) parameter \theta \in R^N, and uses the tree structure to bound the norm of \theta. Could you elaborate on the main novelties and challenges in your analysis compared to existing adaptive OCO results?

2) To be honest, I found the description of the locally adaptive boosting method somewhat challenging:
- When you say decision trees are ""sitting in nodes of a core tree"", does this mean they share a (sub)tree structure with the core tree?
- What role does each tree indexed by k=1,...,K play in the analysis. It seems that the only difference is that each tree at node n is initialized with a different \gamma_k.
- Could you clarify the node experts h_{l,t} as described in Algorithm 2? Are they are shared across different values of n and k (they are not indexed with n and k explicitly)?

3) How feasible is the method in high dimensions? It seems the computational complexity scales exponentially with dimension."
142,"This paper considers the problem of best-arm identification in
multi-armed bandit problems where the best arm is defined by the one
whose reward has the highest quantile (for some pre-specified
level). The authors study a variant of the problem when the player
only receives one bit of information from the reward of the selected arm.
More precisely, at each round, the player can ask a question about the
magnitude of the reward and receives a yes/no answer. To the best of
my knowledge, this exact problem has not been studied before.

The main results establish nearly matching upper and lower bounds for
the smallest number of arm pulls that guarantees (approximate) best
arm identification. The techniques are natural and well-known in the
area, though there are quite a few nontrivial details that the authors
need to sort out. The paper is well and clearly written. This is a
solid contribution and I support acceptance at ALT.

Some remarks and questions:

- is it necessary to assume that the range \lambda is known to the player?

- p.6, l.10: \tilde\epsilon is undefined at this point.

- the input parameter c in Algorithm 1 unnatural (as it is not part of
   the problem) and requires more discussion.

-p.11, l.6 and elsewhere: there are two conflicting definitions of the quantity
    \Delta_k. The one in (13) depends on the parameter c while the one appearing
    in Theorem 13 doesn't. This needs some fixing and clarification."
143,"The paper addresses a crucial healthcare challenge with interesting objectives. It leverages machine learning techniques to improve the identification and management of CKD, which is a significant contribution given the global burden of the disease. The methodology employed is robust, and the results presented indicate a high level of effectiveness in achieving the paper's goals.

However, despite the promising application and outcomes, this study primarily utilizes classical machine learning approaches without incorporating any pre-trained models, which diverges from the conference's emphasis on innovative model foundations and applications in clinical settings. This discrepancy could be seen as a deviation from the core subject matter expected at the conference.

Furthermore, while the paper effectively demonstrates the performance of ML techniques in reducing CKD underdiagnosis, it falls short in situating its findings within the broader context of current state-of-the-art methods. A comparative analysis, not limited to data-driven approaches, in terms of precision and cost-effectiveness, would have greatly enriched the paper. Such a comparison is essential for understanding the true value and innovation of the proposed method over existing strategies.

In conclusion, while the application of machine learning techniques to tackle CKD underdiagnosis is indeed valuable, the paper's methodological approach lacks the novelty and direct relevance to the ""Clinical Foundational Model"" conference theme. The absence of a comparative analysis with state-of-the-art methods further limits the paper's contribution to the field. Therefore, despite its potential impact in healthcare, the paper may not meet the innovation threshold required for acceptance into the conference.

Pros:
- Tackling a challenging problem in healthcare
- Great accuracy

Cons:
- Not related to the main conference theme
- Lack comparison with SoTA methods
- Lack a cost reduction analysis"
144,"The paper studies random exploration for linear bandits. Sufficient conditions for random exploration to result in nearly optimal regret are proposed; namely, smoothness and strong convexity. The class of randomized algorithms samples a $\theta = \hat{\theta}+ V \eta$, where $V$ is the least squares matrix, and $\eta$ is a randomly generated vector from a specific class of distributions. 
This class of algorithms contain Thompson Sampling as a special case, hence, proving the first near optimal frequentist regret bound for Thompson sampling, albeit under extra assumptions on the action set.

Strengths:
- The paper proves the first near optimal frequentist regret bound for Thompson sampling, albeit under extra assumptions on the action set.
- Valuable insights on when randomized exploration results in nearly optimal regret are presented. 
- The observation and proof that the proposed assumptions work is novel to the best of my knowledge. 
- The work allows to use algorithms with better computational complexity, than those based on the optimism principle such as LinUCB, for linear bandits.

Weaknesses:
- The analysis is specific to linear bandits, and do not seem to directly extend for contextual bandits. Despite that, I believe that the papers contribution is an important step.
- It is unclear if the proposed assumptions are necessary. However, I understand that it is difficult question to answer.

Questions:
Can the work in this paper be combined with the reduction from contextual to linear bandits in [] to extend the results for contextual linear bandits? Adding a discussion on that could increase the impact of this work.


minor comments:
- please fix broken link in page 3

[1] ""Contexts can be cheap: Solving stochastic contextual bandits with linear bandit algorithms."" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023."
145,"# Summary

The work presents a comprehensive—within the limited bounds of a brief survey—synthesis and investigation of the integration of Case-Based Reasoning (CBR) with Deep Learning (DL), which aims to integrate the interpretability and knowledge-driven nature of CBR with the powerful feature extraction capabilities of DL models. Starting with a survey that contextualizes the complementary advantages of both artificial intelligence paradigms, the authors systematically delve into various research efforts focused on enhancing DL interpretability with CBR, improving similarity assessments within CBR, and feature extraction using DL for CBR indexing.
The core contribution of the paper is the exploration of neural feature extraction for case retrieval in CBR systems, positing that optimal extraction strategies can substantially increase CBR accuracy.  DL can automate feature extraction, mitigating the challenges of manually acquiring symbolic knowledge, which can be not only resource-intensive, but also prone to inaccuracies.

# Quality and Clarity

The document demonstrates methodological thoroughness in examining the intrinsic strengths of both systems, and suggests that CBR provides inherent interpretability and analogizing capabilities that DL models alone lack. Furthermore, the work clearly elaborates on the advantages of integrating DL into CBR for tasks such as feature extraction for case indexing, suggesting this could enhance CBR applications in domains like image classification. The clarity of the paper is commendable, with clearly defined and well-written sections, and an organized progression from background literature to the authors' current research, facilitated by succinct explanations of complex concepts.

Clarity is only slightly affected by the fact that the ""(Anonymous)"" citation(s) about the authors' current work cannot be followed to verify the claimed results, but naturally that is not a fault of the authors.
# Originality and Significance

The novelty of this study lies in its proposed Neuro-Symbolic approach to feature extraction that has the potential to augment CBR retrieval accuracy for classification tasks. It is significant in that it addresses a potential gap between symbolic and neural systems—the issue of deep learning's opaqueness and the limited applicability of CBR in non-symbolic domains. By targeting improvements in interpretability without sacrificing neural network accuracy, the paper claims that the current work by the authors positions itself as a meaningful contribution to research in AI hybrid systems.

# Pros & Cons
## Pros:

1. The work presents a possible avenue for integration of DL's data-driven feature extraction capabilities with CBR's interpretability and knowledge integration, something that is becoming increasingly desirable in AI research. 
2. The paper presents a short empirical analysis based on different neural network architectures and the effects of feature extraction choices (based on the authors' current work, presumably published in other, anonymized, papers).
3. It proposes a beneficial fusion of knowledge-engineered and DL-extracted features, providing a pathway for synergistic integration of existing domain knowledge with learned representations.

## Cons:

1. It is not immediately clear if the described feature extraction can work on different types of NN layers, or if only specific layers are supported or can even be used (or if even that is desirable). Lacking such details, it is not clear if there is a potential limitation in the scope of DL architectures that can be used, which could affect the generalizability of the presented findings.
2. The paper's depth is restrained by its ""brief survey"" nature, possibly overlooking the complexities and challenges inherent in the integration of CBR and DL, but again, this is not a fault of the authors'.
3. The impact of the proposed methods on computational resources, such as training time or memory requirements, especially when using different neural architectures, is not clearly articulated.

# (Minor) Errors spotted

- *""First, CBR is interpretable, with human subjects study support for the effectiveness of cases for justifying model decisions (Gates, Leake, and Wilkerson 2023).""* - The sentence needs rewording to make it clearer, as is it's not clear what the point being made is.
- The specific versions of DenseNet and VGG (e.g. 16, 19, or both) being used are not specified.
- The acronyms ""CNN"" and ""k-NN"" are not defined before being used in the text - although their meaning is obvious within the context of the paper, they should be defined when first used. Also, it's not immediately obvious what ""KE"" means in Figure 1, it should be explained on the Figure or in the caption.
- *""…because of the the capability to incorporate retrieval knowledge…""* - The word ""the"" is repeated.
- *""We draw the following conclusions from experimental results reported in (Anonymous). as well as preliminary data conducted using…""* - A comma instead of a full-stop should be used after the citation."
146,"Advantages:
1. This paper proposes a novel method, which augments models using only the most predictive hypercube of synthetic data.
2. The motivation of the model is reasonable, and the application is important.

Disadvantages:
1. According to experimental results, the improvement from the model is relatively limited.
2. The author should introduce the concrete implementation process of the article in more detail."
147,"In this work, the authors present a preliminary study for estimating knee adduction moments from data acquired from accelerometers. The authors employ an LSTM-based network that includes an attention layer architecture. Over a cohort of 24 participants, 12 male and 12 female, whole-body motion was captured along with data from two IMU sensors. In addition to evaluating the prediction accuracy provided by the model, the authors analyze the attention weight and an XAI technique, LIME, to gain insights into the prediction made by the model. 

This work demonstrates a solid preliminary study into the feasibility of predicting knee adduction moments using IMU data. However, there are several points that could be addressed:

1. There is no report of the overall performance of the model. It seems like the reported example is for a single subject, but there is no report of the evaluation over the entire cohort
2. There is no comparison to other simpler models that would demonstrate the performance improvement. Although this is understandable for a preliminary feasibility study, the overall statistics would be important.
3. The authors report an 80/20 split, but was this split over the participants? Details such as this should be included for a better assessment of the results.
4. Although the authors admit the limitations of the analysis of explainability, how the explainability relates to the trustworthiness of the model is less clear. Moreover,  the approach does not seem particularly innovative, as it applies existing approaches, nor does it provide significant insights into the model and the broader field of biomechanics. 

Overall, the results are promising but would recommend another iteration of the manuscript before resubmission.

Additional minor comments:
- ASSWS is not explained in the paper
- The manuscript is over the 2-page limit for the non-traditional track"
148,"The RepViT-MedSAM proposed in this article follows the design and experimental ideas of Mobile-SAM, and has been improved to address the problem of uneven distribution of multi-modal medical images, which is a highlight of the article.
Additional data augmentation is utilized in the model distillation stage, but there is a lack of discussion on the effects of using data augmentation and insufficient details on how to improve the problem of uneven data distribution.
Highlights:
1. Detailed experimental details are provided and the complete project is open sourced.
2. Improvements have been made to address the problem of uneven multi-modal data distribution.
3. The overall structure of the article is complete and provides sufficient details for reproduction.
Areas for improvement:
1. The problem of uneven distribution of multi-modal data quantity was discovered, and a unique sampling strategy was proposed, hoping to reveal more details and increase ablation experiment comparison.
For example, at a certain sampling ratio, the model can ultimately achieve a more balanced result.
2. There is insufficient discussion on data enhancement, why data enhancement strategies such as Mixup should be selected, and how much improvement can be achieved after selecting these strategies
3. The final model has been improved in some modes, but has declined in some modes. This can be discussed appropriately.
4. Open source projects can add README files to provide simple instructions for reproducing the project."
149,"this is a very interesting approach to solve puzzles. It uses RL and graph neural networks. However it is outside my area of expertise. 

It is a well written paper and the ideas are well presented. I would like my review to be moderated based on the fact that I am not an expert in this field."
150,"The authors propose a feature mapping scheme termed RBF-PINN that uses radial basis functions to map/transform the inputs of a PINN network. They apply this to six cases for solving the forwards problem and four cases for solving the inverse problem. Their approach is contrasted with the original PINNs approach and several other feature mapping approaches.

I believe the authors have done a lot of work and the paper for the most part is clear to the reader. 

I am familiar with most of the discussed concepts in the paper (e.g. PINNs and feature mapping) but I have followed only to some extend the literature on PINNs to make strong claims regarding the novelty of the work. One of the things I would expect is to have a comparison with the work ``Adaptive activation functions accelerate convergence in deep and physics-informed neural networks’’ but since it is mentioned that this future step I don’t think is necessary here.

My main comments for the paper are:

(i)	Even though the paper is well written there are some cases the notation becomes confusing in my opinion. The author start by using the usual notation u(x,t) for the (time-dependent) PDEs and in equation (2) x_r for the collocation points. However, when they introduce Φ it seems to depend only on x (or bold x). This in my opinion might create some confusion. 
(ii)	Equation 5, a definition of $w_i$ is missing.
(iii)	Regarding the conditionally positive definite RBF. The authors introduce polynomial terms, and they claim this guarantees uniqueness. I would like to see a few references there that support this claim.
(iv)	Can the authors comment if there cases for which the introduction of the polynomial terms can lead to “explosions” of the gradient or to vanishing gradients? For example, when the degree of polynomial is large.
(v)	How the authors chose the parameter σ in the kernel? 
(vi)	In a few places in the text the authors mention the $l2$ loss where the corresponding figures shows $L_2$. Make things consistent. (There are figures also in the Appendix with this inconsistency).
(vii)	Figure 19 it’s not clearly readable. Perhaps the authors can make some changes there.
(viii)	Section Limitation of Fourier Features “Navier-Stokes” is misspelled.
(ix)	Perhaps, for future work, it might be of interest to the authors instead of sampling the coefficients c randomly to use approaches such the “Sampling weights of deep neural networks)”."
151,"This paper studies the problem of online list updates. Here, we have a list of $n$ elements and need to maintain a permutation of the elements at each time step. A query arrives at each time step in the form of an element from the list, and the cost is determined by the rank of this element in the permutation maintained by the learner. The learner then updates the permutation as more queries are observed, with the goal of minimizing the total costs.

This paper specifically studies the case when the queries are sampled i.i.d. from an unknown Zipfian distribution, $\text{Zipf}(\alpha)$, over $[n]$, where $p^{\alpha}[i] \propto \frac{1}{i^{\alpha}}$. The paper shows that the (expected) competitive ratio approaches $1$ as $n \to \infty$ compared to an offline algorithm that knows $p^{\alpha}$. Moreover, this result is achieved by the Transposition algorithm introduced by Rivest (1976). Finally, the paper also introduces an empirical approach based on deep reinforcement learning and provides evaluations of the proposed method.

I have reviewed most of the proofs (though I did not check the detailed algebra carefully in the appendix), and they appear sound. I find the paper interesting and recommend acceptance.


**Strengths**  
Although this paper is not directly within my field of expertise, I find the proof ideas quite interesting. The proof relies on a key technique (Lemma 1) from Rivest (1976) for characterizing the stationary distribution of the Markov process induced by the algorithm, together with some novel ideas for a more fine-grained characterization of the stationary distribution.

The paper also mentions that the derived result resolves a conjecture of Rivest (1976), though I cannot comment on its significance as I am unfamiliar with the relevant literature.

**Weaknesses**  
1. It seems that the guarantee requires the algorithm to run for a long time to ensure that the Markov chain mixes. Are there any bounds on the mixing time of the underlying Markov chain?
2. It is mentioned that Kan and Ross (1980), Tenenbaum and Nemes (1982), and Gamarnik and Momcilovic (2005) also studied Transposition under i.i.d. accesses. Can the authors comment on how their results differ from yours?
3. There are numerous typos, too many to list here. I recommend the authors conduct thorough proofreading to correct them."
152,"This paper provides a comprehensive analysis of the current state and future prospects of clinical Large Language Models (LLMs), focusing on domain adaptation strategies, performance comparisons, and the potential impact on healthcare. The following points should be considered:
1. How did the authors ensure the fairness of the results reported in Table 1, given the varying sizes, training data, and training methodologies employed by these models? Especially the differences in the evaluation settings on these two datasets, for example, BioGPT and GatorTronGPT employed prefix-tuning/prompt tuning to evaluate the pretrained LLMs, where the parameters of the LLMs are kept frozen, while PMC-LLaMA and BioMedGPT employed instruction tuning/fine tuning on the target dataset, where the parameters of the LLMs are trainable and may be better adapted to the downstream tasks.

2. The results in Table 1 are informative but could be enhanced, for example, the performance metric should be clarified, the column name ""Approach"" should be ""Training approach"", and the unit of parameter size should be clarified (e.g., 70B ->70 Billion). In the "" Training data"" column, there is some confusion about the unit (e.g., 160K data, 47B tokens). It would be beneficial to add a column to report the computational cost for each model.

3. Could the authors elaborate on the ethical and legal implications of using potentially copyrighted or sensitive patient data in training clinical LLMs?"
153,"Pros:
1. The paper is well-written and easy to follow.

2. The paper proposes an approach that solves the hand posed estimation task by a novel dense ordinary regression. Moreover, the new discretization method and ordinary regression loss are introduced.

3. Extensive experiments were conducted and demonstrate improvements.

Cons:
1. Missing recent SOTA literature. Table 1 should include the following recent published results for comparison.

[R1] Cheng W, Ko J H. HandR2N2: Iterative 3D Hand Pose Estimation Using a Residual Recurrent Neural Network[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 20904-20913.

[R2] Ren P, Chen Y, Hao J, et al. Two heads are better than one: image-point cloud network for depth-based 3D hand pose estimation[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(2): 2163-2171.

2. The best values in Table 5 should be highlighted as bold.

3. Line-322, how to obtain the ""hand center point"" for each dataset should be clarified. Calculating average location of GT joints? Or using the estimated center point provided by V2V [16]?

Minor editorial issue:

--Line-081, traing -> training"
154,"**Summary** 

The paper studies strategyproof learning problems in the setting of algorithms with predictions.  In strategy-proof learning, agents might manipulate some data labels for their own benefit, and the objective of the algorithm is to be robust in the face of these manipulations. The authors assume that the algorithm is given an advice on the optimal mapping $f$, belonging to a class $\mathcal{F}$, between the data space and the labels space, that minimizes the global risk. The objective then is to use this advice to have a consistent and robust algorithm for learning $f$. 
The authors first focus on regression, with $\mathcal{F}$ restricted to the set of constant functions, and they design a deterministic algorithm with an optimal consistency-robustness tradeoff.
Then, for classification, they prove impossibility results in the case of three or more labels, then they design deterministic and randomized learning-augmented algorithms for the case binary classification.


**Strengths**

- The studied problem is very interesting. Many decision-making problems have been studied in the learning-augmented setting, but there haven't been much work considering learning problems in this framework, and none (to my knowledge) has studied strategyproof learning.
- The paper is well-written, and the authors give intuitions on their algorithms and proofs.
- The paper explores multiple learning problems (regression, binary classification, classification with more than two labels), and provides strong results.
- The consistency-robustness tradeoff for regression with constant functions is optimal, and setting $\gamma = 2$ gives the optimal approximation ratio without advice.
- The authors also give smoothness guarantees, showing how the consistency and robustness bounds are interpolated as the advice error becomes more important.

**Weaknesses**

- The authors do not provide any experimental results supporting the practicality of their algorithms.
- For regression, the authors only consider constant functions, which is very limited.
- Since the paper focuses on ""learning with advice,"" maybe the authors should cite papers studying similar problems, such as ""Online Classification with Predictions"" (Raman, Tewari) or ""Learning-Augmented k-means Clustering"" (Ergun, Feng, Silwal, Woodruff, Zhou).
- I am unsure how the advice on the optimal function $f$ can be obtained in practice. The authors mention that it is weaker than having advice on the true labels, but I don't see how this can be obtained either."
155,"This paper studies the generalization error (sample complexity) of boosting-type algorithms.
Specifically, this paper proposes a new boosting algorithm that improves the generalization error over AdaBoost by a logarithmic factor. The technical contribution includes a randomized extension to sample compression schemes.

Context: The generalization error of the classic AdaBoost algorithm is roughly $\frac{d \log^2(n)}{\gamma^2 n}$, where $d$ is the VC dimension, $n$ is the sample size, and $0 < \gamma \leq \frac{1}{2}$ is the weak learning parameter. Recently, Larsen and Ritzert (2022) proposed a new boosting-type algorithm that removes the $\log^2(n)$ term. However, unlike AdaBoost, which uses a simple majority vote (""voting classifier"") as the decision rule, their algorithm employs a more complex “majority of the majority” rule.
The goal of this paper is to improve the generalization error of AdaBoost while retaining a simpler voting classifier.

This paper proposes such an algorithm, achieving a generalization error of roughly $\frac{d \log(n)}{\gamma^4 n}$. This represents an improvement of a $\log(n)$ factor, with only a mild trade-off in terms of dependence on $\gamma$ (which can be considered a constant). The algorithm is similar to AdaBoost, but with a key modification: while AdaBoost maintains distributions $D_k$ over the sample and invokes a weak learner oracle on the training set with respect to this distribution, this algorithm instead subsamples a set of size approximately $\frac{d}{\gamma^2}$ from $D_k$ and applies the weak learner to that subsample.

To analyze their algorithm, the authors introduce a randomized version of sample compression schemes. A sample compression scheme consists of a compression function that selects a small subset of the training set and a reconstruction function that takes this subset as input to produce a classifier consistent with the training set. A randomized compression scheme defines a distribution over compression functions (rather than a single function), with the guarantee that, with high probability under this distribution, the classifier produced by this compression remains consistent with the training set. The proposed algorithm fits within this framework as a randomized compression scheme.

A key technical contribution that allows for the $\log$-factor improvement is that the algorithm’s output is not only a randomized compression scheme but is also stable. A compression scheme is stable if removing points from the training set that are not part of the subset chosen by the compression function does not change the output of the compression function. An intuitive example SVM: if we remove points that are not support vectors, the resulting support vector set (and thus the output) remains the same. The benefit of stable compression schemes is a $\log(n)$ improvement in generalization error, which is the key idea behind the paper’s improvement by a $\log(n)$ factor. The additional $\gamma^2$ term arises from the subset sizes used in each iteration, which are approximately $\frac{d}{\gamma^2}$.

Overall, the idea of randomized compression schemes is interesting, as is the fact that it is possible to construct such compressions that are stable with respect to any hypothesis class, as opposed to previously known stable compressions, which are limited to specific classes."
156,"The work discusses the recent advancements in Deep Learning Weather Prediction (DLWP) models, highlighting their potential to rival traditional numerical weather prediction (NWP) models. Various DLWP architectures, including U-Net, Transformer, Graph Neural Network (GNN), and Fourier Neural Operator (FNO), have shown promise in forecasting atmospheric states. However, due to differences in training protocols, data choices, and forecast horizons, it remains unclear which method and architecture are most suitable for weather forecasting. The study conducts a detailed empirical analysis comparing these architectures under controlled conditions, focusing on predicting two-dimensional incompressible Navier-Stokes dynamics. Results indicate favorable performance of the Fourier Neural Operator (FNO) backbone in terms of accuracy, memory consumption, and runtime when compared to Transformer, U-Net, and GNN backbones.
 
In general, the present study is of interest for the DL-based weather forecasting community, but also in particular for benchmarking for the machine learning community. The aim of the paper is clearly defined in the manuscript. The text is technically well-written and follows a clear structure and logic. The theoretical concepts are concisely presented in the main text, with further details described in the appendices. The figures and the experiments are in general sound, and the results support the conclusions. While these conclusions are based on synthetic data as a first step, and it is clear limitation of this work, it is fair to point to future work to confirm or challenge these results based on real-world weather data sets."
157,"This submission concerns the construction of transformations between statistical experiments. Suppose we have two experiments about the same underlying parameter: 
$(\mathbb{X}, \lbrace \mathcal P_{\theta}\rbrace_{\theta\in\Theta})$ and
$(\mathbb{Y}, \lbrace \mathcal Q_{\theta}\rbrace_{\theta\in\Theta})$.
Here $\mathbb{X}$ and $\mathbb{Y}$ are data spaces and $\mathcal{P}, \mathcal{Q}$ data distributions. The basic problem is to produce a randomized mapping $K$ such that, if $X\sim \mathcal P_{\theta^*}$, then $K(X)$ is approximately distributed according to $\mathcal{Q}_{\theta^*}$. Crucially, the mapping does not access $\theta^*$. This is a classical setup in statistics. 

The submission gives a general transformation based on rejection sampling. The algorithm assumes access to some function $S(y\mid x)$, a signed kernel between $\mathbb{X}$ and $\mathbb{Y}$. They show that, when this kernel satisfies certain properties, the transformation is good. The rest of the paper focuses on constructing such kernels for specific cases. These include Laplace, Uniform, and Erlang source distributions. Some analysis is for general classes of targets, but we only see complete analyses for univariate Gaussian and log-concave distributions (if I understand correctly).

The results appear non-trivial and, to the best of my knowledge, new. I am not an expert on this topic; I hope the other reviewers will weigh in on contribution here.

I felt there was gap between the results and the informal discussion. The abstract says ""we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families."" But the formal claims of computational efficiency depend on the target, and are only provided for specific targets (eg, Gaussians). Indeed, it seems that such a claim *cannot* be true, if you believe that there are families of distributions which are computationally hard to sample from. It is not clear to me that this approach even gives us an efficient transformation from multivariate Laplace to multivariate Gaussians; might the rejection sampling scale poorly with the dimension?

Does any prior work construct approximate, single-sample reductions for continuous $\Theta$? From the related work I gather the answer is ""no,"" but it was not clear. If that is correct, it should be emphasized more.

Finally, I felt the presentation of the paper leaves much to be desired. I think the proofs would benefit from a handful of pictures to illustrate how they work for simple cases. The first part of the introduction does not address computation at all, and while the related work seems to discuss computational lower bounds it is not clear to me what the cited results imply about the question at hand. 

In Section 3 in particular, I was a often confused by the presentation of the RK algorithm and the associated Lemma 1.
- We compute the Radon-Nikodym derivative $\frac{\mathrm{d}\bar{\mathcal{S}}}{\mathrm{d}\mathcal{P}}(y\mid x)$, but I do not see how to do this using only sampling access to $\mathcal{P}(\cdot\mid x)$.
- The algorithm uses a base measure $\mathcal{P}(\cdot\mid x)$, which is different from the source distribution $\mathcal{P}_\theta$. We also use $p(x)$, which is not a distribution. This notation caused some confusion for me.
- The algorithm takes some $y_0$ as initialization and Lemma 1 quantifies as ""for all $y_0$..."", but it looks like the algorithm doesn't use $y_0$ unless all proposals are accepted. Initializing a dummy $Y\sim \mathcal{P}(\cdot \mid x)$ would seem more natural to me, unless I misunderstand.
- Lemma 1 also says ""for any $\epsilon\in (0,1)$ and $x\in \mathbb{X}$"", but I do not see where these quantities show up. We do see ""$x\mapsto \mathrm{RK}(x,N,M,y_0)$"" but this refers to the algorithm, when we bound the running time.
- I had quite a bit of difficulty with the proof of Lemma 1. For example, the kernel $\widehat{T}(y\mid x)$ is defined in the main text in Eq (6), but in the appendix it appears without a pointer back to its definition. Also, for proving Eq. (49a), isn't it clear that the TV distance is upper bounded by the probability of failing to accept after $N$ steps? That's Eq. (100). I don't understand what purpose the other steps serve; a little more English text would go a long way here."
158,"### Summary
This paper studies the connection between
* PAC learning a (Booelan) concept class $\mathcal{H}$ over input space $X$ _under distributional assumptions_, and
* the problem of _density estimation_, where the goal is to learn the marginal distribution with respect to events in $\mathcal{H}$.

The distributional assumption is modeled by requiring that the marginal distribution over inputs is some distribution $D$ that belongs to a known family $\mathscr{P}$. It is said that $(\mathscr{P}, X, H)$ is PAC-learnable with sample complexity $n(\varepsilon, \delta)$ if there exists an algorithm which for every $D \in \mathscr{P}$ and $h \in H$ outputs a predictor $h'$ such that $\Pr_{x \sim D}[h'(x) \ne h(x)] \le \varepsilon$ with probability at least $1 - \delta$.

Formally, the problem of density estimation is, given an input dataset $T \sim D^n$, to output a density $D'$ such that $TV_{H \Delta H}(D', D) \le \varepsilon$ with probability at least $1 - \delta$, where $TV_{H \Delta H}(D', D) := \sup_{h, h' \in H} |d_{D'}(h, h') - d_D(h, h')|$ where $d_D(h, h') := \Pr_{x \sim D}[h(x) \ne h'(x)]$.\
This paper studies two different weakenings of this definition, in both of which instead of outputting a density $D'$, it is only required to output a distance estimator $\tilde{d}_T : H \times H \to \mathbb{R}^+$:
* _Intermediate Density Estimation (IDE)_: It is required that there exists a set $G\_T$ (that depends on the dataset $T$ such that for all $g, g' in G\_T$ it holds that $|\tilde{d}_T(g, g') - d_D(g, g')| \le \varepsilon$ holds with probability at least $1-\delta$, and that any fixed hypothesis $h \in H$ belongs to $G_T$ with probability at least $1-\delta$.
* _Weak Density Estimation (WDE)_: The same as IDE, but where the set $G\_T$ may be unknown to the learner (hence, denoted $G_{T, D}$).

The main result of the paper is to show that PAC-learning lies strictly between the notions of IDE and WDE. Namely,
* any $(\mathscr{P}, X, H)$ that admits IDE, also admits PAC-learning, and moreover, this is strict, in that there exists an example that admits PAC-learning, but does not admit IDE.
* any $(\mathscr{P}, X, H)$ that admits PAC-learning also admits WDE, and moreover, this is strict, in that there exists an example that admits WDE, but does not admit PAC learning.

Finally an equivalence is shown between $(\mathscr{P}, X, H)$ satisfying the _uniform estimation property_ and admitting density estimation (although the converse also requires uniformly bounding the metric entropy).

### Relevance and Recommendation
The paper provides a new insight on PAC learning under distributional assumptions, and provides a novel understanding in terms of _intermediate_ and _weak_ density estimation. I feel this paper is likely to inspire follow up work. So I recommend acceptance.

### Questions for Authors
One thing that was not clear to me was whether the $\tilde{d}_T : H \times H \to \mathbb{R}^+$ as constructed in this work always corresponds to an actual density function over $X$? (I understand this is not relevant to the proofs in the paper, but was just curious for my own sake.)

### Minor Comments

(Page 7) Proposition 16: Should $m(\varepsilon)$ be in fact $m_{\mathrm{UB}}(\varepsilon)$ ?"
159,"The abstract introduces two clinical LLMs, GatorTron and GatorTronGPT, and their applications in a clinical context. To this end, the authors review the training dataset of the two LLMs and summarize their performances in various evaluation tasks within a medical context. The abstract highlights the necessity of evaluating LLMs specialized for clinical purposes. The detailed description of the applications of the two LLMs provided a comprehensive picture for audience. Some concerns arose when I was reading the abstract:
1. The abstract tries to provide resources to facilitate the use of GatorTron, but I could hardly find lines addressing this claim throughout the abstract.
2. GatorTron and GatorTronGPT were distinguished from general-purpose LLMs in that they were trained with medical corpora. Hence, I’m curious about the performance contrast of a general-purpose model like ChatGPT (as a benchmark), and a specialized medical LLM like GatorTron. A direct comparison such as accuracy scores in medical NLP tasks (if it is possible to quantify their performance) between the two kinds of model will do. 
3. The “Conclusion and Discussion” section gives a nice summary about the applications of the two LLMs in clinical contexts. From my personal conjecture, readers may want to know more about the state-of-art news of GatorTron and GatorTronGPT, such as the technical challenges we are dealing with. Put such questions in a bigger picture, audience may be interested in where we are going to, and what we will be able to do with GatorTron and GatorTronGPT in the future. I believe these topics fit well in the discussion session and can contribute to the discussion in the symposium. 
In sum, the abstract provides a clear review of GatorTron and GatorTronGPT, a significant work in clinical NLP. As a review of existing studies, this abstract is inevitably short of originality. An additional discussion on the state-of-art and limitations of the present models will make the abstract more beneficial to the conference. Recommended."
160,"# Summary
The paper proposes a novel training method for neural differential equations based on the variational formulation (VF) of ODEs.
The benefit of the proposed method is that it bypasses the need for numerical ODE solvers during training, with the result that training may be accelerated by multiple orders of magnitude.
Additionally, the paper addresses the challenges of oscillatory integral terms in the proposed VF loss using Filon's method.
In a number of experiments, the proposed method is 1-2 orders of magnitude faster than the baseline architectures.

# Strengths
1. (**significance**)
The proposed method is well motivated and has the potential to solve a real problem with high impact across multiple scientific disciplines.
2. (**quality**)
The use of Filon's method to address the oscillatory integral problem is a key component of the paper's methodological contribution.
3. (**quality**, **clarity**)
The paper is well written and enjoyable to read.

# Weaknesses
1. (**clarity**)
The paper builds upon the work of Qian et al. [1], but I think a few more sentences would help to rigorously connect the two settings, which are slightly different (neural differential equations versus closed-form ODE discovery).
In particular, Qian et al. [1] go to great lengths to justify their training objective, an optimization problem over a function space.
The training objective of this paper (Eq. 4) is presented without comment as equivalent to Qian et al.'s.
After some consideration, I think the equivalence is probably valid.
However, I think the clarity of this paper would be improved if this step were justified more explicitly.
1. (**quality**, **completeness**)
The main issue I have with the paper in its current form is the choice of experiments.
To the best of my understanding, the ""Fast-VF Neural ODE"" is a standard, vanilla neural ODE architecture trained using the proposed VF method.
This is then compared to a number of more exotic neural ODE architectures, each of which is trained using three standard solver-based approaches (discretize-then-optimize, optimize-then-discretize, seminorm).
The problem is that each of these baselines is effectively ""two steps removed"" from Fast-VF Neural ODE, that is to say, both the architecture *and* the training method are different, meaning I cannot properly evaluate the effect of the proposed training method in isolation.
What I would like to see, at a minimum, is a comparison of Fast-VF Neural ODE with another vanilla neural ODE that was trained using each of the three solver-based approaches.
Conversely, if possible, it would also be interesting to see the more exotic architectures trained using the proposed VF method, which I think is possible in principle.

# Other Comments
1. A number of methods already exist for simulation-free training of neural ODEs in the context of continuous normalizing flows, where they parametrize probability paths; see, for example, [2-4].
I'm not sure whether these methods could be adapted to the present setting, but I think this paper would benefit from a discussion of them.
1. It seems to me that applying Filon's method is essential to making this method work.
With that in mind, I think future versions of this paper would benefit from a discussion of its limitations.
For example, are there situations where Filon's method might fail?

# Conclusion
Overall, I like this paper a lot.
Unfortunately, the lack of more meaningful and direct comparisons, as discussed in the second weakness above, prevents me from giving the paper a higher score at the moment.

# Citations
[1] Zhaozhi Qian, Krzysztof Kacprzyk, & Mihaela van der Schaar. D-CODE: Discovering Closed-form ODEs from Observed Trajectories. In *The Tenth International Conference on Learning Representations*, 2022.

[2] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow Matching for Generative Modeling. In *The Eleventh International Conference on Learning Representations*, 2023.

[3] Heli Ben-Hamu, Samuel Cohen, Joey Bose, Brandon Amos, Maximillian Nickel, Aditya Grover, Ricky T. Q. Chen, Yaron Lipman. Matching Normalizing Flows and Probability Paths on Manifolds. In *Proceedings of the 39th International Conference on Machine Learning*, PMLR 162:1749-1763, 2022. 

[4] Noam Rozen, Aditya Grover, Maximilian Nickel, Yaron Lipman. Moser Flow: Divergence-based Generative Modeling on Manifolds. In *Advances in Neural Information Processing Systems*, 2021."
161,"This paper tackles the challenge of biased predictions in machine learning systems within the clinical domain, where demographic information may be unavailable due to privacy concerns. It introduces an adversarial weighting architecture that utilizes model gradients to identify and prioritize underrepresented groups, offering a novel approach to improving fairness without relying on demographic data. Unlike conventional methods, this approach provides a robust mechanism that significantly enhances fairness while preserving overall accuracy, marking a notable advancement in the quest for fair and reliable machine learning systems in healthcare."
162,"This paper studies the PAC sample complexity of comparative learning for both the benchmark proper and the general learning settings. For the benchmark proper setting, a new dimension (one-sided mutual graph dimension) is proposed based on which novel upper and lower bounds for the sample complexity of ERMs are proved. The authors also provide a family of examples which have mutual VC dimension 0 and one-sided mutual graph dimension increasing to infinity. They show that their lower bounds for ERMs also holds for any proper learner in those examples. They also propose a sufficient condition for quadratic dependence of sample complexity on $1/\epsilon$ in benchmark-proper setting. For the general comparative learning setting, they define diameter and joint diameter for partial classes via which sufficient conditions for both linear and quadratic dependence of sample complexity on $1/\epsilon$ are proposed. 

Pros: The problem studied in this paper represents a meaningful and intriguing extension of standard PAC learning, offering substantial new avenues for exploration. The contribution of this paper is original and significant especially for the benchmark proper setting. The paper is well-organized and written clearly with substantial technical illustration. 

Cons: I didn't find any major issues with the paper. I have the following comments for potential improvement.
1) For the proof of Theorem 12, there is no justification of the claim that ""A sample of size
... contains every point with mass at least $\epsilon/d$ with probability more than $1-\delta$"". I suggest the authors to add a reference or provide a justification for this claim. 
2) In page 12, the following existing result is cited: ""any agnostic learner for the agreement class comparatively learns source and benchmark in the sense of Definition 1 (Hu and Peale, 2023)"". Since it is a key argument used in the proof of Theorem 15, I believe it would be better to cite it as a lemma or refer to the exact position where it is stated in Hu and Peale (2023). 
3) There are some typos. In page 3, there are two ""both"" in the sentence ""both both linear and quadratic dependence..."" and there is no period in that sentence. In page 10 after the title of section 4, it should be $\tilde\Theta$ instead of $\tilde\theta$."
163,"**Summary**: It is known that for noiseless sparse problems, rotation-invariant algorithms perform significantly worse than non-rotation invariant algorithms.  This paper shows that this qualitative statement holds true even when the sparse problems have noise.  Part of the reason why this is interesting is that these new lower bounds hold for the case where the sample size n exceeds the input dimension d  (whereas in the noiseless case, the sample size n is necessarily smaller than the input dimenion d).  The high-level idea to build such lower bounds is to first rotationally symmetrize the problem, and then build lower bounds for this symmetrized problem.  

**Recommendation**: Based on the strengths and weaknesses outlined below, I lean towards rejecting the paper.  Even though the main lower bound is interesting, it in itself seems insufficient for acceptance.  To supplement the result, the authors do provide some results on continuous time trajectories, but these are not especially interesting, and quite disjointed/unrelated to the rest of the paper.  In addition, the writing is quite poor.  This reviewer is knowledgeable in the optimization side of things, but less so on the statistical side --- as such his confidence score is on the low end.

**Strengths**:
1. The main result (that rotation-invariant algorithms suffer on noisy sparse problems) is interesting, especially because in this case the sample size can exceed the input dimension.  To the author's knowledge, the proof of the lower bound seems correct.  The high-level idea of first symmetrizing the problem, and then proving lower bounds for this symmetrized problem is very natural.  [[I am not sure if this approach has been used previously in the literature (it has certainly been used to prove optimization lower bounds).]]
2. Literature is cited appropriately, and ideas seem to be correctly attributed.

**Weaknesses**:
1. The main result confirms an existing high-level observation (rotation-invariant algorithms suffer); it does not propose or support a *new* observation.
2. This reviewer finds the results in Sec 3 (closed-forms for continuous-time trajectories) uninteresting, and also disjointed from the main story on lower bounds.  Moreover, this qualitative observation that certain continuous time algorithms spend time near sparse solutions is already very well-established.  Instead of having this section, it would be better if the authors could supplement their lower bound by answering some of the open questions laid out in Sec 5.  This would strengthen the paper. 
3. The paper is very poorly written.  As just one early example, ""prove much lower upper bounds"" is confusing.  Something like ""prove much better upper bounds"" sounds better.

**Other small points**:
* Theorem 3 (equivalence of continuous-time mirror descent, Riemannian gradient descent, preconditioning) is folklore in the optimization community, and is not new.

**Questions**:
* It is known that for fully connected networks and symmetric initialization, gradient descent is rotation-invariant.  To build non-rotation invariant algorithms, is there a natural non-symmetric initialization one can use which makes fully-connected neural nets prefer sparse solutions?  For example, what if the intialization itself is sparse?"
164,"# Peer Review for the Manuscript: ""Evaluating Large Language Models for Race-Based Medical Content""

## General Evaluation

The paper presents an interesting study on the use of Large Language Models (LLMs) for identifying and evaluating race-based content in medical contexts. This topic is both timely and relevant, given the increasing reliance on LLMs across various sectors, including healthcare. The authors have made a commendable effort to highlight the challenges and nuances associated with race-based content in LLM outputs.

## Specific Feedback

### Strengths

1. **Relevance and Novelty:** The study addresses a critical gap in the current understanding and evaluation of LLMs in handling sensitive and crucial topics such as impact of racial stereotypes in medical advice.
2. **Methodological Approach:** The structured comparison across different LLMs provide a solid basis for the study's findings.

### Areas for Improvement

1. **Typos and Clarifications:** The paper mentions ""nine unique LLM-prompt combinations"" which should correctly be ""twelve unique combinations."" Attention to such details is crucial for the accuracy of the paper.
2. **Consideration of Skin Tone Variability:** The use of a more comprehensive skin tone classification, such as the Monk Skin Tone Scale ([Monk Skin Tone Scale](https://skintone.google/)), would enrich the study by providing a more nuanced understanding of race as it pertains to medical content. The current set of 13 prompts is limited, it is predominately representing black, white and some asian race. Recommend the authors to formulate a more diverse prompts by considering more examples of race-stereotype combinations.
3. **Benchmark Evaluation:** The paper states that there is a lack of methods to evaluate harmful content regarding race. This is not true, major players in Generative AI space like OpenAI, Meta, google all of them have released trust and safety scorecards and responsible AI covering bias and stereotypes is a big focus. However, existing benchmarks and datasets could be explored for their representation of race-related medical data. The absence of this exploration is a missed opportunity to contextualize the study's findings within the broader research landscape.
4. **Physician Backgrounds:** The background of physicians involved in the original research, particularly their awareness of bias and civil rights, is not detailed. This information is crucial for understanding the potential biases in the study's setup and interpretation.
5. **Statistical Measures:** A clearer explanation of the statistical measures used (Sensitivity, Specificity, NPV, PPV, F1) would make the paper accessible to a broader audience, including those not familiar with these terms.
6. **Methodology Suggestion:** Given the limitations of zero-shot prompting in niche domains like race-related medical data, exploring few-shot prompting or fine-tuning the models might yield more accurate results.

### Recommendations for Further Research

The authors are encouraged to explore the representation of race-related bias in publicly available datasets and benchmarks, such as those hosted on platforms like Hugging Face and Stanford's CRFM. Investigating these resources could provide insights into the current state of race representation in LLM training data and benchmarks. Furthermore, the authors should consider building and open-sourcing a dataset specifically for evaluating race-related content in medical advice. This contribution would significantly benefit the research community by providing a specialized resource for further studies.

## Some benchmarks for reference
- [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- [Stanford CRFM HELM Lite](https://crfm.stanford.edu/helm/lite/latest/)
- [Hugging Face Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
- [Artificial Analysis](https://artificialanalysis.ai/)
- [Martian Leaderboard](https://leaderboard.withmartian.com/)
- [Hugging Face Enterprise Scenarios Leaderboard](https://huggingface.co/spaces/PatronusAI/enterprise_scenarios_leaderboard)

## Conclusion

The paper ""Evaluating Large Language Models for Race-Based Medical Content"" contributes important insights into the evaluation of LLMs for sensitive content. With the recommended revisions and further exploration of the highlighted areas, this paper has the potential to significantly impact the field."
165,"## Summary
The authors present a method to approximate the electronic ground-state wave functions in molecules. To this end, related work has mostly proposed to approximate individual wave functions within the Slater-determinant using neural networks. These determinants are then combined linearly and multiplied with a Jastrow factor. In contrast, this paper suggests to replace this linear combination with a neural network which is constructed to guarantee the anti-symmetric property of the wave function. The authors provide proof that the network structure guarantees the anti-symmetric property. They go on to show, that for every function that can be represented by their neural network there is a Jastrow-Slater-Wavefunction that is identical to it wherever (phi(r)^T x) is non-zero. In empirical experiments they do not find a clear advantage of their method.

## Suitability for this workshop
I think the results are interesting to the workshop as the authors build a specialized neural network to ensure certain properties of the solution.

## Comments
While I would have wished for more details on the neural networks for this venue, I think that given the page limit, the selection of information is comprehensible. I think that the paper is clearly written. It was sometimes difficult to follow for me as the paper requires some background in quantum mechanics that I lack. However, given the page limit I do not think a longer introduction to this topic is feasible. I think the discussion of the results is good, only the final sentence of the abstract might be a slight over-claim because it is difficult to claim it being impossible from it not showing in one experiment.

##  Justification of Rating
I think this is a very good work as it presents empirical experiments that are well motivated and further gives good theoretical insights why these empirical experiments turned out the way they did. The only downside in my opinion is that the focus is much more on quantum mechanics and less on learning representations. Nevertheless, I think this paper should clearly be accepted."
166,"Summary: 

This manuscript shows that for linear bandits satisfying certain conditions (detailed below), a randomized exploration around the ridge least square (RLS) estimate at every step achieves the optimal $\widetilde{O}(d\sqrt{n})$ regret, where $d$ is the dimension and $n$ is the time horizon. The specific algorithm is very simple: at each round $t$, let  $\widehat{\theta_t}$  be the current RLS estimate of $\theta^\star$, the learner draws a random $\theta_t = \widehat{\theta_t} + V_{t-1}^{-1/2}\eta_t$ with some noise vector $\eta_t$ (where $V_{t-1}$ is the usual information matrix in RLS up to time $t-1$) and play the optimal action associated with $\theta_t$. When $\eta_t$ is the standard Gaussian noise, the result implies that linear Thompson sampling achieves the optimal regret for linear bandits under the frequentist setup, while existing work only established a regret upper bound $\widetilde{O}(d^{3/2}\sqrt{n})$. 

The key technical idea of this manuscript is to bypass the optimism-based arguments. Specifically, in the existing work [Abeille and Lazaric, 2017], it was shown that if $J(\theta_t) \ge J(\theta^\star)$ (i.e. optimism) happens with probability at least a constant (here $J(\theta)$ is the maximum reward when $\theta$ is the truth), then a UCB-style upper bound works. However, for the above lower bound to hold, the variance of $\eta_t$ must be inflated by an additional factor of $d$ which explains the additional $\sqrt{d}$ factor in the final regret. The main intuition of the current work is that, even if $J(\theta_t) \le J(\theta^\star)$, the randomized exploration due to the noise part $V_{t-1}^{-1/2}\eta_t$ will provide a sufficiently large information gain from $V_{t-1}$ to $V_t$, and mostly importantly make $V_t$ essentially balanced along all directions. This argument critically relies on the dependence of the optimal action $x_t = \nabla J(\theta_t)$ in the change of $\theta_t$, and therefore the authors need to make the following assumptions: 

1. The action set is essentially continuous; 
2. The function $J(\theta)^2$ is both strongly convex and smooth. 

Comments: 

Overall I like this manuscript. I think it is a significant result to show that linear Thompson sampling achieves the optimal frequentist regret even for linear bandits under $\ell_2$ balls, and the technical argument beyond optimism could be of independent interest. I think this paper is a great fit for ALT and I recommend acceptance. 

I don't have major concerns about this manuscript, but I do have several questions and comments to discuss with the authors: 

1. Do you think the consideration of $J(\theta)^2$ in Assumption 2 essential? It seems that imposing strong convexity and smoothness conditions on $J(\theta)^3$ can also make the proof go through - what do you think? A deeper understanding of this question may help identify a weaker set of conditions to make the current proof work through. 

2. Do you think if it is possible to get rid of the optimism-based arguments completely? Right now the condition $J(\theta_t) \le J(\theta^\star)$ is only used at some technical steps, such as in the manner of $|a^2 - b^2| \le 2b|a-b|$ when $a\le b$. In addition, on a high level the intuition for randomized exploration should work for the case with optimism as well (up to certain technical subtleties). What do you think? 

3. Right now the intuition behind Lemma 5 - Claim 7 is not well reflected in the technical analysis. I find the intuition to be simple under the $\ell_2$ ball case, but the current technical analysis is a bit lengthy and annoying. Given that some steps seem to be pretty loose (e.g. replacing $V_{t-1}$ by $V_n$ in the proof of Claim 7), I think a more direct and transparent proof should be possible and beneficial to the community. 

4. The authors are encouraged to discuss the necessity of the assumptions here. I understand that the lower bounds in [Hamidi and Bayati, 2023; Zhang, 2021] for Thompson sampling imply that certain assumptions are necessary, but on the other hand the $\widetilde{O}(d^{1.5}\sqrt{n})$ upper bound in [Abeille and Lazaric, 2017] with an inflated variance does not seem to require any such conditions. 

5. On Page 8, why is $\sqrt{d}$ used in the upper bound $|\xi_t|\lesssim \sqrt{d}$? I feel that even $\widetilde{O}(1)$ seems true? 

6. On the last display on Page 10, although the middle term is polylog($n$), the dependence on $d$ there seems to be $d^{2.5}$. Does this mean that the final regret bound is $\widetilde{O}(d\sqrt{n} + d^{2.5})$, which is optimal only if $n \ge d^3$? 

7. Page 18, before Appendix C: should $\pi$ be $M$ instead?"
167,"## Summary

The key argument of this paper is that, Rubik's cube solver that is built with ARL may produce solutions that are not intuitive to human players. Thus, this paper propose to incorporate human prior from 10,000 collected human solutions with IRL.

## Pros

- This paper proposes to use IRL to enforce Rubik's cube solver to produce human-like solutions.

## Cons
- This paper does not evaluate the interpretability of the built Rubik's cube solver
- For well-defined symbolic problems, such as Rubik's Cube solver and Go, human-like manner does not implies good interpretability. In particular, the reviewer would consider it more meaningful to mine new Rubik's cube solving rules from automatically built solvers.
- It would be nice if the authors could use a pre-trained LLM as an initial policy model"
168,"Pros:
1. This paper presents an agent equipped with such an abstract machine is able to solve a larger set of tasks than those utilizing current approaches. Unlike previous approaches, which are limited to the expression of tasks as regular languages.
2. This paper shows that the state machines required in their formulation can be specified from natural language task descriptions using large language models. Empirical results demonstrate that their method outperforms competing approaches in terms of sample efficiency, automaton complexity, and task completion.

Cons:
1. The version of ChatGPT been used for evaluating in the paper did not been clearly mentioned. The experiment results in terms of sample efficiency from Figure 6 did not show explicitly better than the other baseline methods.
2. The experiment details include the description for the dataset such as train/dev/test sets, the name of the dataset, a detailed description or example for counterfactual reasoning are missing. Figure 6 says the mean and variance were reported over 10 independent trials. A confidence interval will be needed in this case."
169,"This paper develops sample compression algorithms for a handful of learning settings via explicit reductions to the simpler (and better understood) binary classification setting. The authors investigate 3 learning settings: 1) Multiclass classification, 2) bounded regression with lp losses and 3) adversarial robust learning. Their main contribution is to show that in these settings there is a simple, yet general, argument of the following form: for given (multiclass, regression etc) function class $\mathcal{F}$ with complexity parameter $d$, there is an associated binary function class $\mathcal{F}'$ with vc dimension $d$ such that a compression scheme for $\mathcal{F}'$ with compression size $f(d)$ can be transformed into a compression scheme for the original non-binary class F with compression size $O(f(d)\log(|\mathcal{Y}|)$ where $|\mathcal{Y}|$ is the number of labels (or a comparable notion for regression and adversarial robust learning). Furthermore, given more ``structured'' guarantees on the binary compression scheme e.g. 1) the reconstructed function corresponds to a function in the class or a majority vote of functions in the class or 2) the sample compression scheme is \emph{stable}, they are able to show that this addition $O(\log(|\mathcal{Y}|)$ factor can be removed providing an even sharper bounds for the original function class. 

The authors also makes other smaller contributions such as: 1) introducing an ``infinitized'' notion of sample compression for infinite sequences and showing a simple constructions based on Littlestone's Standard Optimal Algorithm and 2) constructing an adversarially robust learnable class that has no finite compression scheme by porting a construction for a similar impossibility in the partical concept class setting to theirs.

I think this a good paper that is well written and easy to follow. The authors do a great job in explaining how their results compare to existing results in the literature. I consider that the most general reduction technique that the authors propose with no assumption on the relevant binary compression schemes to be folklore. However, the inclusion of this result definitely aids in understanding (and appreciating) how the more delicate reduction works when there is a structural assumption on the compression scheme (stable compression scheme or majority of functions in the base class). I believe the reductions with more structural assumptions are much more interesting/novel as they correspond to some of the most well known compression schemes as the authors mention. I also found the results around the infinitized compression schemes as well as the impossibility result for adversarial robust compression scheme to be very standard. 


Some minor comments on presentation: 
In the proof of theorem 7, in the last sentence of the first paragraph (right detalining the compression and reconstruction steps) the authors write Thus $c(x,y) =1$ where I believe they meant $g(x,y) =1$. Actually, I find the notation that uses g for the reconstructed function to be confusing as it clashes with the notation g_c for functions in the ``binarized'' class $\mathcal{C}_{\mathcal{Y}}$. 

In the first display of the proof of theorem 9 the authors define a set $\kappa_b'(\mathcal{S}_{\mathcal{Y}})$ but the definition is extrmeely hard to parse (and there is a typo with the last closing bracket being ""]"" rather than "")"" ).

The proofs of the reductions in section 5 do not match the style of the proofs in the other sections e.g. the compression and reconstruction steps are not spelled out like in the previous sections."
170,"Contributions: The paper designs an algorithm for learning (realizable) halfspaces up to 0-1 error epsilon, under the assumption that an eta fraction of samples are maliciously corrupted (i.e., the adversary is randomly assigned corruptible samples rather than being able to inspect the samples first). The paper is the first to give a polynomial-time algorithm for this problem in the regime eta = Omega(1). To do so, it uses several (fairly strong) distributional assumptions, including: a margin gamma a little larger than d^{-1/2}, and the distribution being a mixture of O(1) components all of which have small means and sub-isotropic covariances.

The paper essentially modifies an argument due to [Talwar, '20] which achieves all of the conditions above except requires eta = O(gamma). The observation is that all outliers can be in the same direction, inducing a gradient change of roughly eta * n, and the ERM algorithm is only tolerant of size-gamma * n perturbations. Instead, the paper uses a classic idea from robust statistics, observing that the gradient norm is boundable by the covariance operator norm. They therefore run a filtering technique by solving an SDP (an idea which has appeared before in e.g., the robust estimation literature, see ""High-Dimensional Robust Mean Estimation in Nearly-Linear Time"" for the first example to my knowledge, and ""Sever: A Robust Meta-Algorithm for Stochastic Optimization"" for the idea of filtering to control gradient sizes). Their final algorithm simply combines the SDP filtering step with an ERM step.

I think the paper is reasonable. The conceptual message is nice (there is a highly-structured setting where PAC learning under Omega(1) malicious noise is tractable), but the techniques are a bit on the straightforward side. Much of the analysis in the paper is either directly adapted from [Talwar, '20], or based on empirical concentration ideas standard in robust statistics. I think the paper would be more interesting if the paper was considering a setting beyond the restricted assumptions in [Talwar, '20] or gave a faster algorithm.

Towards that end, I would have liked to see more discussion about if the algorithm can be implemented efficiently. It seems like it should be doable -- hinge loss ERM (Step 4, Algo 1) is a well-studied problem in convex optimization, so it would be good to discuss what off-the-shelf runtimes would give (should achieve nd / poly(gamma)?) Regarding Algo 2, I think that this problem is a box-constrained packing SDP and hence should also be solvable using off-the-shelf tools from the literature in nearly-linear time. For example, see the reductions in the aforementioned ""High-Dimensional Robust Mean Estimation in Nearly-Linear Time"" paper, Section 4, or the specific box-constrained packing SDP solver in ""Robust Sub-Gaussian Principal Component Analysis and Width-Independent Schatten Packing"", Proposition 2.

A few smaller comments.
1. Is ""instances"" standard terminology for x in R^d? I've seen ""features"" more.
2. It'd be great to discuss a bit the relationship of malicious noise to other noise models for the unfamiliar reader. (Is it the same thing as Massart noise, except rather than only control the labels, the adversary also gets to control the instances of a random eta fraction?)
3. Assumption 2 seems rather strong... it extends to only a constant number of components with very small means, which feels almost like a single near-isotropic component with large margin. Could the author include a bit of discussion of what is technically more challenging about extending to this setting rather than the single-component setting?
4. The statement of Theorem 2 has some weird asymmetry. I'm not sure why the constants are specified everywhere except the random c.
5. ""as far as"" -> ""as long as"" in Section 3.1?
6. The authors refer to (4) as a ""linear program"" and a ""semi-infinite linear program"". Isn't it just a (packing) semidefinite program? 
7. I think by far the most important open question is to remove distributional assumptions from the current work, or show they're necessary for the results. The assumptions used in the work (margin, essentially isotropic) are quite strong and unlikely to hold in realistic instances. This isn't really mentioned at all in Section 5, which makes me a little concerned with the paper's messaging, given its main upside is conceptual (extending to an important parameter regime) rather than technical in my viewpoint."
171,"In this work, the authors present a preliminary study for estimating knee adduction moments from data acquired from accelerometers. The authors employ an LSTM-based network that includes an attention layer architecture. Over a cohort of 24 participants, 12 male and 12 female, whole-body motion was captured along with data from two IMU sensors. In addition to evaluating the prediction accuracy provided by the model, the authors analyze the attention weight and an XAI technique, LIME, to gain insights into the prediction made by the model. 

This work demonstrates a solid preliminary study into the feasibility of predicting knee adduction moments using IMU data. However, there are several points that could be addressed:

1. There is no report of the overall performance of the model. It seems like the reported example is for a single subject, but there is no report of the evaluation over the entire cohort
2. There is no comparison to other simpler models that would demonstrate the performance improvement. Although this is understandable for a preliminary feasibility study, the overall statistics would be important.
3. The authors report an 80/20 split, but was this split over the participants? Details such as this should be included for a better assessment of the results.
4. Although the authors admit the limitations of the analysis of explainability, how the explainability relates to the trustworthiness of the model is less clear. Moreover,  the approach does not seem particularly innovative, as it applies existing approaches, nor does it provide significant insights into the model and the broader field of biomechanics. 

Overall, the results are promising but would recommend another iteration of the manuscript before resubmission.

Additional minor comments:
- ASSWS is not explained in the paper
- The manuscript is over the 2-page limit for the non-traditional track"
172,"This paper analyzes the Metropolis-adjusted Preconditioned Langevin Algorithm (MAPLA) for the purposes of sampling from convex bodies with respect to some underlying metric. As the authors point out, the naive Preconditioned Langevin Algorithm (PLA) is biased even for vanishing stepsize, and the additional Metropolis adjustment corrects for this. As the authors also helpfully point out, MAPLA can be thought of as an interpolation between the vanilla DinkinWalk for sampling from convex bodies, and the Manifold Metropolis-adjusted Langevin algorithm (ManifoldMALA). They require several technical assumptions on the underlying (preconditioning) metric $\mathcal{G}$ to make their analysis work, though there are related works with similar if not identical assumptions. At the end of the day, they provide mixing-time guarantees for MAPLA which depend on the underlying dimension ""d"" and these technical assumptions on the metric. Some of these are recover prior work, namely when $\mathcal{G}$ is the Hessian of the potential of a potential in a certain class, and improve upon others (e.g., Riemannian Hamiltonian Monte Carlo) in the dimension.

The paper is clearly written (minor typos here and there), even for people like myself who are not the most familiar with this body of work. The background section is particularly clearly written. My only criticism would be that the comparison to Metropolis-adjusted Mirror Langevin Algorithm (MAMLA) was not thorough enough, as the main difference between these works is the generality of the preconditioner. In that sense, this paper supersedes MAMLA, though it is not clear to me (someone not in this field) if the proofs are morally the same, which diminishes the contributions of this work (I will also confess that I did not read the proofs in detail, so please take the comment with a grain of salt). Nevertheless, I enjoyed this paper, and I recommend accept.

Comments:
- I recommend changing the wording near the introduction of (PLA) in the background section. As written, the equation is just thrown in-between paragraphs and some structure is lost. Maybe add the equation just after the italicized ""*preconditioned Langevin algorithm*"" in the text"
173,"## Paper Summary
This paper provides a characterization of online learnability with bounded loss
functions via a natural notion of the Sequential Minimax dimension. It directly
generalizes other specific notions of learning dimensions including Littlestone
dimension, fat-shattering dimension, among others. It recovers existing results
up to logarithmic factors.

## Review
This is one of those cases when reviewing a paper does not feel like reviewing a
paper, where one can just enjoy learning from the authors. The authors succeed
on what they claim in the paper title: providing a clear, unifying picture of
the hardness of online learning in various related settings. There are certainly
novel ideas in this paper, but in some sense, the result of the paper is not
extremely surprising. In addition to the main results themselves, I read this
paper as marking the maturity of the area of online learnability, which this
paper brings together and rounds out here. This in itself is a worthy
contribution, and the authors do a wonderful job.

A very nice idea I encountered for the first time here is the idea of
$\epsilon_t$-realizability and the accompanying realizable-to-agnostic
conversion. If this idea came from elsewhere, it would be good to cite that more
obviously. If not, the paper's stated contribution of ""identifying the right
notion of realizability and providing a new realizable-to-agnostic conversion""
is even a little understated and could be emphasized more if the author wished
to do so. 

The final discussion on the finite character property was also interesting,
although I wasn't quite sure how to place it. Is the main importance for the
computability of the Sequential Minimax dimension? Or, is it the case that
online learning is ""easier"" when losses take place in a Helly space, for
example?"
174,"Advantages:
1. This paper proposes a novel method, which augments models using only the most predictive hypercube of synthetic data.
2. The motivation of the model is reasonable, and the application is important.

Disadvantages:
1. According to experimental results, the improvement from the model is relatively limited.
2. The author should introduce the concrete implementation process of the article in more detail."
175,"**Summary**
The paper investigates the utility of sign equivariant neural networks that combine determinants *non-linearly* for representing electronic wave functions. This approach is different from existing neural-network-based approximations to the wave function that rely on *linear* combinations of determinants (so-called Slater-Jastrow wave function).

Specifically, the paper contains two main contributions:
1. A theoretical result, showing that any sign equivariant non-linear combination of determinants can be equivalently represented using the classical ""linear"" Slater-Jastrow form.
2. An empirical evaluation of two classes of wave functions based on sign equivariant neural networks. The evaluation shows that the sign equivariant architectures do not provide a consistent advantage over the traditional methods based on the Slater-Jastrow form.

**Strengths/weaknesses**

The paper studies an important ML problem, provides a clear presentation of the results, and examines an interesting question overlooked by prior work. The findings of this paper may be important in shaping the direction of future work in this ML area.

My only criticism of the paper is that the conclusion in the abstract (""sign equivariant functions are unsuitable for representing electronic wave functions"") might be too strong given the evidence presented in the paper. 

The theoretical result shows that the non-linear component can be in theory ""absorbed"" into the permutation invariant Jastrow factor $J(r)$, while the $\det \Phi(r)$ terms are used to capture the fermionic antisymmetry to permutations. The experimental evaluation studies two specific sign equivariant NN architectures. Neither of these statements excludes the possibility that different architectures for $J(r)$ or the sign equivariant aggregation function $f(x, y)$ would lead to a different conclusion.  As a somewhat far-fetched example, the strong conclusion presented in the paper is akin to concluding that neural networks are unsuitable for learning functions of images by 1/ invoking the universal approximation property of MLPs and 2/ showing their poor results on some image-related ML tasks. I would recommend to add more nuance to the statement, taking the limitations of the experimental setup into account.

**Minor suggestions:**
- Typos in Definition 2, should instead be ""the group acts on $\mathcal{Y}$ as $\pi y \mapsto \operatorname{sign}(\pi) y$""
- The description of ""Linear-logarithmic domain"" is Section 3 feels out of place. The statement about samples drawn from $\phi^2$ refers to the experiments that come much later. Also, previous paragraphs use $x$ to denote the vector of determinants, but in the definition of $\operatorname{linglog}$, $x$ is a scalar
- Potential typo: $f^{(t)}$ instead of $f^{(T)}$ before equation 5.

Disclaimer: I have limited experience in ML for quantum chemistry, so please feel free to downweight my feedback in favor of reviewers who are experts in this domain."
176,"## Summary
---
This paper introduces a method that enhances reasoning abilities in large language models (LLMs) by integrating Monte Carlo Tree Search (MCTS) into process supervision. Unlike traditional approaches that focus on the final answer, this method scores intermediate reasoning steps based on their correctness, allowing for more nuanced training. Experiments on mathematical reasoning datasets (MATH and GSM8K) show significant improvements over baselines, with strong generalization to unseen datasets.


## Strengths
---
- Innovative use of MCTS to generate fine-grained supervision for intermediate reasoning steps, addressing limitations of outcome-based training.
- Consistent and substantial performance improvements are demonstrated on both in-domain and transfer tasks, supported by rigorous evaluation and clear comparisons to baseline methods like Zero-shot CoT and RFT, effectively validating the approach.

## Suggestions for Improvement
---
- Overstated Novelty: While the combination of MCTS and process supervision is novel, the paper could more explicitly acknowledge existing foundational work in these areas. Highlighting prior research on MCTS in reasoning tasks (e.g., its use in planning and decision-making algorithms) and process supervision methods would provide better context and strengthen the positioning of the contribution. Additionally, discussing how this work extends or diverges from existing approaches would clarify its unique value."
177,"Clarity: The authors clearly describe their goal of assessing two groups of reddit posters the posters in the Anxiety subreddit and that then post in the ADHD subreddit and the posters in the Anxiety that do not post in the ADHD subreddit. 
Originality: the task of determining a proxy of possible comorbid ADHD is novel. 
Significance: The link between the proxy of possible comorbid ADHD is not well discussed in the manuscript and therefore it is hard to say what impact this will have in the clinical world. However, it is an interesting approach to the use of foundation models within a ‘semi’-clinical setting. 
Quality: the study quality appears good as the method, data and performance of the model has been clearly stated. 

Major points
1.	You point towards it yourself in the data collection section of the paper where you note that the reddit posts are not a clinical diagnosis. I think you must discuss the implications on the significance of your work. 
2.	Why did you choose to remove data from posters that posted in the ADHD subreddit within 6 months after their post in the Anxiety subreddit?
3.	Clarify if you download posts from the ADHD subreddit. In the data preprocessing section, you state that you don’t use the posts and then that you use the posts from the ADHD subreddit. 
4.	You reference a figure 6 that is not present in the manuscript.
5.	You state that you visualize the phrases leading to “will post in ADHD” or “will not post in ADHD” for any given post but this is not presented anywhere. 

Minor points
1.	In section Data Preprocessing is it correctly understood that the posters who posted anywhere else than the Anxiety or/and ADHD subreddits were removed from the dataset? This should be clearer.
2.	You mention the base rate of the test set, but you provide no detail on the distribution of the training set or if you have done any weighted sampling or indeed how you sampled the test dataset.

Pros 
•	Well written and concise. 
•	Interesting subject sure to spark interest even for people who are not experts in psychiatric disorders.
•	Interesting application of existing models to proxy ADHD comorbidity 
•	In the appendix of the paper the limitations section does a good job of explaining the cons of the study in non-bias manner. 
Cons 
•	The ADHD comorbidity proxy is not well discussed. 
•	The authors state that they have visualizations multiple times where none are shown."
178,"The paper studies multiclass classification under bandit feedback in
the setting of universal learning, a recently proposed learning
framework that is in stark distinction with the more classical uniform
learning. The paper shows an interesting and perhaps surprising
result, that in the framework of universal learning, there is no
distinction between learnability under bandit feedback and under full
information. In particular, universal learning is possible whenever
the concept class does not admit a littlestone tree of infinite
depth. The surprising part is that this is different from what happens
under uniform learning: for uniform learning, learning is not possible
under bandit feedback if the ""effective label space"" is (countably)
infinite.

The techniques incorporate some newer ideas, like the recent works on
universal learning, with some older ideas like multiplicative weight
updates and EXP4 (with some clever modifications). I looked at the
proofs briefly and they seem correct.

Overall, I have no major issues with the paper and think it is clearly
above the bar for the conference. The topic and results are timely and
certainly of interest to the community, and the techniques seem
non-trivial. I only have some minor comments:

- Can get the motivation for defining the triplet sequences
  (X_t,Y_t,Y_t^\star) but what goes wrong at the technical level if
  one does not do this? Is this an issue if the adversary is oblivious
  or is this just an issue of adaptivity?

- For the realizable setting is it possible to derive a quantitative
  mistake bound for the bandit setting (say as a function of the
  mistake bound for the full information setting)? Maybe doing this in
  the special case of a finite label space and taking q_Y = 1/|Y|
  would be illuminating? It seems like it is possible in this case?"
179,"Authors present an approach to use language model to predict the diagnosis of the next visit of patients given as input a sequence of visits and their associated code-diagnose. They benchmark the new approach with existing approaches and standard datasets form medical settings. 

The paper is original in the use of language model technology for next visit prediction, however the reviewer and reader would appreciate more detail in how the system is built and how it learns, specific parameters. What type of model do authors use, what is the architecture, is it an encoder-decoder, only decoder architecture? How is the fine tuning performed?

 I would like to suggest that the clarity of the writing could be improved. There are some sections where the language appears a bit unclear or could benefit from a revision for smoother readability. I recommend a thorough review of the English language throughout the manuscript to enhance its overall clarity. This will undoubtedly contribute to a better understanding of the valuable research."
180,"This paper considers PAC-learning of one-hidden-layer ReLU networks. For dimension $d\in\mathbb{N}$, consider $d$-dimensional Gaussian inputs $X\sim N(0,I_d)$ and labels $Y$ generated by a one hidden layer ReLU network of width $k = \widetilde{\omega}(\sqrt{d})$ with polynomially small Gaussian label noise. Then, the main result of this paper states that any $\epsilon$-PAC learner for this model, for $\epsilon = 1/poly(d)$, gives a poly-time quantum algorithm approximating GapSVP to within ${\rm poly}(d)$ factors, a conjecturally hard task. Notably, $k$ need not be too large (e.g., exponential) for the hardness, the result remains valid even when $k$ grows polynomially in $d$. The paper extends the results of Song et al. (2021) who established a similar hardness result for adversarially chosen noise to the (more natural) case of random noise. (Work by Song et al. studied periodic cosine neurons, but their results transfer to one-hidden-layer ReLU networks via standard approximation results).

Technical arguments appear very similar to Song et al. (2021) with the following additional technical steps: (a) injection of Gaussian noise to labels (Lemma 7), and (b) construction of an appropriate periodic neuron and a poly-width network agreeing on any bounded interval $[-R,R]$ (Lemma 10). 

Strengths:

This is a nice result, yielding cryptographic hardness of PAC-learning one-hidden-layer ReLU networks with Gaussian label noise even when $k$ scales polynomially in $d$. Prior work showed hardness of learning two-hidden-layers and above, as well as the hardness of one-hidden-layers with adversarial noise, as opposed to random setting considered here. The paper is well-written and the relevant literature is adequately discussed.

Weaknesses: 

The technical novelty over Song et al. (2021) feels somewhat marginal. The extension from adversarial to random noise appears to follow from nice yet relatively straightforward arguments: (a) injecting random noise (Lemma 7) and (b) considering a certain periodic neuron (Lemma 10).  

Overall Recommendation:

I think the contribution of the paper is over the bar for ALT, so I recommend a weak accept. I’ll keep an open mind during rebuttals, and I encourage authors to elaborate especially on the technical novelty over Song et al. (2021).

Remarks/Questions:

— I think the hardness result against quantum algorithms should be highlighted further, particularly in the abstract. 

— Display in Page 2: Please clarify that $(x)_+$ refers to ${\rm ReLU}(x) = \max\{x,0\}$.

— The authors have a discussion regarding noise level on Page 3. Can we say something about the width, i.e., what happens when $k=o(\sqrt{d})$? For instance, is it plausible to expect that there is a regime $d^c \ll k \ll \sqrt{d}$  in which one can show hardness, say by using an assumption other than CLWE? 

Minor Typos:

— The notation for sphere $\mathcal{S}^{d-1}$ uses caligraphic S before display (2.3) and capital S at several other places (e.g. Lemma 7).

— Missing parenthesis on Page 7, $(x,\phi(\gamma\langle w,x\rangle+\xi)$."
181,"The author proposed an efficient model for segmentations in medical images, by knowledge distillation from MedSAM image encoder to Rep-ViT.

The manuscript is basically complete. Suggestions or deficiencies:
1. Efficiency description is ambiguous in the abstract.
2. The width between paragraphs is too wide on page 2.
3. Tables in **Experiments** section have no vertical line.
4. Too much space between Table 2 and Table 3."
182,"**Summary**:
This paper studied low-rank symmetric tensor recovery from rank-1 Gaussian measurements. This problem is a tensor generalization of the low-rank matrix recovery problem from rank-1 measurements. This problem also connects to learning a two-layer network with polynomial activation functions, where a teacher network can be written as a low-rank symmetric tensor.

The author considered an NP-hard problem, i.e., rank minimization, and showed that when the number of measurements is $\Omega(rd)$, the rank-minimization program can recover the ground truth with probability 1. This is near-optimal since the number of parameters in a rank-$r$ low-rank tensor is of order $rd$.

The authors also show that empirical risk minimization can recover the ground truth with $\binom{d+\ell-1}{\ell}$ many samples and below $\binom{d+\ell-1}{\ell}$ many samples there are solutions of the ERM that generalize poorly. This is not very surprising since this is the interpolation threshold.

A sample complexity lower bound is also proved for a different statistical model where the ground truth tensor is generated from a discrete space. It is shown that any estimator can not recover the ground truth uniquely with $O(dr^{1-\gamma})$ many samples with $\gamma>0$.

**Pros**:
The paper is well written and the proof combines tools from high dimensional probability, orthogonal polynomials, and information theory. This topic could be interesting in both tensor methods and the neural network community. As discussed in Section 2.4, the sample complexity result has no norm dependence on the weight matrices and it captures the better sample complexity when the width of the network is relatively small (under-parameterized).

The proof combines recent advances in the covering number estimates of low CP-rank tensor space, Carbery-Wright inequality, and Hermite polynomials. This could be an interesting technique for other problems.

**Cons**:
My major concern is that the main result is only a statistical complexity since rank minimization is not computationally feasible. Even the convex relaxation of rank-minimization (nuclear norm minimization) for tensor is NP-hard. So I would interpret this work as Information-theoretical results on low-rank tensor recovery.  It would be more interesting to obtain a sample complexity bound for a computationally efficient algorithm.

**Minor comments**:

1. Page 3 below equation (7): the discussion that ""nuclear-norm minimization"" is computationally efficient is not correct. It is shown in (Hillar and Lim 13) that nuclear norm minimization is NP-hard as it is the dual norm of the spectral norm. 

2. Page 4, Theorem 5. I wonder if there is an error in the threshold case $N=N^*(d,\ell)$ as both case (a) and (b) cover the interpolation threshold and there is an obvious contradiction."
183,"This paper adopts the group lasso penalty for multi-task learning, resulting in cross-task structural sparsity.
The experimental results show that removing redundant parameters further improves MTL performance.
Moreover, this paper shows that appreciating sparsity not only reduces computation costs for each task but also benefits better task performance.

The proposed method is not novel but is effective in learning structured sparsity for multi-task learning, which is potentially utilized in various scenarios.
Considering this paper focuses on the sparsification of hard-shared parameters, the experiments are reasonable, but proper comparison is also helpful.
Like the MTL performance under other structure sparsity methods like SWP[1] and SSL[2].

[1] Pruning Filter in Filter

[2] Learning structured sparsity in deep neural networks"
184,"- The authors demonstrated how a two-step hierarchical diffusion model can generate anatomy-aware, high-fidelity 3D CT images.
- Generating anatomical structures along with the volume, conditioned on the radiology report, appears to be a promising approach.
- The paper is well-written and clearly explains the method.
- Figure 1 could be improved by providing explanations for each component.
- Extending the results to include higher-quality volume generation would help illustrate the trade-offs."
185,"This paper makes a empirically-based contribution to the understanding of how CNNs subitize given two different loss functions, a classical CE and a neuro-symbolic HRR. While the paper is well written and the authors made many relevant experiments, the results are a bit weak. However, given the novelty of their approach, their contribution is to be considered as valuable.
Some minor remarks:
* While mentioned in the text, it should be clear in the caption of the tables that the authors use accuracy as their main metric. In the current version of the paper, the authors merely speak of ""results"".
* It is clearly stated that VSA are regarded as a neuro-symbolic approach and the presentation in this regard is clear. However, it is unclear to me how saliency maps can be used as an appropriate demonstration of neuro-symbolic capabilties. The authors should make clear how saliency maps can reveal symbolic aspects of the inference process in the CNN."
186,"This is a well written paper with clear explanations on the problem context, related work and background. Learning interpretable constraints from demonstrations to improve safety of RL agents is valuable but under-explored. This paper proposes a one-class decision trees to learn safe convex set in feature space. The implementation steps are clearly presented. The test case in autonomous driving simulations demonstrate applicability to complex, safety-critical real world tasks. In additional, the transfer learning experiments provide some evidence on the generalizability of learned constraints beyond isolated tasks.

There are a few limitations in the paper:
1) Constraint learning quality heavily depends on expert demonstration quality and size of the data. This could result in overfitting and bias in the learned constraints for the current few thousand examples used.
2) More analysis is needed for generalization across tasks in terms of transferability of learned constraints, as even slightly different tasks can degrade performance, implying there might be exploiting spurious patterns.
3) there is no human evaluation or visual assessment on whether learned constraints actually capture meaningful safe behavior.
4) only simple simulation environments and one driving dataset are used for evaluation.
5) No comparison of constraint quality with other constraint /rule inference methods limits the implied benefits of proposed one-class decision tree approach"
187,"The work ""Computational Pathology at Health System Scale – Self-Supervised Foundation Models from Billions of Images"" is an interesting report from the conducted experiments. The reviewer would like to claim that the problem stated by the Authors is of high interest to broader public (benchmark of recent Foundation Models on pathology dataset). The important thing is that the Authors trained their models with an extensive dataset (as it was claimed in the paper there was around of 3 billion images from around 423000 digital microscopy slides). The goal of the work is clearly given. However, the reviewer would like to raise two suggestions that need to be addressed before publication:
1. The first one is related to the description of the samples. It was claimed that all of them belong to 76794 patients - but no sufficient details about the patients are given. I mean information about the sex, race, age... etc. All these details can allow reader to better understand the approach (of course, I am totally aware that they could not have any alignment with the dataset itself but may have - as some of the illnesses are more probable in later stages of life).
2. I do not understand why huge amount of information is given in the form of supplementary material. I assume that all these subchapters need to be provided directly into the paper - not in the form of supplementaty material. It will be then easier to understand the whole idea as well as to compare the outcomes with the latest results.

The reviewer would like to claim that after all these corrections, the work is ready for publication."
188,"The main claim is that using a finetuned version of LiteMedSAM, that was created by sampling different modalities with specific probabilities and using LoRA, is able to outperform the baseline when running inference with multiple boxes in parallel. The authors used low-rank adaption for the TinyViT image encoder of LiteMedSAM.
They decided to sample CT datapoints with 40% probability, MR with 10% probability and all other 9 modalities with 50% probability. The authors achieved an average dice similarity coefficient of 87.05% and normalized surface distance of 87.77% beating the baseline on the validation set.

The paper seems to be marginally below the acceptance threshold, it proposes a fine tuning pipeline based on LoRA and sampling modalities with different probabilities, it presents an inference system based on inferencing with multiple boxes in parallel, but it misses a lot of information in the results section, such as the efficiency results on the validation set, which makes it hard to judge how well the proposed approach performs.

## Clarity
It was not clear to me what you meant with multi-processing the boxes and using argmax, I had to read the code, I would recommend clarifying what you did by mentioning that you did batched inference for the prompt encoder and mask decoder for 2D datapoints with multiple boxes.
You mention that 50% of the time you will randomly choose from the other 9 modalities. It would be good to clarify whether the 9 other modalities will be sampled with equal probability or proportional to their number of datapoints.
LoRA makes finetuning more ressource efficient, but in Section 2.2 you mention that LoRA makes deployment more ressource efficient?
It is also not clear to me why concurrently performing inference with multiple boxes is supposed to enhance the segmentation accuracy, as mentioned in the Introduction and abstract. Maybe you meant segmentation efficiency?
Table 3 contains two ablation studies, but there is no information what they were about.

## Reproducibility
The paper contains some information about the environment used, but a lot seems to be copied over from the template, including the yet to be released Python 3.20,
it would be good to add more accurate information about the environment used.
The GitHub repository also lacks a README with instructions on how to reproduce the results as well as environment information like a requirements.txt.

## Typo
In table 2 the number of parameters seem to be off by an order of magnitude. And the number of flops and CO2eq are copied from the template, I would recommend leaving these values out if you did not compute them."
189,"The paper attempts to present an in-depth analysis of the current state and future directions of clinical Large Language Models (LLMs). The authors compare recent clinical LLMs, focusing on medical knowledge injection and domain-specific tuning or pretraining approaches. They provide insights into model performance across various benchmarks, including MedQA and PubMedQA, and discuss the implications of continuous pretraining versus supervised fine-tuning, model size, compute power, and the quality of training data.

**Pros**
1) **Easy to follow**: The paper is easy to follow but has some issues (pointed out in Cons section)
2) **Authors Cover various aspects of Clinical LLMs**: 

    2a. The training data and approach are well summarised in Table 1.

    2b. The Results section provides insightful comments on continuous pretraining versus supervised fine-tuning, model size versus compute power, and the quality of training data.

**Cons**

1) **Evaluation in Table 1**: It's unclear whether the authors conduct the evaluations themselves or if the numbers are reported from respective papers. The table caption states, ""For each model, only listed best performance,"" but it's not clear what ""best performance"" refers to. Additionally, the metrics used for performance (e.g., accuracy) should be clearly mentioned in the table.

2) **Some excerpts need to be refined:** 

     2a. The conclusion appears to be very brief.

     2b. The discussion on downstream use cases is not comprehensive. Important industrial downstream tasks, such as early prevention of diseases and personalized medicine based on patient history, are not adequately covered.

4) **Novelty:** The paper seems to add little value to existing surveys [1, 2, 3]. Many of the discussed points, such as fine-tuning versus pre-training, training data, results on MedQA and PubMedQA, along with applications of LLMs in downstream use cases, are already extensively covered in the cited survey papers.

5) **Lack of Related Works**: Significant related works [2, 3] are not mentioned, which is a notable omission.

[1] Zhou, H.; Gu, B.; Zou, X.; Li, Y.; Chen, S. S.; Zhou, P.; Liu, J.; Hua, Y.; Mao, C.; Wu, X.; et al. 2023b. A survey of large language models in medicine: Progress, application, and challenge. arXiv preprint arXiv:2311.05112

[2] He, Kai, et al. ""A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics."" arXiv preprint arXiv:2310.05694 (2023).

[3] Singhal, Karan, et al. ""Large language models encode clinical knowledge."" Nature 620.7972 (2023): 172-180.

While the authors mention results and discussions insightfully, they lack novelty, and most points exist already in the survey papers pointed out above. Additionally, the missing evaluation details in Table 1 are a concern."
190,"Summary: 
The authors propose to utilize a continuous formulation of generative pre-trained Transformers for PDE forecasting. The continuous formulation is achieved by predicting the parameterizing a Gaussian Mixture Model. Additionally, the computational complexity is reduced by operating in the latent space of a pretrained VAE.
Experimental evaluation is performed on the 1d Burgers and Advection families of PDEs and compares favorably to previously published baseline methods.

Pros:
- Experimental results support consistent improvements over the baselines.
- The framework appears to be of general interest and utility beyond what is shown in this work.
- The text is easy to follow and clearly written.
- The approach is well-motivated from the related work and context of the field.

Cons:
- Technical novelty appears to be low, the authors cite several studies that essentially showed the utility of all components used here.
- Experimental evaluation is performed only on rather simple case studies, precluding any conclusions about the applicability of the framework to more relevant settings.
- The model is only compared against (commonly used) baselines, but not against numerical solvers or more advanced DL methods.
- Performance is only evaluated in terms of L2 loss.

Minor:
- The term ""foundation model"" is used inflationary here, no scaling properties of the model are shown, nor zero-shot performance on other families of PDE.
- A few language hickups (e.g. 'In a tentative to exploit foundation models')
- Out of curiosity: where does the model name come from?"
191,"### Strength:
* The paper clearly proposes a significant concern: the existing fine-grained structured pruning algorithm struggles to accelerate backpropagation when used with specific scenarios, such as CNN with GEMM. Section 3 delves deeply into this issue and presents a compelling argument.
* The method proposed focuses on regrouping using a kernel-wise mask and integrates pattern finding. This seems to be a sound approach to tackling the identified issues.

### Weakness:
* The presented results are incomplete and lack organization. In Table 1, for instance, the speedup values (which should be indicated in brackets as said in the caption) are missing. To grasp the benefits of the proposed method, readers have to cross-reference both Figure 5 and Figure 6, as these figures jointly demonstrate that the method offers improved speed without compromising performance. Notably, Figure 6 omits the ResNet-56 configuration, making it inconsistent with Figure 5.

* The paper does not provide sufficient details about its evaluation. The procedure by which training acceleration was gauged remains unclear, and the experimental setup is not adequately defined."
192,"I am terribly sorry but I initially uploaded the wrong review

Summary

This paper considers the stochastic unconstrained submodular maximisation with bandit feedback problem. This is a sequential setting where in each round a learner chooses an action, which is a set, suffers a loss, which is assumed to be submodular, and subsequently sees the value of the loss at the learner’s action. The goal is to control the 1/2-pseudo regret, which is the difference between half the reward of the best action times the number of rounds and the cumulative rewards of the learner. 

The authors provide an algorithm which has logarithmic problem-dependent regret bounds as well as problem-free regret bounds. The problem-dependent regret bounds come with a new notion of hardness for this problem setting.

The algorithm is an explore then commit algorithm. The main technical realisation is that one can use the fact that one can identify the (approximate) best action relatively quickly, after which in the commit phase the authors show that one can guarantee that with high probability, the algorithm no longer suffers any 1/2-regret relative to the best action. Intuitively, the reason for this is that the error in identifying the best action can be compensated for by negative 1/2-reward that is left after bounding the 1-regret.

Strengths and weaknesses

The main results of this paper is a first of its kind for this setting. Prior works only obtained problem-free regret bounds. I like the idea of compensating for the errors in identifying the best action with the negative 1/2-reward that is left after bounding the 1-regret. This could be useful in other settings as well. 

The paper is reasonably well written. 

The result appears to be correct, although I did not check the proofs in the appendix in depth. 

The main thing I found lacking in this paper is a brief discussion of a related, but significantly easier setting: online submodular minimisation with bandit feedback. There have been several works that control the regret through the Lovász extension combined with convex bandit algorithms to obtain T^{2/3} and T^{1/2} regret bounds with efficient algorithms, and a brief discussion of these results is warranted."
193,"### Summary

This paper proposes a new strategy to solve “medical event prediction”. They motivate the problem, describe its main challenges and propose a solution that addresses these challenges. The proposed method consists of two main steps - (i) medical concept memorization - to adapt the vocabulary of the LLM, and (ii) contrastive learning to capture inter and intra-visit relations in medical events. This method achieves SOTA performance on MIMIC-III and MIMIC-IV datasets.

### Strengths:
- Provides a clear motivation for the problem and outlines the main challenges in solving the problem.
- Novel and effective LLM pre-training tasks are proposed to directly address the above challenges
- A diverse set of baselines are considered is evaluating the proposed method

### Weaknesses:
- No ablation study in experiments to understand which of the pretraining tasks leads to performance gains.
- Does not consider any Clinical LMs as a base model in experimentation. Clinical LMs like BioGPT, MedPaLM may already understand medical vocabulary. This may remove the need for the “memorize” phase in the proposed method.
- Poor readability in certain sections. Eg: explanation of proposed method can be improved by correcting grammatical errors and adding equations/figures.    

### Other Feedback:
- Other baselines also evaluate on MIMIC-III and MIMIC-IV. Consider releasing your train/test split and characteristics of the dataset in your next iteration of this work. This could help standardize research in the “medical event prediction” field.
- Since your method is built on top of a generative LM, it could be inherently better than other methods at quickly (with limited data) understanding new medical events. It would be interesting to see experiments on predicting medical events in a few-shot/zero-shot setting.
- The authors noted that some other methods modeled external information as knowledge graphs. There are some advantages to these methods (eg: updating information) and those should be further explored in future work. For example, authors could add pre-training tasks using the knowledge graphs, similar to the way the ontology of medical codes is currently used."
194,"The abstract introduces two clinical LLMs, GatorTron and GatorTronGPT, and their applications in a clinical context. To this end, the authors review the training dataset of the two LLMs and summarize their performances in various evaluation tasks within a medical context. The abstract highlights the necessity of evaluating LLMs specialized for clinical purposes. The detailed description of the applications of the two LLMs provided a comprehensive picture for audience. Some concerns arose when I was reading the abstract:
1. The abstract tries to provide resources to facilitate the use of GatorTron, but I could hardly find lines addressing this claim throughout the abstract.
2. GatorTron and GatorTronGPT were distinguished from general-purpose LLMs in that they were trained with medical corpora. Hence, I’m curious about the performance contrast of a general-purpose model like ChatGPT (as a benchmark), and a specialized medical LLM like GatorTron. A direct comparison such as accuracy scores in medical NLP tasks (if it is possible to quantify their performance) between the two kinds of model will do. 
3. The “Conclusion and Discussion” section gives a nice summary about the applications of the two LLMs in clinical contexts. From my personal conjecture, readers may want to know more about the state-of-art news of GatorTron and GatorTronGPT, such as the technical challenges we are dealing with. Put such questions in a bigger picture, audience may be interested in where we are going to, and what we will be able to do with GatorTron and GatorTronGPT in the future. I believe these topics fit well in the discussion session and can contribute to the discussion in the symposium. 
In sum, the abstract provides a clear review of GatorTron and GatorTronGPT, a significant work in clinical NLP. As a review of existing studies, this abstract is inevitably short of originality. An additional discussion on the state-of-art and limitations of the present models will make the abstract more beneficial to the conference. Recommended."
195,"**Summary**
The paper proposes a large language model-based foundation model for medical code prediction tasks. The proposed model is based on LLaMA-2-7B, with further fine-tuning on (1) the medical code and text definition pretext task; (2) the medical code prediction task. The model is tested on MIMIC-III and MIMIC-IV, with a comparison to RNN/CNN-based models and graph-based models.

**Strengths**
- Clear motivation for the ""concept memorization"" training and ""hierarchical contrastive learning"".
- I appreciate that the authors include a task formulation section, for readers to better understand the task and data format.
  - This part can be further improved if authors can clarify whether ``diagnosis prediction`` is a multi-label classification
- The technical part is easy to follow

**Weaknesses**
- The input sequence perturbation makes sense to me. However, authors do not explicitly mention whether the within-visit order of target visit matters or not. 
  - During training, will the sequence perturbation also apply to the target visit?
  - During evaluation, how the accuracy/precision/recall are calculated? (If target code order is ``996.74, 428.0, 414.1``, and model outputs ``414.1, 428.0, 996.74``)
- Authors mention that transformer-based LMs are widely used in the same task (Paragraph#3 in the Introduction Section). Is there a reason transformers are not included as compared baselines?

**Misc.**
- Missing reference in Paragraph#1 in the Introduction Section. 
  > 13,000 disease candidates in ICD-9 ``(?)``

Overall, the quality of this paper is great. I would be very interested in seeing authors' responses regarding my concerns."
196,"**Strengths:**
- The authors effectively demonstrate how various prompting techniques can lead to superior performance compared to models finetuned/pretrained on domain-specific data. This experimentation showcases the practical utility of these techniques for open-source models, benefiting the community.
- The paper provides clear and comprehensive background details, making it easy to follow.

**Weaknesses:**
- It is essential for the authors to include an abstract and adhere to the formatting guidelines specified in the AAAI-24 Author Kit.
- The authors have failed to cite papers referenced in Figure 1.
- Figure 2 suffers from clarity issues, possibly due to colour choices. Reproducing results from open-sourced models like Meditron would ensure consistency in the computing environment. The same applies to models such as Med42, Clinical Camel, and PMC-LLaMA.
- The evaluation of the authors’ claims is limited. Experimentation across different model architectures and sizes would provide stronger support for their assertions.
- The lack of code release for their OpenMedLM prompting platform needs to be addressed."
197,"The paper proposes a novel method to solve mathematical problems by classifying them into categories and strategies.

Key issues:

- Hallucination was mentioned earlier in the paper, but no clear metrics were used to show if the approach reduces hallucination.
- It's not clear what category-wise instructions were given to the model to solve problems.
- The categorization of problems was weak; a more robust approach could have been to use a small transformer-based model for category classification.
- A transformer model trained to determine which strategy is better for a particular problem could have been more effective instead of using a pre-defined distribution.
- Could have provided details of the prompt structure used to convey problems,categories and instructions to the model.

Minor issues:

- Tables could have been more readable."
198,"This paper evaluates the usage of in-domain pretraining for medical segmentation tasks as opposed to the use of a generalist foundation model for segmentation (SAM), and proposes a new PEFT technique: LoRaMedNet. The authors show that the in-domain pretrained model, MedSAM, does not consistently outperform SAM when fine-tuning on medical segmentation data, which brings the usefulness of MedSAM into question. Additionally, their proposed LoRaMedNet outperforms standard PEFT techniques for fine-tuning on medical data. The technique combines standard LoRA with an additional convolutional head. The set of comparisons also seem solid as a workshop contribution, as they compare to a variety of existing fine-tuning baselines on their target dataset -- the Automated Cardiac Diagnosis Challenge (ACDC). This paper seems quite relevant for the workshop and is well-written."
199,"### Summary
This paper proposes an approach that combines Natural Semantic Metalanguage (NSM) and Bayesian inference to perform structured linguistic reasoning. Input sequences are processed in different steps. First, they are simplified into a predefined set of 65 semantic primes and successively further recombined to represent semantic relationships, as defined in the NSM framework. Then, these basic words and combinations are projected into a vector space, which encodes semantic relations between these vectors. Finally, Bayesian inference is used to generate potential hypotheses about the relationships between concepts and “the nature of the world”. The authors claim that this approach is superior to Transformer-based and neuro-symbolic approaches, showing better contextual understanding, interpretability, and scalability.

### Review
It is impossible to understand from the submitted manuscript how the model really works, as the authors only sketch a vague overview of the different components and do not provide any detail on their practical implementation and integration. No experimental results or empirical evidence are included to support the authors claims. Furthermore, the submission is not anonymous."
200,"OpenMedLM proposes a platform employing prompt engineering techniques to optimize the performance of open-source large language models (LLMs) on medical question-answering benchmarks, achieving state-of-the-art (SOTA) results without fine-tuning. Using the Yi 34B model, OpenMedLM surpasses previous SOTA results on MedQA, MedMCQA, PubMedQA, and MMLU medical subsets through few-shot prompting, chain-of-thought (CoT) prompting, and self-consistency strategies. The study emphasizes the potential of prompt engineering in enhancing the capabilities of generalist LLMs for specialized tasks like medical question answering.

**Strengths**
- Innovative Approach: The study introduces a novel use of prompt engineering as a viable alternative to fine-tuning, offering a cost-effective method for enhancing LLM performance in specialized domains.
Comprehensive Evaluation: It evaluates the model across multiple benchmarks, thoroughly assessing its capabilities in medical question-answering tasks.
- Clear Methodology: The methodology, including using few-shot prompting, CoT prompting, and self-consistency, is well-articulated, offering clarity on the process and potential for replication.

**Weaknesses**
- Generalization Concerns: The study's focus on a single LLM (Yi 34B) raises questions about the generalizability of the findings to other models.
- Lack of Comparative Analysis: While it compares OpenMedLM's performance with that of the Meditron model, a broader comparison with more models, especially those using different fine-tuning approaches, could have provided a more comprehensive view of its relative performance."
201,"Thank you for this interesting paper which proposes a method for evaluating conversational reasoning, history taking, and diagnostic accuracy for dermatological conditions, using AI to simulate patients in the conversation.

I find it curious that they chose such a vision-centric specialty to evaluate a text-based model, as this is simply not realistic - no doctor would diagnose a skin condition without looking at it! Nevertheless the authors should be commended for going beyond the ‘simple’ MQA approaches of other works as they describe in the background and I think their agent based solution is promising, despite the performance demonstrated here.

Whilst the focus of this paper is on the evaluation framework the following key details are missing:
The test set vignettes are constructed from an unspecified mixture of dermnet and bespoke cases. The proportion of these should be reported, and performance stratified by case origin, as the dermnet cases will have formed part of the training data for GPT3.5/4
MCQs are used for evaluation but there is no detail on how these were constructed. This is important to confirm whether they were part of model training and to gauge how realistic the questions are - e.g. were the other answers blatantly wrong or plausible differentials. It’s for this latter reason that single best answer (SBA) questions are more commonly used in medical exams nowadays. It’s also unusual for MCQs to have only 4 options, which makes me wonder how they were generated!

I appreciate the word limit but to me the most interesting element of this work is the patient conversation agent and there is little detail on how they did this. For example 10 simulations are mentioned, presumably to capture variation in how patients may present histories for the same disease, but no explanation is given of how this variation is modelled - different prompts, temperature settings etc?

The authors include an appropriate benchmark (the whole ‘vignette’) against both multi-turn conversation, single conversation (unclear why this is useful?) and a summarised version of the vignette. Interestingly the the highest performance is observed when prompting the model with the complete case vignette. Whilst the authors suggest this indicates limitations in medical history gathering skills, however without knowing the case mix I think it is more likely that the model is simply remembering the dermnet vignettes from it’s training. I would imagine that then the summarisation, single and multi-turn conversations are then adding progressively increasing noise/corruption of the original vignette and this is what is hurting performance.

The authors include qualitative ‘expert’ evaluation in their methods but do not show any of the results of these, including whether the grader-AI agent reliably evaluated equivalent diagnoses (a central requirement for their evaluation framework).

Despite these critiques and the results shown, I believe this study is an important demonstrator of an agent-based conversational evaluation framework and is worthy of acceptance."
202,"Summary Of The Paper

This paper studies domain adaption by performing augmentations. The authors introduce a method, domain-invariant feature mixup, which is an enhancement of Mixup. This paper also introduces a margin loss to better discriminate among each class. 


Main Review

- Strength
1) The theoretical validation are shown to proof its effectiveness. And the analysis on two aspects: 1) distribution coverage and 2) inter-class distance, is informative.

2) The proposed method is extensively compared against other methods on several dataset including image classification and time series, and is showing the effectiveness on moment retrieval and highlight detection tasks. And it is consistently better than other methods. 

- Weakness
1) With extensive experiments and analysis, the paper has demonstrated its proposed strategy on several dataset, however, the intuition of the proposed method is not thoroughly discussed, and the novelty of this work is somewhat not very strong because the paper is adapting existing strategies (feature mixup, margin loss) together into the learning scheme.

2) In order to demonstrate the effectiveness of the proposed method, larger scale image classification dataset based on natural images, such as ImagNet must be explored, at least the author should try ImageNet-100 (https://github.com/danielchyeh/ImageNet-100-Pytorch) data, which is a subset of ImageNet. I think the proposed method may not be practical without such larger scale dataset.

Summary Of The Review

Overall, without discussing the novelty of the proposed method in details, I am not sure if the novelty that authors mention in the paper is reliable. Also, I think we need to see valid elaboration and the intuition of the proposed method, and scale up to larger dataset on image classification task. Combined with the weaknesses I mentioned above, I vote for 5. I would like to see authors response to consider raising the rating."
203,"Pros:
- The results show that utilizing hybrid ViT is an efficient approach to optimize both accuracy and speed.
- The authors put an effort into the ablation study by training the model from scratch to show that two-stage training (distillation, then fine-tuning) is critical for good performance.

Cons:
- No code for reproducibility

The manuscript is basically complete. Suggestions or deficiencies:
- ""Abbreviated paper title"" should be replaced with actual title
- Please publish code"
204,"In reviewing this manuscript, it is clear the authors can intellectually articulate the focus of their study. They were able to explain the rationale of their research as well as their results. 

My constructive feedback would be as follows:

1. The abstract should be more concrete in explaining the “encouraging results.” At present, there is no concrete description of any results whatsoever in the abstract. 

2. In defining the attention function, the authors should define all terms in the model; at present, they do not describe the \\(d_k\\)
scaling term and the role it plays in the attention function. 

3.  In the section about pretraining, the authors state, ""Specifically, we detect the word “Figure” in sentences and remove the corresponding sentences."" Without having looked at the curated pre-training dataset, it would be imperative to know if all sentences analyzing images had the word ""Figure"" or words such as ""Image"" or other synonyms might have been used.

4. I would encourage the authors to explicitly define the ""cls"" subscript used in their arrays.

5. When describing the space of retrieved sets, I believe the authors may have a typesetting issue; namely, the authors state the set as \\(R^{1xh}\\), using an italic \\(x\\) as opposed to \\(\\times\\), indicating the array size of R being 1 by h. Hence, I believe the dimensionality of R should be represented as \\(R^{1 \\times h}\\).

6. Since the authors are using LaTex markups, I encourage them to express the learning rate as \\(10^{-4}\\) as opposed to 1e-4

7. The authors should define lora_alph and lora_r in the context of LoRA and display them with the appropriate LaTex markup (if applicable).

Overall, the authors have an extremely strong paper."
205,"The paper studies the general problem of finding computationally efficient PAC learners (for VC classes), who are at the same time optimal in the sense of their sample complexity. Existing approaches to achieving optimality require multiple queries to an ERM oracle (linear in the sample size m). This is problematic, since for many classes ERM is computationally hard. Therefore, the authors are guided by two goals here: 1) to find an optimal PAC learner which requires sublinear (in m) number of queries to an ERM oracle b) to guarantee that this learner is computationally tractable, ideally, requiring linear number of operations (in m). They are able to achieve the first goal, by obtaining an optimal PAC learner which requires a constant number of queries (with the constant depending linearly on VC dimension) and get very close to satisfying the second goal, by requiring O(mlog^2(m)) number of operations, under some assumptions, e.g. that evaluating the function given by ERM is constant time operation.

Personally, I like the presented results a lot. They improve on existing work, and from what I have been able to understand provide a clever analysis of existing techniques extended by some nontrivial mathematical ideas. The paper is clearly relevant to ALT community.

On a more negative side, while the arguments look sound, I haven't managed to verify the proofs sufficiently. This is to some extent due to my lack of competence in this specific topic, but there are other reasons. The paper has more than 40 pages. The authors made an attempt to fit into the first 12 pages some sort of proof sketches, which was a good idea, but I am not sure that the execution is equally good. They start by explaining the existing techniques, in particular that of Hanneke (2016b) and Larsen and Ritzert (2022) and Larsen (2023), to build theoretical foundation for the new technique. Again, this is a great idea from the didactic point of view, but I feel like it could be executed better. The issue seems to be that these sketches of the proofs are not exactly high-level intuitive explanations, they are filled with too much details and look kind of as if the authors just compressed the full proofs and equations into plain text. In the effect, what we get is somehow the harder to digest version of the full proofs. This is made even worse by the fact that some notations only appear later and some notions are left undefined.

If this was a journal paper, I would simply take more time to digest it before reaching any conclusion. However, this is not the case here. Since the results, if correct, are very neat, I would suggest accepting the paper but with the understanding that I cannot vouch for its correctness. 

I also have a question for the rebuttal. I would like to better understand when the presented results could give us a significant practical advantage. From your presentation I understood that the training complexity of both Hanneke (2016b) and Larsen (2023) depend only on the cost of ERM, whereas the training complexity of your algorithm depends both on the cost of ERM and on the cost of evaluating the function given by the ERM. The latter part grows linearly with m. In your analysis, you work with the assumption that the latter cost is bounded by a constant, while ERM is not. Is this reasonable to assume in general or could it happen that the 3mU_I part in your training complexity will be much bigger than U_T(500d)? As a stupid example, does we still have an advantage if the class is given as finite table? 

Some minor comments:

Page 1, last paragraph: 'The following lemma,(...) ensures that any ERM algorithm has bounded sample complexity.' This phrasing is unfortunate. You defined sample complexity as a function with values in \mathbb{N}. Usually, a bounded function with natural values means a function which has a maximal value. 

Page 2, par 2: 'Furthermore the matching
lower bound also holds for the larger class of learners that always outputs a hypothesis in H, known as proper learners' This is hard to digest. Lower bound on what, sample complexity? And matching in what sense? It also adds to confusion that you first talk about upper-bounding sample complexity, then Lemma 10 is stated in a form of an upper bound for the generalization error. 

Page 2, definitions of U_{Train} and U_{Inference}. What U_{Train}(m):=U_T(M) means? both symbols were not defined earlier.

Page 2, last paragraph: Notions of training/inference complexities are used without definition.

Page 4, Algorithm 1: `\sqcup` is not defined until page 13. Hanneke (2016b) seems to use just a normal set-theoretic union.

Page 4, Algorithm 1, line 5 - the | seems to be missing in the last sequence of symbols.

Page 5, paragraph 4, 'to show existents' -> to show existence

Page 8, paragraph 3, 'in a similar fashion as in the other proof - however will also play a key' -> better use -- or ---

Page 8, paragraph 5, 'By a Chernoff bound' -> By the Chernoff bound"
206,"The paper proposes an algorithm for online fair clustering with recourse.

Consider a graph $G=(V, E)$ the cost of a clustering $\operatorname{cost}(\mathcal{C})$ is:
$$
\operatorname{cost}(\mathcal{C})=\left|(u, v) \in E: u \not_{\mathcal{C}} v\right|+\left|(u, v) \notin E: u \sim_{\mathcal{C}} v\right|
$$
where $u \sim_{\mathcal{C}} v$ if and only if u and v ar in the same cluster of $\mathcal{C}$. The fairness constraint is that for each group $i$, the faction of group $i$ in each cluster should not exceed $a_i$, with the exception of singleton clusters that are considered fair.

The paper builds on the work of Online and consistent correlation clustering, modifying their algorithm to incorporate the fairness constraint. The crux of the modification is to slpit the clusters into singletons whevener they do not respect the fairness constraint. This is proven to not increase to much the cost of the clustering nor the number of recourses.

Overall, the paper is nice and the algorithms and proofs intuitive. 

I have a few questions:
- how do the CR  and number of recourses scale with epsilon?
- why cost-free in the title? It seems that the CR  and the number of recourses scale up as epsilon decreases and the maximum value of epsilon depends on the fairness constraint. What did you mean by this?

The writing of the paper seems to have been rushed. The first half is well polished, the second more messy. Here is a non exhaustive list of possible improvements:
- page 6: give the intuition of MAKECONSISTENT before the routine.
- reintroduce AGREEMENT in the main text. It is short and will clarify things.
- Page 8, after lemma 3: the sentence is messy and is hard to parse.
- Page 7: below thm, the recouse is upper bounded by twice the latter quantity"
207,I like the idea of applying induced automata into language models fine-tuning. The experimental results especially controller construction and formulation of steps are good. The flow of the reading could have been better.
208,"The paper addresses a crucial healthcare challenge with interesting objectives. It leverages machine learning techniques to improve the identification and management of CKD, which is a significant contribution given the global burden of the disease. The methodology employed is robust, and the results presented indicate a high level of effectiveness in achieving the paper's goals.

However, despite the promising application and outcomes, this study primarily utilizes classical machine learning approaches without incorporating any pre-trained models, which diverges from the conference's emphasis on innovative model foundations and applications in clinical settings. This discrepancy could be seen as a deviation from the core subject matter expected at the conference.

Furthermore, while the paper effectively demonstrates the performance of ML techniques in reducing CKD underdiagnosis, it falls short in situating its findings within the broader context of current state-of-the-art methods. A comparative analysis, not limited to data-driven approaches, in terms of precision and cost-effectiveness, would have greatly enriched the paper. Such a comparison is essential for understanding the true value and innovation of the proposed method over existing strategies.

In conclusion, while the application of machine learning techniques to tackle CKD underdiagnosis is indeed valuable, the paper's methodological approach lacks the novelty and direct relevance to the ""Clinical Foundational Model"" conference theme. The absence of a comparative analysis with state-of-the-art methods further limits the paper's contribution to the field. Therefore, despite its potential impact in healthcare, the paper may not meet the innovation threshold required for acceptance into the conference.

Pros:
- Tackling a challenging problem in healthcare
- Great accuracy

Cons:
- Not related to the main conference theme
- Lack comparison with SoTA methods
- Lack a cost reduction analysis"
209,"This paper provides an overview of GatorTron's use in existing literature. While the authors raise concerns about existing LLMs for medical applications, the paper does not delineate the methodological novelty and performance improvement of the presented model. A crucial ambiguity lies in the selection process of the cited studies, leaving readers uncertain whether it constitutes a systematic review of evidence. Overall, the paper offers some use cases of GatorTron within the literature."
210,"1. Summary and contributions: Briefly summarize the paper and its contributions
Outlines the development of an LLM called SoftTiger. This is a finetuned version of an open-source LLM named TigerBot. This is achieved using supervised finetuning tuning from a dataset of general text, a previously released clinical dataset and a novel clinical workflow dataset. The novel dataset is made up of instruction pairs for 3 tasks performed on the MIMIC-IV dataset with the outputs generated by GPT-4 and validated by 5 physicians. The results show that the finetuned models gained accuracy on automated evaluation benchmarks.

2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance to the community.
- It is an early example of finetuning large (70B) open-source LLMs across multiple GPUs.
- After carefully evaluating the trade-off between clinical complexity and helpfulness, 3 clinical data structuring tasks were chosen. This gives the work a clear potential for clinical impact.
- A very good section outlines the administrative burden on physicians.
- Making IPS or FHIR structure the output is optimal for potential future integration into current e-health systems

3. Weaknesses: Explain the limitations of this work along the same axes as above.
-MIMIC data user agreement prevents the sharing of the data or derivates with 3rd parties. Therefore, the SoftTiger and dataset should not be publicly released. They could be hosted on PhysioNet though.
- Similarly, MIMIC data should not be sent to 3rd party LLM providers as seems to have occurred in Table 5 unless via Azure or Amazon (see https://physionet.org/news/post/gpt-responsible-use). Please state clearly in text or “Ethical Considerations and Reproducibility Statement” if these services were used.
- Evaluation and training data only uses MIMIC-IV, which are discharge summary notes from the ICU department of a single health centre. This should be noted as a limitation.
- GPT-4 is used to produce the clinical training and evaluation set. This is then corrected by clinical review. No mention of the performance of GPT-4 on the task is made or the inter-annotator agreement. Furthermore, justification (most likely from a data governance perspective) is given on why GPT-4 cannot be used for this task directly if it can produce the labels for the task.

4. Correctness: Are the claims and method correct? Is the empirical methodology correct?
- In the introduction, it is claimed the 2 primary challenges for LLM clinical adaptation are finding a ‘helpful clinical task’ and input length constraint. I do not believe this to be true. Numerous clinical tasks could be performed by LLMs, e.g. diagnoses, discharge summary writing, reporting of adverse drug events etc... Barriers such as effective and safe evaluation, data privacy and governance, and integration into healthcare providers’ electronic systems would seem equal if not greater to this constraint. 
The second constraint of input length is notable but only for open-source models (closed-source models have context lengths >100k), a distinction that is not made. However, the trained model is only extended to 8k and claimed as a source of novelty. Current open-source models such as mistral have been trained with an 8k context window.
- Claimed that note length usually follows power law without proof or citation
- The approach is claimed to be “light-weight” but requires 64xA100s GPUs
- It is claimed that as TigerBot has a larger vocabulary size than Llama-2, it has a larger clinical vocabulary. However, as TigerBot is multilingual this claim only seems true if evaluating on multilingual data also. The claim that TigerBot has a greater English clinical vocabulary needs further explanation or proof.
- It is not obvious that the addition of the general-purpose or Asclepius datasets will improve performance on the clinical workflow tasks.
- “Dictionary of abbreviation expansion to standardize abbreviations” is known not to work due to the redundancy of terms. For example, “hr” could be expanded to hour or heart rate, depending on the context.

5. Clarity: Is the paper well written?
- “We then evaluate TigerBot and Llama-2 chat models using next-token prediction.” It is not clear to me how this evaluation works. Is this exact matching? Further explanation is required.
- Not clear how Llama-2 and TigerBot were extended from 4k to 8k inputs.
- It is claimed that “it is beneficial for worldwide adoption to build multilingual models”, which is true. But it is not clear if the fine-tuning dataset is also multilingual.
- Fig.1 shows some very helpful information, but it is not clear which task is related to which plot point due to the use of repeated colours. Furthermore, the Fibonacci scale is not explained.
- Not clear if, in normal practice, discharge summaries are the only source of information used to complete the 3 subtasks trained and evaluated in this work.
- Figure 2 is a direct screenshot from tensorboard or similar. Removal of the UI buttons, and adding full and axis titles would improve this figure. Not clear what the faint lines are in the figure
- Table 3 should be moved higher up the training data section and would be more instructive swap the size column for number of examples in each dataset.
- Figure 3’s final column is all 0% and so does not need to be included. Moreover, the information may be more helpfully presented as a table of min, median, max for input, output and total

6. Relation to prior work: Is it clearly discussed how this work differs from previous contributions?
- This is the first open-source LLM finetuning to output on FHIR IPS, FHIR Clinical Impression and FHIR Encounter from medical discharge summaries.

7. Reproducibility: Are there enough details to reproduce the major results of this work?
- The number, speciality, nationality, and seniority of clinicians surveyed to produce Fig 1 is not stated
- Would be useful to link or add in the appendices the exact FHIR structures of the 3 subtasks.
- The training framework section is limited. No training hyperparameters are given. The acronyms PP and DP are used without explanation.
- The settings, prompt, and model version used to generate the clinical workflow dataset using GPT-4 are not stated"
211,"## 1. ****General Description of the paper****

This paper falls in the category of the first day of the workshop : Neuro-symbolic methods. It introduces a novel approach to train and enhance the Natural Language Understanding (NLU) models that are known to use knowledge graphs during training. The proposed method claims to increase performance without manipulating NLU benchmarks.

## 2. **************************************************Evaluation of the quality**************************************************

For the sake of this review, these indicators of research quality were inspected :

- **Do hypotheses follow logically from previous work?**
    
    A clearer alignment between the research question and the methodology is needed. The authors address the vulnerability of NLU benchmarks to manipulation,  but evaluate their proposed approach on the same benchmarks they criticize. 
    
- **Are the background literature and study rationale clearly articulated?**
    
    While the authors mentioned some related literature, they haven’t included a well-detailed section for related or previous work, which is needed to clarify the differentiation of the proposed approach from previous methods in terms of combining input data with external knowledge, and how their method doesn’t manipulate bechmarks to inflate performance.
    

- **Could this methodology have answered the addressed issue/question?**
    
    The problematic being addressed is the vulnerability of NLU benchmarks to manipulation and the difficulty of evaluating NLU models, so the reader might conclude that the paper is going to propose new ways of fairly evaluating NLU models, and not propose a whole new NLU pipeline. The authors are highly suggested to find alternative problems to address that could emphasize their approach's ability to avoid benchmark gaming.
    

**Recommendation:** A more thorough literature review is encouraged, as it can significantly emphasize the work of the authors and highlight the potential of their proposed method.

## 3. ************************************************Evaluation of clarity :************************************************

- The style of writing as well as the language being used by the authors is clear and straight-forward.
- The authors guide readers throughout the whole article and explain every figure.
- The paper is formatted according to AAAI conference guidelines, it respects all the formatting instructions.
- The figures are really well presented and commented.

## 4. **Evaluation of originality:**

- The idea of incorporating additional objectives in the training process and minimizing a combined loss to enhance the training has been used in many domains (in Physics Informed Neural Networks for example), but I am not aware whether this idea as been applied before in the context o NLU models.
- The incorporation of the new losses to initial cross entropy sounds promosing in terms of helping the model learn to authentically understand natural language and perform better on NLU tasks, but this has to be further evaluated and confirmed.

## 5. **********Evaluation of the significance of the work**********

- According to their result section, they got an improvement of up to 2% and average of 1% compared to baselines : BERT, RoBERTa and ALBERT on GLUE benchmarks (actually the maximum is 1.81% and average is approximately 0.67%, after redoing the calculation from Table 1).
- The proposed method of training is promising, but lacks more work.

## 6. ****Critical review of each section****

Instead of a general pros and cons list, here are bullet points with critical reviews of each section individually :

**Abstract :** 

- The discussed issue is the vulnerability of NLU benchmarks to manipulation by neural models, so proposing a new method and testing it on these same benchmarks raise skepticism about its effectiveness.
- The claim of avoiding benchmark gaming needs more substantial evidence.

**Introduction**

- There is some redundancy in the first two sentences, they said the same thing twice by reformulating it differently.
- When mentioning previous works, they mentioned a very recent approach  (Zhu et al. 2023) with under 20 citations and said it’s a well established approach, and based their addressed issue on it.
- They mentioned that the approach they used for training prevents models from exploiting statistical artifacts to artificially inflate performance metrics, but they didn’t argument.
- The promised examples illustrating the capacity of models trained using the RDR methodology are missing in the results section.
    
    

**RDR methodology**

- The traditional method that they described lacks references or examples.
- They haven’t elaborated on the difference between the language model in the traditional method and the paraphrasing model in their proposed method.

**Figure 1 :** 

- The process of extracting the ground truth subgraph from the input and external graph, in order to compute the graph embedding loss, is not clearly explained.
- No details were provided on its inner workings of the paraphrasing model : it outputs recapped input and embedded input at the same time.

**Experiments and Results :** 

- Mention of five knowledge graphs with citation for only three.
- Comparison was done with baseline models that do not follow the traditional method described in the methodology section, so the methods are totally different. For example : BERT doesn’t use a knowledge graph in training.
- Evaluation on benchmarks criticized for vulnerability and manipulation without substantial arguments.
- The claim of non-manipulation based on randomly choosing 10% of the knowledge graph lacks supporting details."
212,"The paper discusses the extension of deep learning emulation for assessing stochastic events across different parameter regimes, focusing on spontaneous transition events driven by stochastic factors. It employs transformer-based networks to emulate complex dynamical systems, highlighting a significant computational cost reduction and the ability to generalize across diverse scenarios. This research presents a novel approach in combining deep learning with stochastic dynamics, aiming to enhance the understanding and prediction of unpredictable events in various scientific domains.

The manuscript presents an innovative integration of deep learning with stochastic dynamics, using the Stochastic Transformer (ST) to emulate complex systems across various parameter regimes. This approach not only demonstrates a significant computational efficiency but also extends the model's generalization capabilities to novel conditions. The use of pretraining for the SST across a broad spectrum of parameters showcases an interesting application of current PDE research with deep learning in stochastic modeling.

However, the application to a single system, though understandable given scope constraints, leaves open the question of the methodology's scalability and effectiveness across diverse domains. Future work could benefit from exploring a range of systems to validate the approach's universality, as also highlighted in the final section of the paper.

Minor comment:
- Second sentence in 4. Conclusions: typo for ""it's"" where it should be ""its"".

Overall, the paper is articulate and detailed, effectively communicating its contributions and findings. Given its novel application and the depth of analysis, it would undoubtedly spur valuable discussions at the workshop. I recommend acceptance of this work."
213,"This paper studies the sample complexity of threshold functions in the context of noisy queries each of which errs with a certain probability i.i.d. The authors present both a lower bound and an upper bound on the sample complexity, which match up to a constant factor. The employed algorithms built on several pre-existing ideas, including the 'checkbit' from reference GX 23 for k≤n/log and 'maxheapthreshold' from Feige et al. 1994 for k≥n/log. The established lower bound merges ideas from the Le Cam two-point method and concepts from Feige et al. 1994.

The problem of learning threshold functions with noisy queries is a fundamental problem in learning theory. Hence, obtaining an accurate understanding of its sample complexity is of significant theoretical interest. The primary strength of this paper lies in its resolution of a fundamental theoretical problem in learning theory. However, its weakness stems from the lack of novelty in the techniques used. The authors essentially combine several existing standard techniques and perform a dedicated analysis, which, while demanding considerable effort and sophistication.
Given the fundamental nature and theoretical interest of the results, I recommend that the paper to be accepted by ALT."
214,"Summary:

The paper introduces a novel online learning framework that extends traditional delayed and corrupted feedback models to handle adversarial environments where feedback evolves over time. In this new setting, the observed feedback can change after each round, potentially overriding previous observations and introducing unique challenges for regret minimization.
 
The authors propose algorithms for both full-information and bandit settings, achieving regret bounds that depend on the accuracy of the feedback over time. This generalized evolving feedback setting includes many previous settings, like delayed feedback, corrupted feedback, optimistic delayed feedback, and composite delayed feedback. In the optimistic delayed feedback setting, after choosing the action, the agent observes the hints for the delayed feedback, which is also included in the evolving feedback setting. For this setting, they show the first regret bound for the bandit setting. 

Pros:

(1) The paper successfully generalizes several existing feedback models (e.g., delayed, corrupted) into a unified framework, showing broad applicability to real-world problems.

(2) The paper is well-structured, and the explanations of both the model and algorithms are clear and accessible, making the theoretical contributions easy to follow.

(3) The regret analysis is thorough and demonstrates asymptotically optimal regret bounds for many previous settings including delayed feedback setting and composite delayed feedback setting. For the optimistic delayed feedback, they achieve the regret bound for the bandit setting which was previously not known. 

Cons:

(1) Although the framework is extended to evolving feedback, the main algorithm (Algorithm 2) and its analysis share substantial similarities with the time-delayed feedback setting discussed in Van der Hoeven et al. (2023). This overlap may limit the perceived novelty of the core algorithmic contributions. Maybe the authors can elaborate more on the specific differences between the new regret analysis and that of Van der Hoeven et al. (2023), particularly in the aspects that handle evolving feedback.

(2) Section 3.2 is a little confusing. In Algorithm 3, there is an otherwise part in the last line, which is not clear in which case to use and seems not required since the part before it already truncated at $\tau+d_{\max}$. In Corollary 10, the $\bar{\Lambda}$ is an upper bound on the total inaccuracy (ignoring inaccuracy larger than $d_{\max}$), but ignoring the large delay part is not shown in the formula. Also, since here you assume the feedback for the large delays is $\varepsilon$ accurate, it seems unnecessary to include this. 
Finally, the regret bound in Section 3.1 works for the maximum delay is small and the regret bound in Section 3.2 can handle the large delay with the assumption that the feedback is fairly accurate for large delays. What would the regret bound be for the large delay without this assumption? Is the proposed regret bound still working?"
215,"This paper has sufficient sections and provides detailed information for each section.
However, there are some problems that must be solved.
1. Validation scores are not presented in the abstract.
2. Figure 1 is too small, and the quantized model should be detailed.
3. Please use 'Fig.' as the Figure identifier instead of 'Figure'.
4. No vertical lines in tables.
5. Table 3 exceeds the default layout.
6. The first author and abbreviated paper title are not shown in the header.
Please go through the paper and correct some errors."
216,"The authors make an interesting case for using PINNs to solve PDEs over traditional semi-discretization methods and solvers. They provide a simple example of a steady-state PDE that experiences strong numerical diffusion for a case when the gridlines are at a 45-degree angle to the convection velocity. The authors convincingly show that solving the same PDEs with PINNs does not produce any numerical diffusion. 

Pros:
- The work makes a really nice case for using PINNs to solve PDEs over traditional methods. This could be a convincing argument for sceptics why PINNs may be useful in their work

Cons:
- The authors do not give a lot of details about how they solved the steady state problem with classical methods
-  While the contribution makes a novel argument for why to use PINNs, the method presented itself is not novel"
217,"The paper attempts to present an in-depth analysis of the current state and future directions of clinical Large Language Models (LLMs). The authors compare recent clinical LLMs, focusing on medical knowledge injection and domain-specific tuning or pretraining approaches. They provide insights into model performance across various benchmarks, including MedQA and PubMedQA, and discuss the implications of continuous pretraining versus supervised fine-tuning, model size, compute power, and the quality of training data.

**Pros**
1) **Easy to follow**: The paper is easy to follow but has some issues (pointed out in Cons section)
2) **Authors Cover various aspects of Clinical LLMs**: 

    2a. The training data and approach are well summarised in Table 1.

    2b. The Results section provides insightful comments on continuous pretraining versus supervised fine-tuning, model size versus compute power, and the quality of training data.

**Cons**

1) **Evaluation in Table 1**: It's unclear whether the authors conduct the evaluations themselves or if the numbers are reported from respective papers. The table caption states, ""For each model, only listed best performance,"" but it's not clear what ""best performance"" refers to. Additionally, the metrics used for performance (e.g., accuracy) should be clearly mentioned in the table.

2) **Some excerpts need to be refined:** 

     2a. The conclusion appears to be very brief.

     2b. The discussion on downstream use cases is not comprehensive. Important industrial downstream tasks, such as early prevention of diseases and personalized medicine based on patient history, are not adequately covered.

4) **Novelty:** The paper seems to add little value to existing surveys [1, 2, 3]. Many of the discussed points, such as fine-tuning versus pre-training, training data, results on MedQA and PubMedQA, along with applications of LLMs in downstream use cases, are already extensively covered in the cited survey papers.

5) **Lack of Related Works**: Significant related works [2, 3] are not mentioned, which is a notable omission.

[1] Zhou, H.; Gu, B.; Zou, X.; Li, Y.; Chen, S. S.; Zhou, P.; Liu, J.; Hua, Y.; Mao, C.; Wu, X.; et al. 2023b. A survey of large language models in medicine: Progress, application, and challenge. arXiv preprint arXiv:2311.05112

[2] He, Kai, et al. ""A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics."" arXiv preprint arXiv:2310.05694 (2023).

[3] Singhal, Karan, et al. ""Large language models encode clinical knowledge."" Nature 620.7972 (2023): 172-180.

While the authors mention results and discussions insightfully, they lack novelty, and most points exist already in the survey papers pointed out above. Additionally, the missing evaluation details in Table 1 are a concern."
218,"## Summary
In this work, the authors present EEGFORMER, a foundational model for electroencephalography (EEG) data. They present a new pretraining method for EEG data, that works as follows: first, EEG signals are segmented into patches and passed into a Transformer encoder. Then, they apply a vector-quantized model to convert the patch representations into discrete indices, which as subsequently fed into a Transformer decoder, with the objective to reconstruct the input. The authors apply EEGFORMER in 5 downstream tasks, showing good performance and transfer. Moreover, they show that the representations learned can also be highly interpretable. 

## Strengths
- The self-supervised approach the authors propose is promising, and has the potential to efficiently utilize the vast amounts of raw EEG data available.
- The idea to encode the EEG signals into quantized vectors can push the model towards learning interpretable representations, e.g. similar signal patches may be mapped into the same quantized encoding, that may then help us interpret predictions, as the authors show in an experiment.
- The empirical evaluations demonstrate good performance on all downstream tasks tested, and seem transferable. 

## Weaknesses
- It would be good to further explore the transferability and interpretability of EEGFORMER's representations, by performing further experiments on additional cases and corpora, to verify if the authors' observations truly generalize.  

## Overall Assessment 
The paper proposes a novel foundational model and pretraining method for EEG data, and shows strong downstream results and promising interpretability of the representations. It has the potential to pave the way for utilizing large amounts of unlabeled EEG data for various downstream tasks."
219,"Summary: 

NeuroMixGDP is a novel approach to privacy-preserving data release that proposes a new mixup scheme inspired by the Neural Collapse phenomenon. The paper discusses the challenges that can arise when using the feature mixup framework, such as the sensitivity blowup of RW-Mix and label collapse. The authors examine how Avg-Mix and HS can be used to address these issues, respectively, and how these approaches informed the development of their novel designs, NeuroMixGDP(-HS). The paper provides the asymptotically optimal mixup degree rate using GDP. The proposed method is shown to significantly enhance the utility of released data while protecting user privacy. The paper also discusses the effectiveness of the proposed method in defending against attacks, such as model inversion attack and membership inference attack. Overall, the paper presents a promising approach to privacy-preserving data release that can improve the utility of released data while protecting user privacy.

Pros:

1. Enhances the utility of released data: The proposed mixup scheme can significantly improve the utility of released data, making it more useful for downstream machine learning tasks.
2. Asymptotically optimal mixup degree rate: The paper provides the asymptotically optimal mixup degree rate using basic linear model, which can help to understand the ""sweet spot"" choice of mixup degree.  
3. Defends against attacks: The paper demonstrates that the proposed method can defend against attacks such as model inversion attack and membership inference attack.

Cons:
1.  Lacks some definitions: some core definitions are lack (utility and sensitivity), which poses certain difficulties to readers who are unfamiliar with the DP area.
2.  Requires further validation: while the paper presents promising results, the approach will need to be further validated and tested in larger datasets scenarios to determine its effectiveness and practicality.
3. Lacks neural collapse reference: The related neural collapse references are very minimal. It does not cite many of the theoretical works in NC literature (e.g. [1-7])
4. Some typos: the equation of y in definition 2.1 should use index j rather than i?  ""While the ... m samples"" in line 131-133 is not a complete sentence. 

Reference:

[1] Ji, Wenlong, et al. ""An unconstrained layer-peeled perspective on neural collapse."" arXiv preprint arXiv:2110.02796 (2021).

[2] Zhu, Zhihui, et al. ""A geometric analysis of neural collapse with unconstrained features."" Advances in Neural Information Processing Systems 34 (2021): 29820-29834.

[3] Han, X. Y., Vardan Papyan, and David L. Donoho. ""Neural collapse under mse loss: Proximity to and dynamics on the central path."" arXiv preprint arXiv:2106.02073 (2021).

[4] Zhou, Jinxin, et al. ""On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features."" International Conference on Machine Learning. PMLR, 2022.

[5] Zhou, Jinxin, et al. ""Are all losses created equal: A neural collapse perspective."" Advances in Neural Information Processing Systems 35 (2022): 31697-31710.

[6] Mixon, Dustin G., Hans Parshall, and Jianzong Pi. ""Neural collapse with unconstrained features."" arXiv preprint arXiv:2011.11619 (2020).

[7] Tirer, Tom, and Joan Bruna. ""Extended unconstrained features model for exploring deep neural collapse."" International Conference on Machine Learning. PMLR, 2022."
220,"Just be mindful about some typos. 
For instance, a the end of the first paragraph in the introduction, ""to model the of""."
221,"This paper introduces a new autoencoder design that uses convolutional sparse coding (CSC) and trains it using the closed-loop transcription (CTRL) method. The resulting generative model performs well in both image reconstruction and generation.

Pros:
1. Clear writing: The paper is easy to understand, and the experiments are well explained and supported.
2. Good literature review: The related works section covers the most relevant research in the field.
3. Strong experiments: The experiments are convincing, with comparisons to other models that make the proposed approach look solid.

Cons:
1. Discuss weaknesses: The paper should discuss its limitations more clearly.
2. Higher resolution experiments: Please test the model on higher-resolution datasets like 128x128 if possible, or explain why not and how it compares to the baselines in this regard.
3. Include timing information: Please clarify how long the training and inference take compared to the baselines.
4. Reconstruction quality metrics: Please use metrics like PSNR or SSIM to measure image quality in Figures 2, 3, and 5.
5. Please show the number of trainable parameters for different models in Table 1. You can also add training and inference times to this table.
6. Please Include the original images in Figure 5 so readers can see how well the model recreates them.
7. Please make Figure 4 clearer by adding more space between blocks and brief captions for each block.

I believe this is overall a decent paper. The proposed framework is promising but should address these concerns to improve its overall quality and clarity. These improvements could lead to a higher evaluation score."
222,"The paper provided clear understanding of problem statement, and necessary background information to understand the challenge faced by less accurate techniques for detecting co-morbid ADHD. The data used for training, however, is from a platform that is biased in representation of general population, which is also acknowledged by the authors, and also appropriately explained the limited scope of the application of the results discovered by the authors.
The quality of work is good and meets the expectation. I do not consider myself to be able to comment on the originality of the work, as I need more experience in the field to be fair in my evaluation, however, the work is fairly original in my opinion. Significance of study presented is that it provides the comparison of three different models in performing classification for the task at hand, and find a model that outperforms the other two by a significant margin. 
Pro of the paper is that it has found a model that has significantly higher prediction accuracy over other models.
Con of the paper is that the data set is biased and the visualizations cannot be published to protect the patients.
However, the results of the study are significant enough to outweigh the cons. This paper deserves publication in the esteemed conference."
223,"This paper addresses the significant challenges and limitations faced by current ensemble methods in achieving robust generalization under distribution shifts. It proposes a novel framework that incorporates selective models evaluated by error, supported by theoretical proof, and filtered by Determinantal Point Processes (DPP), taking into account both diversity and quality.

Strength:
 - The authors propose a novel kernel construction based on DPP to select models that balance both high performance (quality) and diversity.
- The authors first propose a generalization error bound and provide strong theoretical derivations to demonstrate that the previous weight-averaging methods for model aggregation can achieve a tighter generalization error bound by considering the contributions of both quality and diversity.
- The method is plug-and-play: This approach is designed to be easily applicable without requiring extensive modifications to existing systems.
- The method overcomes the limitations of traditional assumptions for i.i.d. data: It provides a solution that is not constrained by the i.i.d. assumption, making it more flexible and applicable to a wider range of scenarios.
- The output of experiments seems improved, even though the test dataset is small and there is a lack of benchmarks from other merging methods in the field for comparison

Weaknesses:
- Limited novelty: The proposed method mainly introduces the use of DPP for selecting ensemble models and provides theoretical support. However, the innovation is somewhat limited, as it primarily focuses on applying an existing technique (DPP) without introducing substantial new methodologies.
- Lack of effective comparison: The experiments lack comprehensive comparisons with other established methods in the field. Only the proposed method is added on top of the baseline, which makes it difficult to evaluate its performance relative to other merging techniques. The absence of benchmarks from other merging methods in the field undermines the robustness of the comparison.
- The derivation of the generalization error bound is also based on certain assumptions(like i.i.d), which may not hold in some cases, potentially affecting the practical applicability of the theoretical results."
224,"paper summary  
The paper presents a novel approach to the Abstraction and Reasoning Corpus (ARC) that integrates LLMs with program synthesis solvers based on a DSL in a reflection-based architecture. It introduces AugARC, an enhanced benchmark to boost LLMs generalization.  This approach achieves a record 166/400 accuracy on ARC tasks, outperforming previous methods.

**Originality**  
- **Strengths**:
The Reflection System leverages the strengths of both LLMs and program synthesis solvers based on a DSL, addressing the dataset limitations of the ARC through the AugARC benchmark. This benchmark enhances LLM generalization by incorporating augmented tasks, which broaden the scope of the original ARC. Furthermore, the system employs a self-reflection technique inspired by those used in LLMs, intelligently combining multiple solvers to tackle ARC tasks. The Reflection System demonstrates notable flexibility by supporting various solver types (e.g., LLMs and program synthesis tools) and allowing dynamic adjustment of solver configurations.

- **Weaknesses**:
The model lacks a theoretical analysis explaining why this reflection mechanism architecture improves task performance. 

**Quality**
- **Strengths**:
The experimental setup is comprehensive and the performance improvement is significant.   Fine-tuning experiments with smaller LLMs (e.g., 7B and 13B parameters) demonstrate a significant improvement in ARC task performance, highlighting the value of data augmentation.

- **Weaknesses**:
 The theoretical foundation behind the reflection process is not fully explored. Specifically, how the reflection model determines the correct solution among solvers could benefit from more rigorous justification. The computational complexity of the reflection system, especially with multiple solvers and fine-tuned LLMs, is not deeply discussed. Most of the gains come from combining with Claude 3 Opus, raising questions about generalizability with other solvers.    


**Significance**
- **Strengths**:
ARC is a challenging benchmark, and improving its performance meaningfully contributes to advancing AI’s ability for broad generalization and abstract reasoning. The system provides a new perspective on combining solvers, demonstrating the potential of reflection-based architectures for other reasoning benchmarks. The fine-tuning results suggest that even smaller LLMs can perform well on reasoning tasks, making the approach accessible for researchers without access to large-scale models.  


- **Weaknesses**:
While the improvement over the ensemble system is clear, the gain of solving five additional tasks may not seem dramatic to practitioners.


**Questions and Suggestions for the Authors**
- Could the authors provide theoretical insights into why the reflection model effectively selects the correct solution?
- How does the computational complexity of the system (e.g., fine-tuning, running multiple solvers) scale with graph size or the number of tasks? 
- While the results demonstrate state-of-the-art performance, could the authors include more extensive comparisons with other ensemble approaches, such as those combining different LLMs without program synthesis solvers?
- Since DSL Search contributes most solutions (160/400), is the reflection system’s performance reproducible with alternative program synthesis solvers?    



**Limitations**
- The system relies on DSL Search and Claude 3 Opus for most of its performance gains.
- The computational cost of fine-tuning, reflection, and multi-solver integration could make the system infeasible for larger datasets or less computationally capable researchers.



**Ethics**

There are no obvious direct ethical concerns related to the method as it stands. The paper does not deal with sensitive data or produce sensitive content. The approach is a method improvement and not directly involved in human-facing decision-making applications at the evaluation stage. No unethical dataset or methodology usage is apparent. Thus, no ethical issues need to be flagged for special ethics review."
225,"1. Please add a paragraph to describe the strategies for improving inference efficiency. Even failed attempts are also worth mentioning. 

2. Sec 2.4: Instead of only using one sentence as a paragraph, please merge it with other paragraphs. 

3. Fig. 1-2: Please add more descriptions of the network in the caption. 

4. Table 6: Explain w/o and w/ in the caption. 

5. Please add a comprehensive description of the algorithm's advantages and weaknesses in discussion section. 

6. Please add a paragraph on future work. 

7. References: please delete the doi links"
226,"This paper studies submodular maximization in a stochastic bandit feedback model. There is an unknown submodular function $f$ on some discrete universe U whose output is in the range $[0,1]$. The algorithm at each step picks a set S \subseteq U, and gets reward $f(S) + noise$.   In the offline setting, when $f$ is submodular but not necessarily monotone, and $S$ is unconstrained, there is an algorithm that gets a $1/2$ approximation to the submodular maximization problem. In this work, the authors consider the online stochastic bandit setting, and aims to minimize the gap between the algorithm’s reward and $OPT/2$. This is referred to as the (1/2)-approximate pseudoregret.

The adversarial experts version of this problem has algorithms that achieve an approximation regret of $O(\sqrt{dT})$. In the bandit setting, the work of Niazadeh et al. (2021) achieves a regret bound of $O(dT^{2/3})$. Fourati et al. 2023 achieve regret $O(dT^{2/3} log^{1/3} (dT))$ for the stochastic setting.

The main result in this paper is an adaptation of the Double-Greedy algorithm to this stochastic bandit setting, and shows a 1/2-approximate-regret bound that matches the Fourati et al. result in the worst case and can be better in a problem-dependent setup. In particular, the paper defines a quantity $H_f$ for a submodular function $f$ and shows that the regret of their algorithm is controlled upto logarithmic terms by $H_f$. In particular, this means that the regret is $function(f) \log dT$ and has no polynomial dependence on $T$.

Intuitively, this is achieved by running the Double Greedy Algorithm of Buchbinder-Feldman-Naor-Schwarz, where at each step a 
randomized decision is taken. In this algorithm, the randomized decision is made adaptively. Briefly, the new algorithm stops early in this “step” if it finds a $p$ that is guaranteed to be good even accounting for the noise in estimation, it takes that step. Else, it continues until the estimation noise is small enough to lead to low regret. The authors define a quantity depending on the function $f$ (and the ordering of the points ${1,2,…d}$) and show that this quantity allows them to upoer bound the number of steps taken by the algorithm.


*Questions*


- Does the adversarial bandit upper bound of $O(dT^{2/3})$ imply the same bound for the stochastic setting? Why/why not? What does this mean about the Fourati et al. result and your worst-case result?

- The paper is missing a discussion of several relevant references. E.g. Streeter and Golovin: “An Online Algorithm for Maximizing Submodular Functions” seems to be an early work on online submodular maximization, and Yue and Guestrin (Neurips 2011) studies some version of submodular bandits.
Much more relatedly, the current work seems to be much more in the vein on maximizing submodular functions given a noisy oracle. There seems to be a long line of work on this question (e.g. “Noisy Submodular Maximization via Adaptive Sampling with Applications to Crowdsourced Image Collection Summarization” by Singla, Tschiatschek, Krause. Also there is the long line of work on Optimization-from-Samples by Yaron Singer and colleagues, and work on “Stochastic Submodular Optimization”. These seem to me to very intimately related to the current work.

- It would be interesting to understand if this H_f also gives a lower bound on the regret of this algorithm, or better still for any algorithm."
227,"Authors present the use of LLM to improve psychotherapy sessions integrating common factors approach into LLM to provide feedback.
The paper is certainly original. It would enhance the clarity and understanding of the paper if authors present more detail on how the system is built, the interpretation of the metrics and how the system can improve the quality of visits. I would also appreciate a discussion on the ethical or social implications of using this type of technology in the medical setting."
228,"The paper addresses the challenge of adapting generalist foundation models like SAM to medical image segmentation tasks, crucial for diagnostic and treatment processes. Despite initial promise, the specific adaptation model, MedSAM, fails to consistently outperform SAM in medical task-specific contexts. However, the introduction of LoRaMedNet, a novel parameter-efficient approach, demonstrates the potential for enhancing SAM's adaptability and achieving superior performance in medical tasks. The findings underscore the significance of exploring adaptation techniques for generalist models, suggesting that they can excel in specific medical applications even without dedicated medical pre-training."
229,"### Summary of contributions:
This work demonstrates, both theoretically and empirically, how adding weak correlations between weights in an infinitely-wide neural network can counter the dissimilating effect of sparse activations which in turn improves generalization performance on three small scale datasets.

### Strengths:
1. This is a timely and important topic as sparse activations are observed in many neural network architectures and in particular the now ubiquitous transformer architecture. 
2. The paper is well motivated based on prior art and biologically similar mechanisms within cortical circuits. 
3. The study of using correlated weights in a NNGP setting appears to be novel. 
4. The formula presented for calculating the optimal weight correlation is of particular interest. Future work exploring applications of correlated weights may be able to realize modest generalization performance improvements by implementing correlated weights. 
5. Empirical results generally match very closely with results predicted by theoretical results, particularly for experiments with smaller training sets. 

### Weaknesses:
1. Experiments with m=20 do appear to consistently yield the best generalization performance, especially at moderate to high sparsities with small training datasets (P<=512). However, as P is increased, the gap between m=0 and m=20 appears to narrow, particularly for CIFAR and MNIST. I also note that the gap between theoretical predictions and empirical results in Figure S4 appears to increase proportionally with P. A more detailed discussion on the effect of increasing training set size would improve the paper. 
3. While typical of the NNGP literature, the datasets used for the analysis are small and the training set sizes used are very small for typical training of neural networks. It is unclear if the weight correlation benefits will remain if larger, real-world datasets are used. Did the authors conduct any experiments using training sets larger than 4096? If not, what was the reason for excluding larger numbers of training samples? I remain skeptical if benefits observed with weight correlation will continue to be present as the training set size is scaled up.
4. The empirical study is limited to the NNGP setting. Experiments with finite-width networks in addition to the kernel method experiments would help establish whether the proposed weight correlation methodology is broadly applicable to real-world neural networks. 
5. For MNIST and FashionMNIST, the benefits of weight correlation appear to be most prominent in the highly sparse regime. However, the best absolute generalization performance is typically found at modest sparsities where the benefits of using correlated weights are less compelling.  
6. Only a single type of network architecture was considered for the empirical study. Additional experiments with deeper models would improve confidence in the results.


### Clarifications:
1. ""More recently, the presence of sparse activation has been observed in high-performance neural networks such as AlexNet, **ImageNet**, LeNet, and various models of Transformers, even without explicit regularization for sparsity.""ImageNet"" typically refers to the ILSVR challenge datasets and not a neural network architecture. Please confirm intended network. 
2. ""f"" in equation 5 refers to the ""sparsity level"". However, I believe the actual intent of this variable is to represent the ""fixed fraction of neurons with non-zero activations"" as per [1]. I would expect sparsity level to be defined as (1-f) if this is correct. Please clarify.
3. Figure 3 caption states that ""Red triangle marks f = 0.2, m = 20 as the best-performing model."". This seems like a reach to me as essentially all correlation levels have the same performance at that sparsity. What is the absolute difference between m=0 and m=20 at the red triangle? If it's as small as it appears in the plot, I recommend removing this statement as it seems disingenuous. 
4. On line 192, P is defined as ""the size of the training set"". This is clearly established in plots throughout the paper, but seems to be placed somewhat awkwardly with respect to equation 13 since P does not appear in that equation.  
 
### Suggested changes:
1. Adding hyperlinks to the in-text citations that link to the bibliography would be helpful. 
2. Move definition of P to a more appropriate location, preferably near Figure 2 when it is first used in caption and figure plot titles. 
3. In Figure 4b), m=150 series is hard to read. Consider use of a different color. 

### Broader impact concerns:
None.

### Citations
[1] C. Chun and D. D. Lee, “Sparsity-depth Tradeoff in Infinitely Wide Deep Neural Networks.” arXiv, May 17, 2023. doi: 10.48550/arXiv.2305.10550."
230,"The authors performed LoRA finetuning on top of Mistral 7B model to do TNM staging classification tasks on breast cancer pathology reports. The authors carefully curated a dataset of anoymized reports with labels from subject matter experts. The results look very promising. Only one foundational model was used in the finetuning and evaluation, so it is unclear whether there could be significant result difference among different foundation models with different sizes. It is also unclear how well the model is generalized, e.g. whether the quality and the format of the original reports may affect the final results,  though the authors mentioned it in the future work section."
231,"This work presents a simulation of porous media using physics-informed ML models. The idea is not new and has already been done in the literature; see, for example,

1. Faroughi, S. A., Soltanmohammadi, R., Datta, P., Mahjour, S. K., & Faroughi, S. (2023). Physics-informed neural networks with periodic activation functions for solute transport in heterogeneous porous media. Mathematics, 12(1), 63.
2. Lehmann, François, Marwan Fahs, Ali Alhubail, and Hussein Hoteit. ""A mixed pressure-velocity formulation to model flow in heterogeneous porous media with physics-informed neural networks."" Advances in Water Resources 181 (2023): 104564.
3. Faroughi, Salah A., Ramin Soltanmohammadi, Pingki Datta, Seyed Kourosh Mahjour, and Shirko Faroughi. ""Physics-informed neural networks with periodic activation functions for solute transport in heterogeneous porous media."" Mathematics 12, no. 1 (2023): 63."
232,"The paper presents a foundation model based on a custom dataset called CheXinstruct. The authors train a BLIP-2 model on the proposed dataset and evaluate on several downstream tasks. For the metrics the authors choose accuracy as stated in table 1, which seems to be ill-chosen in the chest x-ray domain since there is a massive class imbalance. Also in the proposed evaluation benchmark a comparison from the generalist BLIP-2 model based on mistral 7B  to expert models specific to the individual tasks as a quasi upper bound is missing. Especially for the classification tasks this might provide some better perspective on the difficulty of the task. Similarly, a simple baseline might be beneficial to get a grasp of the lower bound of each task.
Overall there are some serious concerns regarding the evaluation protocoll of this paper which might be resolved by the use of better suited metrics and comparison models."
233,"In this paper, the authors explore the application of linear transformations to features, leading to a joint optimization encompassing parameters and transformations, while adhering to a bilinear interpolation constraint. Additionally, they demonstrate how constraining adaptivity imparts specific regularization characteristics to the solution, resulting in a group approximate low-rank penalty on a neural network-based model. This provides a framework for understanding the properties of neural networks. In summary, this paper presents an interesting and valuable contribution.

Comments:

1. In Line 146, you assume that $M$ is symmetric positive semidefinite, but it would be helpful to explain how this condition is guaranteed through the rotation of $\theta$. Furthermore, I am curious if your conclusions are applicable to symmetric matrices, which are more commonly encountered in practical implementations.

2. Reviewers have suggested that the authors provide a simplified proof sketch in main paper to assist readers in grasping the key points of the proof more easily. This addition would enhance the paper's accessibility and comprehension."
234,"The paper provides convergence bounds in $\Phi$-divergence for two discrete-time Markov chain algorithms, the unadjusted Langevin algorithm (ULA) and the proximal sampler, for the class of all twice-differentiable strongly convex $\Phi$. The main results are presented as Theorem 1 and Theorem 2, and the bulk of the work focuses on explaining and outlining the proofs. The key technical tools are 
- Definition 3, which define the Phi-Sobolev inequality used as assumptions to the main results,
- Lemma 7, which relates the rate of change of Phi-divergence along a SDE regardless of its drift, and
- Definition 10, which defines a strong data processing inequality in terms of Phi-divergence and whose contraction term is controlled throughout the subsequent proofs.

Strengths:

- I find the paper very well-written. It not only provides a clear contribution but also does a very good job at providing the relevant literature context and explaining the complex technical steps

- While $\Phi$-divergence results for continuous-time ULA, as well as special cases of $\Phi$-divergence results for both ULA and proximal samplers, are known, general $\Phi$-divergence resutls for discrete-time settings are not available to the best of my knowledge, and I think this paper provides concrete contributions in this direction. I also find the authors' unifying view of known results and proof techniques through Phi-divergence very satisfying; the exposition has done a very good job in relating results that use log-Sobolev inequality / Poincare inequality and that use different metrics together, and allows readers to understand its shared mathematical structure.

Minor comments:

- The main results seem to convey that, depending on the Phi-sobolev inequality satisfied by the stationary distribution, the algorithms will enjoy convergence guarantees with different strengths (in the sense of convergence in different d_Phi's). While it is easy to see how the strengths of d_Phi's compare according to different Phi's (e.g. KL is dominated by chi-squared), comparing the strengths of different Phi-sobolev inequalities is a bit less obvious. I would appreciate it if the authors can say a word or two about this, which would allow for a better appreciation of the general results beyond the common settings of KL and chi-squared divergences

- I also feel that the paper's contributions would have been stronger, if the authors can show that this unifying view through Phi-divergences allows for new discrete-time results through other metrics that were previously unknown. However I do understand that this can be difficult.

- In the second paragraph below Theorem 1, the authors say ""It is interesting to ask for .... weaker assumptions on $\nu$ (See Section 5)... Therefore one can ask if ... when $\nu$ undergoes a suitable perturbation."" The wording is slightly confusing: It seems to suggest that additional results on this are available in Section 5, but Section 5 is just a discussion section, and if I understand correctly, neither the paper nor the appendix actually provides such results. I think it'd be great if the authors can make it clearer that this is a point of discussion / future work rather than something in the paper.

- I wonder whether there's an intuitive explanation of why the drift term b_t disappears in Lemma 7. As commented by the authors at the bottom of page 11, this plays a role into the results later on. While I appreciate that the algebraic terms cancels by inspecting the proof, I wonder whether there's some intuition which would help readers appreciate Lemma 7 better, especially since it's a core tool of the paper. Again I understand that this may be difficult and not obviously doable.



All comments above are minor points that I think could improve the paper even further. Even if none of the above are addressed, I think the paper is clearly above the acceptance threshold and I am happy to see it being accepted into ALT."
235,"Neural PDE solvers need accurate methods for quantifying the approximation uncertainty. This work focuses on the Galerkin method as a neural PDE solver. It then interprets the Galerkin as maximum a posteriori estimator, and then the posterior is approximated via Laplace approximation to capture the uncertainty. Experiments with heat equations and burgers PDE demonstrate the effectiveness of the method.

The proposed method based on Laplace approximation makes sense. The experiments however are for toy PDEs, and more extensive experiments for realistic scenarios are needed to prove the method. The inversion in laplace approximation also impedes scaling the method."
236,"The paper provides an analysis of the plugin method for solving average-reward MDPs with generative sample access. It shows that this canonical method achieves optimal sample complexity in the diameter and uniform mixing time setting. Importantly, these guarantees are achieved without the requirement to know problem-dependent quantities, a limitation of other methods based on reduction to discounted MDPs. The key ingredient in the analysis is a new error decomposition which recursively expands the error with higher-order terms. This technique also leads to sharper sample complexity bounds in the discounted MDP setting.

The paper is well written and overall easy to follow. The authors do a good job putting their results and techniques in the context of prior work. The paper also manages to convey good intuition for required techniques that enabled the new sample complexity results, in particular in the proof sketch in Section 4.

Optimal sample complexity results for the simple plugin method are a significant contribution. Many algorithmic and analytical techniques are similar to prior work, e.g. anchoring, reward perturbation or the higher-order error expansion. Still, details are different and those details matter. Certainly, showing that an anchoring probability of 1/n is sufficient is a very nice result. This paper offers a valuable set of tools that are likely to be useful in other settings and future research. All presented results are plausible, and no technical errors were identified in the proof sketch or a brief skim of the full proofs in the appendix. Overall, this is a strong paper that merits acceptance."
237,"This paper introduces MedSyn, a new hierarchical diffusion model approach for generating CTs. While this is a valuable contribution, more motivation behind design decisions are needed. For example, why isn't latent diffusion needed? What differs from medical diffusion and why does that lead to better performance? Evaluation on more standard public datasets would also be ideal for this paper for more systematic comparison (Medical Diffusion uses LIDC-IDRI for example)."
238,"Summary

This paper integrates GNN (graph neural network) with RL (reinforcement learning) to address PUZZLES problems. This paper claims the GNN can improve RL performance.

Compared with RL-only method, the new method reports considerable improvement on small size puzzles (2x2 ~ 6x6), but it only reports similar performance (to RL-only method) on larger size puzzles.

Beside above comparison, other empirical evaluations are based on the new method itself with different configurations, e.g.: iterative vs partial, recurrent vs state-less, GNN vs transformer. All these tests show a complete zero-solvability (cannot be solved) at x9 scale size. This empirical result matches the conclusion from other methods and research.

Issues

1. this paper does not provide a comprehensive explanation of how these puzzles transformed to a GNN network. Also, no runable program provided. 

2. At large scale-size problem, this GNN+RL method matches the performance of RL-only method, it only perform better at (2x2 ~ 6x6) small scale size. Also, it lacks enough cross-comparison from RL-only method. 

3. The major reason for its better performance (at small scale) is due to its richer NN-node representation capturing more state relationships. This benefit is not scalable when PUZZLES size is growing up."
239,"*Summary*

In this paper, the authors study the error probability of the optimal decision rule in the binary advice aggregation setting. This paper provides sharp upper and lower bounds in the asymmetric case, which sharpen the best known results in the symmetric special case. 

*Strength*

The authors make concrete contributions to the binary advice aggregation setting by providing a novel and sharpened analysis of the error probability of the optimal decision rule.

*Weakness*

It would be better if the authors discuss more about the challenge of asymmetric setting compared to previously studied symmetric case."
240,"Summary: This paper trained the RoBERTa model to predict whether individuals discussing anxiety in their posts will subsequently express interest in ADHD. They showed that it shows high performance (76% correct), which can give insights into their comorbidity. 

Comments:
1. It is unclear what the ability of the RoBERT model to classify the groups implies. There could be many ways that drawing some clinical insights from the model performance can go wrong or be indefinite. The examples are below
- Some symptoms of anxiety are also indicative of ADHD. The RoBERT model captures the terms related to the comorbidity. However, it still seems unclear if they are just associative or if they have a causal relationship.
- ADHD patients have some features in common in their posts (not related to disorder symptoms)
- Or it could just imply selection bias as the reddit is not prospective data. 
I think the implication should be more clearly stated. Also, I think modification of the experimental design could be necessary.

2. “Social media such as Reddit provides publicly available text data of anonymous
first-person experiences (Low et al. 2020).” This sentence at the end of the first paragraph in the introduction section looks abrupt. The first paragraph is mainly about the problem of misdiagnosis of ADHD and anxiety, so I think this sentence on the data source of this study should be discussed in the next paragraph."
241,"This paper considers the unconstrained online maximization problem of non-monotonic submodular functions, focusing specifically on situations where only stochastic bandit feedback can be observed. The authors aim to determine the achievable reduction in (1/2)-approximate regret. They propose a natural explore-then-commit algorithm based on the double-greedy algorithm and present an upper bound on the regret achieved by this algorithm. Notably, they define a new metric to measure the problem's difficulty and demonstrate an $ O(\log T) $ regret upper bound dependent on this metric.

**Strengths of the Paper:**
- Prior research is appropriately cited and compared, with a clear explanation of the paper's position in the field. Particularly in Remark 2, the weaknesses of the obtained results are mentioned appropriately.
- The newly proposed concept (a metric for the difficulty of the problem) is explained in a straightforward manner through specific examples.
- The paper demonstrates a steady improvement in the context of online submodular optimization, which is expected to attract interest within this research community.

**Weaknesses of the Paper:**
- The proposed algorithm is a combination of naturally inspired existing techniques (double-greedy and explore-then-commit), and it seems to lack technically non-trivial elements or challenging ideas.
- There is no regret lower bound supporting the algorithm’s effectiveness. However, this is typical in the context of analyzing approximate regret and should not be considered a significant weakness.

**Questions:**
Does the difficulty metric defined in this paper depend on the ordering of the indices of the items in the large set? If so, to what extent does it change depending on the ordering? My intuition suggests that it would be more natural for the intrinsic difficulty of the problem to be independent of the index ordering, and I am curious whether it is possible to define the metric in a way that reflects this."
242,"**Summary**
Self-directed learning is an online learning variant where the learner can adaptively choose the sequence of examples it is presented. The authors study node-classification with convex clusters, where if two nodes have the same label, all nodes on the shortest path between them share the label. They later relax this assumption to approximately convex labelings, and also discuss a simple algorithm for homophilic labelings.

**Pros:**
 - The application of self-directed learning to node classification is well motivated.
 - The authors take the time to carefully introduce all the definitions and notations used.
 - The Good4 algorithm is interesting, and the technical ideas introduced here could be of independent interest. It is also useful that the algorithm works for near-convex labelings with no modifications.

**Cons:**
 - The assumption on convexity is somewhat restrictive, and even with the near-convex case the mistake bound can be bad.

**Questions:**
 - In Proposition 2, it states that $M(Halving, \mathcal{H}) = O(h(G) \ln n)$. Is it the case that $vc(\mathcal{H})=h(G)$? (so it directly follows from the general mistake bound of the halving algorithm?)
 - If I am understanding the definitions correctly, for any set of vertices U, $q(U) = 3{|U| \choose 4}$? So q_good(U) is bounded from below by a function of the size of U, with no dependence on the graph structure? 

**Typos:**
 - Algorithms GridWalker and Traverse are mentioned but it does not say where they are defined. 
 - Typo in Observation 6: “Then, in any subset U ⊆ V of at least max(w, 4) nodes, there exists an ε-good node, and [the ?] node participating in the biggest number of good quadruples is [an?]ε-good node.”
 - Observation 7: Should this be {(a,b), (c’, d’)} forms a good quadruple?"
243,"This paper proves lower and upper bounds on the number of noisy queries needed to compute the threshold function (given $n$ input bits, whether at least $k$ of them are $1$s). Previously, it is known that $\Theta(n\log(k/\delta))$ queries is both necessary and sufficient (where $\delta$ is error probability), and this paper provides bounds on the leading coefficient. When $k=o(n)$, the lower and upper bounds match. For general $k$, there is a multiplicative gap of $2$ between the lower and upper bounds. Also note that the case $k=1$ is resolved in a previous work (Zhu et al. (2024)).

The lower bound (Theorem 1) occupies majority of the paper. The proof is divided into two regimes: the case $\log k \le \frac{\log (1/\delta)}{\log \log (1/\delta)}$ is proved by an easy application of Le Cam's two point method. The case $\log k > \frac{\log (1/\delta)}{\log \log (1/\delta)}$ is more difficult and is proved using a refined version of Feige et al. (1994)'s method, which reduces the query model to a two-phase version, where the first phase has non-adaptive noisy queries and the second phase has adaptive exact queries. (This second case is further divided into two sub-cases.)

The upper bound (Theorem 2) is by a simple algorithm.

In recent years there is an increased interest in determining the leading coefficient in the query complexity of noisy computation problems. The threshold problem is a very basic problem which everyone can appreciate, so it is natural problem to study. This paper is a solid step towards fully determine the leading coefficient and resolves the problem in the regime $k=o(n)$. The proof technique is not entirely novel (essentially a more detailed analysis of Feige et al. (1994)'s proof) but the work put into actually performing this more detailed analysis is non-trivial.

The paper is well-written and easy to understand.

In summary, this paper proves nice and interesting results on a natural problem. I recommend it to be accepted."
244,"This paper explores the use and adaptation of large-scale language modeling (LLM) to the clinical domain. It builds on previous work and provides an in-depth analysis of existing clinical LLMs, focusing on domain adaptation approaches. The study compares various models according to their medical knowledge infusion strategies, including domain-specific adaptation and pre-training from scratch using a medical corpus. The study emphasizes the effectiveness of continuous pre-training and supervised fine-tuning in improving model performance on MedQA and PubMedQA medical benchmarks. The discussion raises important points about the selection of training data, the potential for retrieval enhancement generation, and the need for careful consideration of copyright issues. The discussion also touches on the utility of clinical LLM in downstream applications and the challenges that remain in bridging the gap between clinical needs and academic research.
While there are areas for further research and clarification, the study stimulates meaningful discussion on the potential and limitations of clinical LLMs, paving the way for future advancements."
245,"SwiftMedSAM presents an optimized model for medical image segmentation, designed to perform efficiently in resource-constrained settings. Building on LiteMedSAM, it significantly reduces model size and computational complexity through hyperparameter optimization of the image encoder and mask decoder components. Validation results indicate that SwiftMedSAM offers a trade-off between accuracy and efficiency, achieving a validation score of 0.75. The detailed methodology and extensive experimental results support the reproducibility and robustness of the model. I recommend this paper for acceptance due to its less computational resources with full details for reproducibility."
246,"This paper considers bandit problems where the feedback over rewards is delayed (or evolving over time). 

The regret of their algorithms achieve depends on the time averaged (over both rounds and prior rounds’) $\ell_2$ distance between the feedback loss and the true loss.   The algorithms they analyze exponential weights in the full info setting, and a variant of FTRL adapted to this setting. While the analysis is reasonably straightforward, this reviewer agrees that the results are a nice way to generalize the general bandit feedback framework. As someone who is not an expert in results in related delayed or imperfect feedback settings, the reviewer will defer to others more familiar with the relevant literature to the novelty of these results.

Minor:

The reviewer would like a more detailed explanation of what is similar and different between this work and (van der Hoeven et al., 2023). 

The related work feels rushed and could be more thoughtful in its treatment."
247,"This paper addresses the task of hand pose estimation from depth imagery. The authors reformulate the 3D pose estimation problem as a dense ordinal regression problem and introduce Dense Ordinal Regression 3D Pose Network (DOR3D-Net) for 3D hand pose estimation. The DOR3D-Net simplifies the solution space to binary values, reducing noise and improving learning efficiency. It uses a transformer-based feature extractor and UV coordination maps for enhanced spatial information. The network has achieved state-of-the-art performance on the HANDS2017, MSRA, NYU, and ICVL datasets.

Pros:
- Improve upon previous state-of-the-art methods (Sec. 4.2)
- Thorough ablation (Sec 4.3), motivating the design choices
- Visualization of both successful and unsuccessful predictions

Cons: 
- The improvements are minor and could be considered within noise. Considering this, it would've been more appropriate to report an average of multiple seeded runs to make sure the improvements are not within error. This is exacerbated by the fact that the annotations of NYU/ICVL are noisy.
- The suitability of this paper in the POETS workshop is debatable. This paper address 3D hand pose estimation from depth imagery using standard benchmarking datasets. The POETS workshop webpage states that the workshop is about virtual humans for robotics and autonomous driving. None of these topics are addressed. Therefore I recommend resubmitting this paper to a pose workshop."
248,"The paper discusses machine learning for chronic kidney disease prediction. Multiple baselines are tested. The strength of the paper includes the detailed background introduction and problem-driven study, with thorough data analysis and experiment discussion. The code is made available online. The weakness of the paper includes too few baselines being tested, not standardized and lacking comparison to some foundation models (that are not specifically for chronic kidney disease but can be easily tailored to do so). Figure 1 is too small, especially with too small fonts. The Discussion section lacks more tables/figures/statistics to back up the sentences."
249,"This paper studies repeatedly Generalized Nash Equilibrium Problem (GNEP) a game theoretic setting where the constraints of each agent also depends on the actions of alternative agents. For such a game the authors design two algorithms that provably converge to a Nash equilibrium while not violating the constraints throughout the path. Such strong result cannot hold for harder constraint settings, e.g., online learning with adversarial time varying constraints. 

Technically introduce  anew classes of models which put additional assumption on the GNEP structure: the Strongly Benign GNEP (definition 2) an Benign GNEP(definition 3). The authors establish the existence of such structure under natural assumptions (e.g., smoothness and strong convexity of the loss functions).

Next, the authors provide the naive algorithm that updates the agent policies in a round-robin way and present their main algorithm, Alg. 1 Alg. 1, intuitively, improves over the naive approach since it allows for simultaneous updates of agent. The authors show Alg. 1 converges with rate of 1/\sqrt{T} to a Nash equilibrium while satisfying the constraints during the application of the algorithm (assuming all agents follow Alg. 1). 

Finally, the authors review the connection of their new result to existing art, and, especially, to online learning with time varying constraints -- which is a harder problem, in which satisfying the constraints and achieving \sqrt{T} rate is impossible.

Opinion.
I found the result interesting and novel. The fact Alg. 1 does not violate the dependent constraints while achieving \sqrt{T} convergence is surprising, to the best of my knowledge. Nevertheless, the setting the authors study is somewhat specialized (e.g., the assumptions the authors made, and the way the constraint set is constructed as an average of other agents' action). Additionally, the proofs slightly lacks clarity in my opinion (I did not follow the feasibility proof the authors supplied. See questions below). Would be happy to increase my score if the authors can provide clarifications for my questions below. 

Question.
1. Even though the theoretical appeal, is the constraints studied in this work are relevant for practical applications? Namely, when do we expect to have time varying constraints in form of the setting in Section 2.1? 
2. The benign angular condition is not properly explained in my opinion. In which cases we expect this condition to hold? In which cases it won't hold? Can the authors supply few examples for intuition? Possibly providing a discussion about it after Proposition 5 can be helpful.
3. In the proof of Theorem 6: how come x_t-1^{(i)}\in S_{t-1} implies the result? We would like to show feasability of all the agents' vectors. I would ask the authors to expend on the current proof as it seems to me incomplete.
4. Are there any variations of lemma 12 in literature? Additionally, is the angular condition was studied by prior works?
5. Lemma 12, if i understand correctly, is weaker than standard results when it comes to smooth and strongly convex functions due to the 1/\sqrt{T} term. Could you elaborate on this? What is the source of such additional term comparing to other existing results?

Comments.
1. The authors use nabla in places it would be more proper to use nabla_{x^{(i)}}, namely, when taking gradients only wrt to x^{(i)} (see, for example, definition of D_min^{(i)}(\phi)."
250,"The paper presents a study on the development of the CycleTrans model for predicting specific medications for patients based on their disease diagnoses. The model incorporates a cycle-embedding module to enhance symptom and drug embeddings, utilizes cross-attention and transformers to integrate patients' longitudinal data, and achieves high clinical precision and low drug-drug interaction (DDI) rate. The study also discusses the need for additional data, ethical concerns, and the unresolved issue of AI explainability in the medical field.

Pros:

The study introduces a novel model, CycleTrans, which addresses the need for precise medication recommendations based on patient diagnoses.

The incorporation of a cycle-embedding module and the use of cross-attention and transformers demonstrate a comprehensive approach to addressing the complexities of medication recommendation in clinical settings.

The model achieves high clinical precision and low DDI rate, indicating its potential for improving patient safety and treatment efficacy.

Cons:

The study acknowledges the need for additional data, particularly recent clinical domain data, to substantiate and validate the findings, indicating potential limitations in the current model's training and evaluation.

Ethical and moral concerns about AI-generated conclusions and the lack of clear AI explainability in the medical field remain unresolved, raising questions about the practical application of the model in real-world clinical settings.

The study does not provide a detailed discussion of potential biases or limitations in the model's predictions, which could impact its practical significance and real-world applicability.

Overall, the study presents a novel approach to medication recommendation in clinical settings, but it also highlights the need for further validation, consideration of ethical concerns, and addressing potential biases in the model's predictions. The work's significance lies in its potential to improve patient care and treatment outcomes, but its practical application may be limited by unresolved ethical and explainability concerns."
251,"**Paper Summary:**

This paper identifies the overfitting issue and the problem of simple datasets in blind IQA, possibly resulting from the isolation of the IQA model and data. In response, the paper presents a computational framework that integrates model-centric and data-centric IQA. This is achieved by enhancing the quality predictor with an auxiliary module to guide the sampling process. Its effectiveness is substantiated through a specific demonstration.

**Pros:**

1. Integrating model-centric and data-centric IQA is a promising approach and may shed light on future advancements in this field.

2. Recognizing the overfitting issue and the easy dataset problem can serve as valuable references for the community.

**Cons:**

1. A primary drawback of this work is its writing style, which may prove challenging for general readers unfamiliar with IQA. Ideally, the authors should initiate with the underlying rationales before delving into experimental details. The current presentation makes it challenging to grasp the nature of the easy dataset problem, and statements like ""the newly created ones are more difficult to challenge the most recent UNIQUE"" remain vague. Furthermore, certain background information and details, such as the outputs of an IQA model and the rationale behind the metrics chosen in Section 2, are essential for comprehending the paper's core content. The authors are strongly encouraged to refine their writing for broader accessibility.

2. Given that this paper's main technical contribution focuses on utilizing existing IQA models to inform the development of new IQA datasets, the phrase ""integrating model-centric and data-centric IQA"" might not accurately reflect the technical contribution of this work."
252,"Quality: The evaluation is comprehensive and convincing. The authors first chose the best evaluator agent using dataset from Omiye et al. Then the evaluator was used to assess candidate responses from 10 LLMs across 13 questions. The authors also studied different combinations of prompts, and showed GPT-4 with simple prompts is the best evaluator.

Clarity: The paper is well structured and easy to follow.

Significance: Race-based beliefs in healthcare can be harmful and it constructs a major concerns for doctors to apply LLMs in clinical settings. Conclusion from this paper is important to guide doctors to choose the best LLMs in practice."
253,"The authors consider the problem of studying and exploring the (chemical) latent space of a generative model. They aim to make connections of how one can explore the latent space of a (generative model) and dynamical systems. They use different approaches e.g. computing the gradient (gradient ascent or descent), learn a wave type parametrization of the potential (with the help of PINNS) a HJE and also use Langevin dynamics.

In my comments below I would mostly focus on the dynamic’s aspect of the work and less on the chemical/molecular aspect that is not my field.

I believe this is a sound paper and that the authors have done a lot of work.

The four-page limitation makes the paper difficult to follow therefore going back and forth between the main text and the appendix becomes necessary to get understand the overall frameworks. There are a few parts of the paper that I believe need improvement, I provide more details below. 

Major Comments:

(I)	In many expressions in the main text there is a superscript k e.g. (equations 3,5,6) I do not see where this is defined. Can you clarify/change this?

(II)	Can the authors mention why learning a vector field is better and not evaluating a network in a dense grid and choosing the minimum or maximum on the grid? What cases one can expect the one to be better than the other?

(III)	What happens if the maximum, minimum is close to the boundary? Are the trajectories going to “leave” the latent space and explode? Are there ways they can encode this information to their model?

(IV)	Why the Langevin dynamics might be better? Is it better regardless of the size of the noise? 

(V)	Even though I understand the limitations because of the four-page limit I think is important to improve the section “conclusions, limitations”. Now is more a reiteration of the overall paper and it does not mention what the conclusions and the limitations are.


Minor:

Abstract: (a) Check a little bit the English (e.g. that unified -> unifies?)

Introduction: (a) (Figure 1) The NN in the schematic of the decoder has as input two neurons and one (yellow) output, I think this should have been reversed right? (from less inputs to more outputs in the decoder). 

(b) Perhaps also a change is needed for the green box in the schematic for the predictor (go from the yellow to a function)?

(c ) The yellow color is not so visible.

Background: (Traversing Latent Space of Molecules) (a) The gradient of the potential energy function h is estimated in equation 1. Can you say how this is computed (automatic differentiation, symbolic differentiation, finite differences)? The same holds also for the case of the potential φ later (e.g. equation 3).

(b) The first time PINNs are mentioned is in page 2 perhaps move the citation for the paper there?

(c ) “Compared with traditional PDE solvers PINNs can be orders of magnitude faster”; can you clarify if this includes both the training time or only the inference time?

(d) Please clarify for each case you used PINNs if you solve the forward or inverse problem (fit the constant c)? 

(e ) Do you assume in this case you have some data for the potential or not? If not, how can you guarantee the uniqueness of the solution for the constant?

(f) Equation (6) does not mention a loss function for the Boundary Conditions, is this necessary or not in your case if not can you please explain why?


Supervised semantic potential guide (a) The input for the surrogate model $h_η$ lives in the high dimensional space is this correct? I thought we are working with the latent space of the generative model. 


(b) It is not clear to me, how equation 7 gives you the vector field z_t, can you add a few words in the SI and explain in more detail if you think is important for your overall scheme? The output seems to be just a scalar to me.

(c ) In high-dimensional space, a trivial solution exists such that all the flows learn the exact direction -> Can you justify that why this is the case, intuitively I would expect the opposite?

(d) Table (1) What is delta? Please explain>


Appendix : 
(a) Disentanglement Regularization: Since here there is no limit in the SI can you make things more clear? Why having the maximum Jacobian change is a problem? Why an auxiliary classifier is helpful? Perhaps cite or say a few words here of what is auxiliary classifiers.

(b) Check the text after equation 28, I think it might be wrong.

(c ) Algorithms: For the Langevin dynamics how β was selected. How sensitive the method might be to that value?  Can you mention include the selected parameters in your test cases?

(d) For algorithms 2, I don’t know if I missed it in the text or in the Appendix what sampling the timestamps and the potential functions means and why there are two random variables for that? 

(e ) Check Caption for Figure 4 in the SI.

(f) Figure 9, those are representative molecules, right since you have a generative model. Perhaps check the captions."
254,"The paper introduces a new framework for operator learning by separating the optimization of the basis coefficients from the basis functions in a pre-existing architecture, DeepONet. The authors first use an autoencoder to find the basis functions from the target space, which are then fixed to find the coefficients using the input functions, and show that this approach leads to the discovery of more expressive basis functions. The paper, for the most part, is clear - the idea is simple and is relevant to the field. A major weakness is that the autoencoding step to determine the basis functions requires knowledge of the target space at points (unlike DeepONet and POD-DeepONet which just uses locations to encode the basis functions). This limits its use for test examples where there is zero or limited access to the target space. There are also many typos in the paper."
255,"Advantages:
1. Gene pathogenicity prediction is a significant task, which can help us understand genetics and its impact on human health.
2. A benchmark study is an urgent need of this research community.

Disadvantages:
1. This paper focuses on three models: HyenaDNA, GenaLM, and Nucleotide Transformer. However, these models are relatively small.
2. This paper is far away from ""comprehensive"", which is important for benchmark study. There are few datasets, tasks and models for evaluation."
256,"This work consider N-armed stochastic bandit with subGaussian feedback, where the mean vectors are from a set of cardinality K. The goal is to reveal this unknown partition using a minimum budget. The authors propose a novel ACB algorithm, whose budget matches the lower bound in many regimes. Also, the authors find there is no computation-information gap in the bandit setting, as opposed to the batch setting.

Some comments and questions about the paper:
1. In Appendix C.2, the authors have shown that |S_M|=K w.p. at least 1-\delta, which is important for the algorithm to work. But this has not been mentioned in the main paper.
2. The Appendix is not well organized, for example, STEP1 to STEP4 span Appendix C.1 and C.2. Also, the authors can consider introducing the purpose of each part in Appendix at the beginning of the appendix.
3. Quoted from the abstract: ""we establish that there is no computation-information gap in the bandit setting"". But based on the introduction, the computation-information gap can come from whether repeated measurements are allowed, instead of ""bandit vs batch"".
4. Do the authors invent the estimator in Eq (9)? If not, please cite the corresponding reference.
5. Regarding Section 5, while a kmeans++ initialization is better than maximin, the maximin guarantees a 2-approximation to the global optimum of kmeans (Celebi et al., 2013). This property is important in establishing the theoretical results in Yang et al. (2024), but kmeans++ does not have this property.

After reading the rebuttal, I have adjusted the score accordingly."
257,"This paper focuses on runtime complexity of PAC learning with black box access to an ERM and introduces a new algorithm that would be more efficient if the ERM runs in super linear time in the number of samples. 

I think, to put the results in context it might be helpful to have some examples of hypothesis classes and discussion of when ERM would run in super linear time in terms of the number of samples. 

The results seem interesting and novel, however the writing and organization of the paper makes it difficult to understand.
- The algorithm is not described (in an algorithm environment) in the first 12 pages. I would suggest first giving a high level description of the algorithm, and then the subroutines used in it. 
- The notation and definitions are given on page 13, this might not be helpful for some readers.
- Section 2, which is currently most of the main text of the paper, is hard to follow. Perhaps a more high level intuitive description of previous work would be helpful. I also prefer to first read the proof sketch of the presented algorithm and insights/results from previous works could be mentioned when needed. Currently, the first parts are describing the previous works, and given that the algorithm has not been described yet, I'm not sure what to look for there. 
- There is some typos and some sentences are not clear. For example, the third sentence in ""Our approach"" paragraph."
258,"Summary
-- the paper applies LoRa fine-tuning of a Mistral model to perform TNM phenotyping using pathology reports. 

Pros
-- they perform data augmentation using 200 real world pathology reports. They synthesize new reports by stripping relevant sentences from existing reports and replacing them with example sentences that were mapped to certain label classes a priori
-- the inclusion of an UNKNOWN label class in cases where the relevant information was missing
-- the use of JSON-enforced output 
-- reporting of training time as well as performance
-- the ability of the model to cite the relevant information
-- the use of LoRa here is interesting

Cons
-- While the data augmentation method is creative, the methodology is not described clearly enough and the quality of the resulting data is not examined. For example, ""These sentences were then injected into the template reports at randomly selected marker locations"" -- does this mean that the data is being pulled from a finite list of sentences in the JSON? If so, this is clearly not sufficiently representative of the diversity of natural clinical language.
-- unclear how the path reports were labeled -- what exactly was being labeled, and what were the qualifications of the labelers?
-- how did you strip the report of all info relevant for TNM using a script? How did you validate the accuracy of this process?
-- unclear how many data examples were generated in total. Also not clear whether or not the resulting dataset was high quality. It sounds like you replaced the parts relevant to TNM with random TNM ratings to augment the dataset. Were the resulting pathology reports realistic? It's not obvious that this procedure would result in realistic pathology reports.
-- Some irrelevant text, eg. there is no Section 5
-- This is an interesting application of LoRa, but it's just one task. A more comprehensive evaluation across several tasks would be much more compelling. 
-- ""We also attempt to develop a more generalized approach, so that our work can be applied to other NLP tasks within the medical field"" -- it's not clear where this was done"
259,"**Summary**: The papers propose a method to optimize parameters of physics simulations using samples generated from a genetic optimization algorithm, which is then used to construct an FNO surrogate. The authors evaluate the performance of the FNO and propose further optimization that could be applied to this surrogate via gradient-based algorithms.


**Strengths**
1. Combining differential evolution as a precursor to surrogate models seems like an exciting direction. The authors evaluate their results on computationally intensive tasks such as hydrodynamic simulations.

**Areas of improvement**:

1. The idea of combining multiple techniques lacks clarity and motivation. The choice of CMA-DE and FNO does not seem convincing, as the manuscript lacks details about clear advantages compared with the existing state-of-the-art.
2. There is no evaluation of different sampling strategies or other optimization methods for surrogates, which may have provided better insights into their algorithm choices. There are no details about computational performance analysis.
3. For the CMA-DE and FNO - The CMA-DE method has previously been studied in the literature, but this work doesn't cite any previous publications and claims the technique as a novel [arabas2019, grady2023]. The role of FNO in the work is unclear, and the experiment details are missing.

[arabas2019] Arabas, Jarosław, and Dariusz Jagodziński. ""Toward a matrix-free covariance matrix adaptation evolution strategy."" IEEE Transactions on Evolutionary Computation 24.1 (2019): 84-98.

[grady2023] Grady, Thomas J., et al. ""Model-parallel Fourier neural operators as learned surrogates for large-scale parametric PDEs."" Computers & Geosciences (2023): 105402."
260,"## Pros:
1. This paper combined MedSAM and CNN-based transformer RepViT to enhance the performance and efficiency. It first distilled a RepViT 
image encoder from pretrained TinyViT image encoder, and then it fintuned the entire pipeline. It improves the efficiency from about 300 seconds to 170 seconds on the most complex 3D case (CT_0566). For the performance, it achieved the DSC of 0.8688 and NSD of 0.8746, outperforming the baseline and other models.

## Cons:
1. This paper doesn't apply any other tools to improve the performance of deployments (e.g. torch script, torch.jit, onnx, etc), which may further improve the efficiency.
2. Some minor problems in the paper writing (e.g. the phrases ""mandatory table""  in Table 1 should be deleted)."
261,"## Summary

In **""Light-weight Universal Medical Segmentation Network for Laptops Based on Knowledge Distillation,""** the authors propose a model distillation approach for optimising the applicability of foundation models for medical image segmentation on edge devices. Specifically, their approach consists of two steps. The first consists of replacing the image encoder from LiteMedSAM with Swim-T. Then, the entire pipeline (I assume it means the new image encoder in addition to the prompt encoder and mask decoder, but could you mention that explicitly?) is trained from scratch. In the second step, to optimise inference runtime, they distil the teacher (Swim-T) model's feature into a student model (RepViT-M0.6), a CNN-based model that allows for faster computation. They report that their approach is 3-fold faster than the challenge-provided baseline, while the performance is equivalent (if not improved). Overall, this is a very well-written and clear manuscript, and I have only a few minor comments/suggestions to improve the manuscript's **completeness** and **reproducibility**.    
## Detailed Comments
**Abstract:**  
1 – Could you report the DSC and NSD scores explicitly instead of relative to LiteMedSAM? Without knowing the performance of LiteMedSAM, ""improvements by 2%"" are meaningless.  
**Methods:**  
Overall, the methods are clear. However, there are some inconsistencies or missing information:  
2 – Pre-processing: The authors share some descriptive statistics of the training dataset from the challenge (and, hence, from MedSAM work), but they don't say that explicitly. Someone who is not part of the challenge might be unaware of what the author means by ""... statistical analysis on our dataset."" So, I assume that all the data used is solely from the training data provided by the challenge? That is unclear as the authors used the term ""external public datasets"" after ""our dataset."" I'm assuming Table 1 shows all the training data used for model training (minus 5% for validation). The authors also forgot to reference Table 9 in the text.  
3 – Proposed method-Teacher Model: There is a minor language error in ""... significant challenge for **reproduction**..."". I'd assume the author meant ""reproducibility.""  
4 – Proposed method-Teacher Model: The authors say, ""We replace MedSAM's original encoder..."" and continue to mention MedSAM in that paragraph, but in the rest of the manuscript, they report that LiteMedSAM's image encoder is replaced. Which one is the case?   
5 – Proposed method-Loss function: Here, there are other inconsistencies about the original model used (MedSAM or LiteMedSAM) and the loss function used for training the teacher model, where it is said ""... we train the entire pipeline from scratch using a combination of Dice loss and Focal loss"", but in table 3 it says: ""Dice Loss, Cross Entropy Loss, MSE Loss."" Which one is it? Could you perhaps provide a formula or links to PyTorch functions used?   
6 – Proposed method-Post-processing: You mention that after the logits are transformed to probabilities, ""the masks are then cropped to match the new size"". I would assume the output has the same 256x256 size as the resized and padded input image. Could you clarify that?  
**Experiments:**   
7 – Training protocols of LiteMedSAM with Swin-T image encoder: A key missing information is about how the bounding box was generated for model training.  
**Results and Discussion:**  
8 – Quantitative results on online validation set: In ""The LiteMedSAM (without knowledge distillation) ..."" I assume the authors are referring to LiteMedSAM with Swin-T. Could you rephrase that?  

Apart from these suggestions for improvement, the authors did a nice job in their proposed approach and manuscript. The authors met all the requirements from the checklist, but in the checklist table, the references to tables with environment settings and training protocols need to be corrected.  

As a final note, the authors’ GitHub repository looks well organised, but I haven’t tried to run the code myself."
262,"- Myocardial injury after non-cardiac surgery (MINS) is a postoperative complication presented in diverse profile patients and marked by varied symptoms. Thus, its early prediction is a challenge and ML approaches, even heavily utilized, present some difficulties in processing unstructured and imbalanced data.
- Authors propose a framework based on the one presented in https://arxiv.org/pdf/2306.02052.pdf (RBF), called Retrieval Based Disease (RBD) Prediction framework. Their approach transforms multifaceted pre-operative and intra-operative tabular data into coherent text-based descriptions, enabling the use of Language Models (LM) for data interpretation.
- The authors face an imbalanced dataset of MINS, with only 7.9% of the total patients experiencing MINS. They compare their framework with a RandomForest, a XGBoost, a BERT-base and a SciBERT, ClinicalBERT and BioBERT fine-tuned for binary classification. They show that their RBD framework outperformed the ML approaches and the LM ones. Even with an ablation study, RBD-t showed better performance than the others (but the original RBD).

The presented framework is a variation of a pre-existing one (RBF) and combines it with other Languange Model for Biomedical domain (PubMedBERT). However, the methods seem to be properly adapted and a good choice to the problem to be tackled, with the corresponding citation. The framework is correctly presented in Figure 2 and the results of the few experiments presented clearly in Tables 1 and 2. 

The presented results show a good improvement compared with the other baselines, which seem to be diverse and relevant enough; first, two widely used ML techniques and then 4 domain specific language models. Even the ablated version of the framework (RBD-t) overperforms the other baselines, despite performing worse than the not ablated RBD. 

Even though it uses pre-existing methods, the need of adaption of it to imbalanced datasets is essential and the significance of having early detection techniques without the need of post-operative data is valuable. 

Regarding to format and clarity, the work is well presented and clearly structured, having not found major errors in the writing. The trivial next steps are presented as future work, with the aim of making the framework more general by extending it to multi-label datasets. 

COMMENT:

- If I am not wrong, the only innovation here is the adaptation of RBF to medical data by using a specific BERT model for biomedical NLP tasks, named PubMedBERT. Right?
- What is the explanation of K=3 showing the best results?"
263,"1. Summary and contributions: Briefly summarize the paper and its contributions
Introduces a new automated evaluation framework for assessing the conversational ability of LLMs to diagnose medical conditions. The framework uses a patient AI agent (also an LLM) to simulate a doctor-patient interaction. The conclusion of the interaction (the diagnosis) is then evaluated by both a grader LLM and a medical expert. 4 conversational setups are tested using 140 skin disease vignettes. The author claims this shows the shortcomings of LLMs in gathering patient histories, synthesize information over dialogues, and clinical reasoning for diagnosis without answer choices

2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance to the community.
- The development of robust automated evaluation of LLMs is vital for the safe adoption of this technology.
- Increasing work is being done on conversational agents and as is correctly stated in the paper this strategy significantly enhances the scalability of evaluations and allows for broader and quicker testing. Another strength not mentioned over human evaluation is the ease of reproduction.

3. Weaknesses: Explain the limitations of this work along the same axes as above.
- No human baseline is provided and so it is difficult to gauge the level of performance shown in Table 1. It is fair to assume that the drop-off in model performance seen with increasing task complexity, e.g. mcq -> frq, clinical vignette -> multi-turn, would also happen for humans. - The extent of this is unclear though.
- The evaluation dataset is currently limited to 140 cases only focused on skin diseases. Furthermore, an unknown proportion are sourced from the internet. As LLM are trained on large-scale internet crawls, there is a high possibility of test set leakage. 

4. Correctness: Are the claims and method correct? Is the empirical methodology correct?
- As an initial paper establishing the framework, this paper needs to first establish the method as valid before making claims establishing the undoubted shortcomings of LLMs. For example, an underlying assumption is that the grader AI agent is accurate and unbiased. Previous LLM evaluation work has shown this not to be true, for example GPT-4 models favouring GPT-4 outputs. A similar assumption holds that the patient AI agent answers accurately and, for example, never hallucinates. 
- The validity of the method could be shown by running the same framework swapping out the AI agents for real human and showing the metrics are unchanged. This is suggested by the medical expert box in Fig 1, but no mention of these results is made in the main text.

5. Clarity: Is the paper well written?
Text is clear and well-written
- The icons in Fig 1 make it unclear which parts of the framework are automated, and which are human. For example, the clinical LLM has a human icon, and the grader-AI and patient-AI have different icons.
- It is not clear to me if the patient-ai and grader-ai agents are fixed models (and if so, which models) or change with the clinical LLM model being tested. Both could lead to differing methodological issues. By fixing the models, the framework may favour models of the same “type” but by changing the evaluating models metric is fundamentally changed. 

6. Relation to prior work: Is it clearly discussed how this work differs from previous contributions?
- A thorough, succinct review of the need for medical innovation, LLM development and shortcomings in automated medical evaluation is given. The introduction could be explicit about the pros and cons between human and automated evaluation.
- The recent work “Towards Conversational Diagnostic AI” would be highly relevant to mention. But an acceptable miss as it was published very close to the submission deadline.

7. Reproducibility: Are there enough details to reproduce the major results of this work?
- The link to framework’s code is provided for reproducibility. 
- It is mentioned that 10 simulations are run. I assume this is done via sampling (e.g. through temperature) but not mentioned what these hyperparameter settings are"
264,"- Fig. 2. Please add more details on the network modules to caption, e.g., W-MSA, SW-MSA
- The authors mentioned that ""we opt for a tiny Swin Transformer [6] as our image encoder, which is designed to handle large images
more efficiently compared to ViT in terms of both computation and memory consumption."". However, in Table 8, the proposed method is slower than the ViT encoder for some modalities. Please explain the potential reason. 
- It would be great to try OpenVINO to speed up the model inference."
265,"The manuscript proposes a retrieval-augmented LLM approach for text-based cardiovascular disease detection. 
Overall, the proposed approach is reasonable. However, there exists a number of unclear aspects that need to be addressed before the manuscript to be published.

**Strengthese**
- cardiovascular disease detection is one of the important risk prediction scenarios for clinical foundation models.
- the proposed retrieval augmentation is a useful technical to enhance foundation models

**Weaknesses**
- Heart disease dataset
  - since it is an author-collected dataset, it is better to mention the size of training and testing set
  - in the real clinical setting, there can exist healthy patients. However, the dataset does not contain the label for the healthy condition.
- Technical part
  - how do the authors implement the re-ranker? Now there is no explanation for that.
  - how to derive ``<RAGHere>`` from ``A``? In the introduction of the case fusion layer, the authors stop after introducing their cross-attention operator. There still exists a gap between the cross-attention and ``<RAGHere>``
  - is there a particular reason to use the same $W_K$ for both $K=W_K A$ and $V=W_K A$?
- Experiment setup
  - what is the RAG model? there is no reference for it. If it is a custom baseline, it would be better to introduce it.
  - ""During training, we randomly mask $m$ cases."" $m$ is first introduced here. This introduction of a new variable without prior explanation can pose challenges for understanding the method effectively.

**Questions**
- Can the authors explain why the retrieved cases are still from the training data? Assuming the foundation model is well-trained, then it does not need 
- what do ``standard values`` refer to (Results section)? Also, there is a typo for ``w/o stanard`` in Tabl 2. Is the ``standard value`` similar to the concept of ``standardization of units`` mentioned in the Heart disease dataset section? I can guess it may refer to the standard value range of a vital, but it is better to explicitly introduce it."
266,"1. The first author's ORCID is missing. 
2. The equations on page 6,7 are not numbered. 
3. It seems that the strategies to improve inference speed on the CPU have not been detailed. 
4. The code link is not provided."
267,"Comparative learning is a recently introduced variation of the PAC-framework.
In comparative learning, the labeling is assumed to be realizable by one
hypothesis class (the source class $S$), while the learner's performance
is measured against the best hypothesis of another class (the benchmark
class $B$). It had been shown by Hu and Peale that the comparative
learnability of $(S,B)$ can be characterized by the so-called mutual
VC-dimension of $(S,B)$. The latter is denoted by $\vc(S,B)$ and
the sample complexity grows linearly with this parameter. In this submission,
the authors consider benchmark-proper learners (whose final hypothesis
must be taken from $B$) or, even more restrictive, benchmark-ERM learners
(whose final hypothesis is taken from $B$ and must be one of the hypotheses
with minimum empirical error). The main results are as follows:

1.
A new combinatorial parameter $d_G^\rightarrow(S,B)$, called the one-sided mutual
graph dimension of $S$ and $B$, is introduced. It is shown that this
parameter characterizes the learnability with benchmark-ERM
learners. The sample complexity grows linearly with $d_G^\rightarrow(S,B)$.
It furthermore grows at least linearly, and at most quadratically,
with $1/\varepsilon$ where $\varepsilon$, as usual, denotes the accuracy parameter.

2.
The sample size required for comparative learning is denoted in the paper
by $n_{S,B}^{gen}(\varepsilon,\delta)$, respectively by $n_{S,B}^{prop}(\varepsilon,\delta)$
or by $n_{S,B}^{ERM}(\varepsilon,\delta)$ if the learner is assumed to be
benchmark-proper or benchmark-ERM, respectively. Clearly
\[
\vc(S,B) \le d_G^\rightarrow(S,B)\ \mbox{ and }\
n_{S,B}^{gen}(\varepsilon,\delta) \le n_{S,B}^{prop}(\varepsilon,\delta) \le
n_{S,B}^{ERM}(\varepsilon,\delta) \enspace .
\]
The authors present an example which demonstrates that the gap between
the parameters $\vc(S,B)$ and $d_G^\rightarrow(S,B)$ can be arbitrarily large.
An analogous remark applies to the
parameters $n_{S,B}^{gen}$, $n_{S,B}^{prop}$ and $ n_{S,B}^{ERM}$.
This implies that there exists a pair $(S,B)$ that is comparatively learnable
but not by a benchmark-proper learner. And there exists a pair $(S,B)$
that is comparatively learnable by a benchmark-proper learner but not
by means of benchmark-ERM.

3.
The authors examine more closely how the sample complexity may depend on $\varepsilon$.
They remind the reader of special extreme forms of comparative learning
(like, for instance,  classical PAC-learning in the realizable case
or properly learning a known true labeling) which demonstrate that both,
linear and quadratic dependence, may actually happen. They furthermore
present a variety of bounds on the sample complexity (in terms of  
dimensions different from $d_G^\rightarrow$) which shed some light on the
question under which conditions we may expect a quadratic, respectively
sub-quadratic, dependence on $1/\varepsilon$. 

4.
In the appendix, the authors introduce the mutual graph dimension $d_G$.
It is shown that, if $d:= d_G(S,B) <\infty$, then $(S,B)$ can be
comparatively learned using either source-ERM or benchmark-ERM.
Moreover the sample complexity of these learners is upper bounded
by $O\left(\frac{1}{\varepsilon^2}(d + \log(1/\delta))\right)$.


The authors make a reasonable contribution within the relatively new
framework of comparative learning. They extend the analysis of general
comparative learners in (Hu and Peale, 2023) by an analysis of
benchmark-proper and benchmark-ERM learners. As far as I can see, the
results and the proofs in the paper are correct. 

On the other hand, the proofs in the paper are relatively easy to obtain.
Most of them are close relatives of existing proofs in the standard
PAC-learning framework. The proof of Lemma 6 looks more complicated but,
I think, it can be considerably simplified. See the technical comments
below.

Summary:
This is a nice contribution to a relatively new learning framework
but, as far as I can see, there were no major technical hurdles to
overcome.

\centerline{\bf Technical Comments}

Definition 1:
$n(\varepsilon,\delta)$ should be replaced by $n_{S,B}(\varepsilon,\delta)$.

Proof of Observation 1:
It should be said explicitly that $X$ (which is not specified further
in the proof) is an infinite set.

Theorem 4:
$\tilde O$ should be replaced by $O$ in this Theorem. Compare with Lemma 6.

Proof of Theorem 15:
In the proof, you run three learners. Wouldn't it be simpler to run the
learner whose associated combinatorial parameter is the smallest one
(Learner 1 if $d = \vc(S)$, Learner 2 if $d = \diam(B)$,
Learner 3 if $d = \diam(A_{S,B})$)?

Proof of Lemma 6:
I think, the proof can be given in a very simple manner.
The inequality (1) in Lemma 27 is verified in the same way as it is
done in (Anthony and Bartlett, 2002). For $b \in B$, define
\[
R(b) = \{(S,T): |L_S(b) - L_T(b)| > \varepsilon/2\}
\]
so that $R(B) = \cup_{b \in B}R(b)$. For each $b \in B$, the
term $\Pr_\sigma[\sigma(z) \in R(b)]$ can be bounded from above by means
of Hoeffding's inequality (as it is done in (Anthony and Bartlett, 2002)).
In order to bound $\Pr_\sigma[\sigma(z) \in R(B)]$ from above, one plans
to apply the union bound. A naive application would introduce the
factor $|B|$. The crucial observation is that $R(b)$ depends on $b$
only weakly:

a)
Let $z = (z_1,\ldots,z_{2m})$ and $z_i = (x_i,y_i)$. With each $b \in B$
associate the zero-one loss function
\[
h_b(z_i) = \left\{ \begin{array}{ll}
   0 & \mbox{if $b(x_i) = y_i$} \\
   1 & \mbox{if $b(x_i) \neq y_i$}
           \end{array} \right. \enspace .
\]

b)
Observe that $h_b = h_{b'}$ implies that $R(b) = R(b')$.

Thus, an application of the union bound yields only
factor $s := |\{h_b: b \in B\}|$ instead of factor $|B|$.
Let $d := d_G^\rightarrow(S,B)$ and note that the VC-dimension of
the class $\{h_b: b \in B\}$ cannot exceed $d$. An application
of Sauer's bound yields $s \le \left(\frac{2em}{d}\right)^d$.
Now the proof can be completed as in (Anthony and Bartlett, 2002). \\
Actually the whole proof is almost the same as in (Anthony and Bartlett, 2002).
The only thing is that one has to bring the hypothesis
class $\{h_b: b \in B\}$ into play and one has to observe
that $\vc(\{h_b: b \in B\}) \le d_G^\rightarrow(S,B)$. \\
Note that the functions $h_b$ are 0,1-valued. There is no need to make
a detour on partially defined hypothesis classes. Lemma 28 is
therefore not needed here.

Theorem 32:
Does the logical ``or'' in ``benchmark-properly or source-properly''
express what you mean? I guess, you mean that both forms of proper
comparative learning have a sample complexity upper bound as given
in the theorem.

Proof of Theorem 32:
Which hypothesis do you denote by $\bar a_{s,b}$ in this proof.
Do you mean $a_{s,b}$? \\
Replace $((X \times Y \times ) \{0\})^m$ by $((X \times Y) \times \{0\})^m$.

Proof of Corollary 33:
Shouldn't the expression $\frac{1}{n} \sum_{i=1}^{n} 1[a_{s,b}(x,y) \neq 1]$
represent the empirical error of $a_{s,b}$? Then this expression should look
like $\frac{1}{n} \sum_{i=1}^{n} 1[a_{s,b}(x_i,y_i) \neq 0]$.


General Remark:
In several definitions of sets in the paper, one finds unnecessary occurencies
of ``$\forall$''. For instance in Lemma 31 (but not only there),
the set $(U,g(U))$ is defined as $\{(x,g(x)): \forall x \in U\}$.
The symbol ``$\forall$'' should be removed. Please check the whole paper
for meaningless occurencies of ``$\forall$''."
268,"## Summary
The paper provides an introspective exploration of the potential for machines to exhibit scientific creativity, comparing and contrasting machine and human capabilities. 
It highlights machine advantages such as unbounded effort, immunity to bias, and lack of emotional interference while recognizing human strengths like collaboration, embodied intelligence, and motivation. 

The paper proposes a balanced approach to developing creative AI, urging us to consider not just what can be built, but what should be built. Through philosophical, technical, and ethical discussions, it challenges existing benchmarks and notions of creativity while envisioning a complementary role for AI in advancing human knowledge.

## Strengths
1. The writing style is engaging, using compelling analogies and historical examples like Henri Poincaré’s ""bus moment"".

2. The paper takes a comprehensive approach, combining insights from psychology, philosophy, neuroscience, and AI research. It discusses ""Big-C"" and ""little-c"" creativity.

3. The article offers a balanced examination of machine and human capabilities. It highlights areas where machines can surpass humans (e.g., lack of emotional attachment) while acknowledging the importance of human traits like collaboration.

4. The paper emphasizes the importance of creating AI systems that complement rather than replicate human intelligence, providing a clear and meaningful purpose for AI in scientific discovery."
269,"The authors propose to use ECG examination records (text) to classify heart defects. This is in contrast to the more ""classical"" approach of using directly ECG data.
To my understanding, at inference time this method would rely on a doctor first describing the ECG to produce the ECG examination record which would then be used as input to the model. I wonder if this is something that limits its applicability. In the introduction the authors note that there is a lack of cardiologists, but this method would not alleviate that issue.
The authors first apply a classic few-shot strategy using a llama2 variant that was pre-trained on a curated version of a public dataset of ECG notes. They note a low performance with this strategy.
Then they finetune the model on their dataset (1006 cases) and observe a marked improvement. Notably, there are no descriptions of whether they split the data in training and validation or of any cross-validation strategies. Additionally, pre-training and finetuning are done for very few epochs 10 and 15 respectively.
Then the authors use RAG to include context from similar cases to aid in the prediction. They can only include 1 or 2 retrieved samples before running out of tokens with a standard RAG strategy. The RAG strategies improve the performance. But I have several issues here. i) The knowledgebase for RAG is the same training cohort. This seems a major issue. ii) If the retrieved context includes a diagnosis, the model may just use the retrieved diagnosis. It may be beneficial to investigate this potential issue.
To include more context, they propose a context fusion model based on cross-attention. Then the entire context coming from the RAG portion boils down to a single vector if I understood correctly. This seems to improve performance further.
In general I would have liked to see baselines of performance using ECG data to compare if there could be benefits of the LLM strategies based on ECG reports.
I was confused about the ""standard values"" I could not find a description of them. They are mentioned only once in the text but they appear in the results table.
It would have been good to see the number of cases in each type of case: single, double, triple."
270,"**Summary:**  
   
The paper introduces a method for learning representations that are applicable to both discriminative and generative tasks, employing a closed-loop transaction framework. Building on the groundwork laid by [2] (referenced as [16] in this paper), which proposes a rate reduction objective to quantify the disparity between encoded representations, this paper identifies a limitation in this approach - the absence of sample-wise self-consistency. This concern is addressed through the introduction of a specialized rate reduction loss. Additionally, the authors employ another loss term to adapt the CTRL framework for unsupervised settings.

**Pros:** 

- The proposed method demonstrates versatility, being applicable to both discriminative tasks and image generation, including unsupervised conditional image generation.
- The implementation of self-supervision, requiring that the sample and its augmentations are closely situated in the encoded space as measured by the rate reduction, is a straightforward yet interesting concept.

**Cons:**

- The proposed modifications appear to be largely incremental in comparison to the work in [2], primarily revolving around the incorporation of specialized losses to influence the encoded space. Notably, such losses have previously been discussed in the literature (see [1] in the References below for self-consistency, and „Detailed” notes).
- The experiments primarily focus on the CIFAR-10/100 and Tiny-ImageNet datasets, which I perceive as a somewhat limited evaluation. This is particularly notable given that CIFAR-100 shares a largely similar semantic structure with CIFAR-10. It would be preferable to see a more extensive evaluation on additional datasets. Specifically, it would be valuable to assess whether the method scales effectively for larger datasets or images.

**Detailed:** 

My main concern with this work is that it appears to represent a modest incremental advancement over existing approaches, particularly [1] and [2]. Both of these works consider rate reduction for representation learning, with [1] even placing particular emphasis on the self-consistency of the representation. Consequently, I find the contribution of this paper to be somewhat constrained.

**References:**     

[1] Ma, Yi, Doris Tsao, and Heung-Yeung Shum. ""On the principles of parsimony and self-consistency for the emergence of intelligence."" Frontiers of Information Technology & Electronic Engineering 23.9 (2022): 1298-1323.    
[2] Dai, X., Tong, S., Li, M., Wu, Z., Psenka, M., Chan, K. H. R., ... & Ma, Y. (2022). Ctrl: Closed-loop transcription to an ldr via minimaxing rate reduction. Entropy, 24(4), 456."
271,"Summary

This paper presents a (APPS) method to capture the potential parameters for differential equations from trajectory data. This paper proposes a new active learning method integrated into ODE discovery. This new active learning method incorporates an active path discovery based on more informative data assessment.


Novelty

The method in this paper is very similar to (d’Ascoli, 2024). They have similar pipeline that is made of transformer. (d’Ascoli, 2024) is to read point data as transformer input, by contrary this APPS method is to set mathematical rules as transformer input. The APPS method employs predefined grammar generation rules, by contrary the (d’Ascoli, 2024) is to employ pretrained model. 

This paper claims that APPS method is more accurate. 

We could say that the (d’Ascoli, 2024) is a global learning, but the APPS method is based on the ranking of local informativeness.

Is this informativeness rating method scalable ? Considering, if there is a large curve, then its local trajectory pattern may be a straight line.


Issue:

1. Some very important explanations and paragraphs are in appendix, particularly the main algorithm is also put into appendix. The main paper is not self-explained without appendix. 


2. Although this paper highlighted a new solution to address the “initial condition” dynamic issue, the experiment regarding the improvement on “initial condition” issue are not shown in report.


3. It is necessary to significantly reorganize all the paragraphs for 8-page limit.


Typos:

page 4, figure 2(a), “Categorial distribution” → “Categorical distribution”

page 12, Om = {+, −, ×} → Om = {+, −, ×, /}"
272,"The paper introduces EMT, a novel method for evaluating catastrophic forgetting in multimodal large language models (MLLMs) by treating them as image classifiers, revealing that while early-stage fine-tuning of MLLMs can enhance performance across various image datasets by aligning text and language features, prolonged fine-tuning tends to induce hallucinations in the models, ultimately limiting their generalizability and indicating room for improvement in current MLLM fine-tuning methodologies.

Pros:
1. The EMT method is an innovative approach to assess catastrophic forgetting in MLLMs, providing a different lens by evaluating them as image classifiers
2. The paper provides a interesting and crucial insight of fine-tuning: early-stage fine-tuning appears to enhance performance across various image datasets, while later fine-tuning can make MLLMs  hallucinate, diminishing their generalizability even when the image encoder is frozen.
3. The paper is well-written and nicely presented. The experiments are thorough and extensive. This paper opens avenues for subsequent research.

Cons：
1. While EMT offers valuable insights, its applicability and transferability to varied multimodal tasks beyond image classification (segmentation/detection) may need further validation. 
2. While the paper expertly identifies and diagnoses issues related to catastrophic forgetting and hallucination in MLLMs, it may lack in offering concrete, actionable solutions or mitigation strategies to address these identified issues.
3. The phenomenon of models beginning to ""hallucinate"" during fine-tuning is deeply intriguing and could be elaborated further. The paper might benefit from a more in-depth analysis, exploring why and how these hallucinations occur and the intrinsic factors within the MLLMs that contribute to this issue."
273,"## General comments
The manuscript titled 'Approximating Family of Steep Traveling Wave Solutions to
Fisher’s Equation with PINNs' presents an innovative approach using Physics-
Informed Neural Networks to model steep traveling wave solutions in Fisher’s
equation. The proposed method, including a novel residual weighting technique
and a specialized network architecture, demonstrates improved accuracy in
approximating sharp solution profiles.
The findings are promising. I therefore recommend that the paper be published but
after the authors address the comments presented below, which are mostly minor.

## Comments
1. Here the efficacy of the proposed method is compared with traditional PINN
only. There are multiple variants of PINN [1,2,3,4, etc.]. The authors are
encouraged to spend a few words on better contextualizing their work
within the broader context of the current body of literature.
2. The values of the reaction rate coefficients are written as 1.000, 10.000. I am
not sure if they mean 1 and 10. However, if the authors mean 1,000 or 10,000,
those should be revised.
3. “Optimization is carried out with Adam for 50k training epochs for the
discrete-ρ approximation and for 100k epochs when applying the
generalizing architecture. A default learning rate of 0.001 is applied.”
Authors should clarify if these are true for both PINN and wave-PINN ?
4. The authors are advised to add a brief discussion on the limitations of the
method and the NN architecture.

## References:
[1] L. D. McClenny and Ulisses Braga-Neto, “Self-adaptive physics-informed neural
networks,” Journal of Computational Physics, vol. 474, pp. 111722–111722, Feb. 2023.
[2] U. Braga-Neto, “Characteristics-Informed Neural Networks for Forward and Inverse Hyperbolic
Problems,” arXiv.org, 2022.
[3] R. Mojgani, M. Balajewicz, and P. Hassanzadeh, Lagrangian pinns: A causality-conforming
solution to failure modes of physics-informed neural networks, arXiv preprint arXiv:2205.02902,
2022.
[4] A. D. Jagtap and G. E. Karniadakis, Extended physics-informed neural networks (xpinns): A
generalized space-time domain decomposition based deep learning framework for nonlinear partial
differential equations, Communications in Computational Physics, vol. 28, no. 5, pp. 20022041, 2020."
274,"The paper focuses on learning with surrogate losses. More precisely, they revisit the H-consistency setting, and prove more refined results where the bounds now can depend on the choice of the particular hypothesis. They instantiate their results on three different problems: binary and multi-class classification, and bipartite ranking. 

To start with, I am not an expert on learning with surrogate losses, hence some of the concepts presented in the paper were new to me. However, the paper is very-well written and easy to follow, and it is quite clear what the improvements that the authors brought to the current literature. By looking at the proofs, it is not obvious to me if the authors developed new proof techniques (it seemed to me they used quite standard tools), yet I think the results they proved seem significant. In the classification cases they refine existing results, whereas (I think it is the more interesting part) in the bipartite ranking problem their results provide more theoretical support for explaining existing empirical observations.

In general, the paper has clear improvements over the prior work, it is well-written, and sheds more light to certain empirical observations. Hence, given these explanations and the fact that I am not an expert in this field, I recommend an accept with a low confidence score."
275,"This paper presents heuristics for improved modeling of Lagrangian fluid dynamics which takes a particle-based view of the dynamics. Recent works have used graph neural networks (GNNs) to model such dynamics. This work identifies the limitations and failure modes of these models in long-term rollouts, and proposes three corrections to these models. 

### Strengths

- The paper is very well written, especially when read as a whole (including appendix). Authors provide sufficient background on Smoothed particle hydrodynamics and the related neural models for the reader to understand their contribution.
- The heuristics are well motivated and appear to improve the performance of both GNS and SEGNN models. Appendix G provides intuitions and support for the proposed heuristics. (Suggestion: Move some of the discussion in Appendix D, E, and G to the main text to better contextualize the corrections.)

### Weaknesses

- The empirical evaluation is limited. Since the proposed corrections are heuristic in nature, they need to be tested more rigorously on larger and more diverse datasets and on additional base models. (The authors have also acknowledged this limitation)"
276,"The proposed estimator effectively integrates boundary conditions without resorting to penalty terms, surpassing the efficiency of alternative methods like PINNs, the Deep Ritz method, and diffusion loss. The paper introduces innovative loss functions for neural networks, leveraging the Green function and recursive solution of Poisson equations within domain spheres, eliminating the need for computing spatial gradients in the loss calculation. The detailed implementation information provided in the appendix offers valuable guidance for readers. This paper demonstrates remarkable technical and mathematical rigor, presenting novel approaches and fully laying on the community's interest. I strongly recommend accepting this paper."
277,"The work addresses an important problem - the current lack of a meaningful comparison of methods for application of LLMs to process unstructured data for clinical survival analysis. It is very well-written, organized, and enjoyable to read. I am not well-versed in this subfield, but it appears to be reasonably comprehensive. It provides a framework for addressing the issues brought up in the review.

There is also room for improvement. The descriptions of many works lack details and are hard to draw conclusions from. At several points recommendations are given, but they are not very clear or specific. If the point is to highlight that prior work does not support clear recommendations, it would be clearer to state this, as the current structure makes it appear that the intent is for the authors to give methodological recommendations based on the reviewed work. A large part of the contribution is a common evaluation framework, which addresses the gaps in the field highlighted by the review; however, this is largely relegated to the appendix, not described in much detail, and not justified clearly in its design choices. I cannot view the current state of the project as the link is hidden for anonymity.

Also, I would point out that the strategy used for literature search seems brittle. For example, the search includes titles with either ""survival analysis"" or ""time-to-event"" AND ""medicine"" or ""healthcare"". Ironically, this submission itself meets neither of these criteria. This suggests both the need for a more robust search strategy and possibly the need for a more descriptive title for this submission. It would also be helpful if it was clearer from the title that this is a review. 

Questions and suggestions for the authors:
1. In section ""Fine-tuning: Adjusting LLMs for the task - Limitations"": are there any conclusions to be drawn from this? The findings seem contradictory.
2. Prompting: Querying in natural language - Strengths. Can you elaborate? The main point given is that these methods are the most novel, which is not a strength per se.
3. I found the following statement confusing: ""However, multiple risk scores are rarely evaluated due to the prohibitive cost of extracting the required covariates x_i, which are often present in patients’ unstructured health records."" What are the multiple risk scores being referenced here?
4. You encourage researchers to ""compare LLMs strategies on private sources using our implementation."" Can you clarify how this works? Is the data private, and if so, how do other researchers use and present it?
5. What is the current state of the GitHub? Can it be used yet? Can you provide any results for the methods that are implemented so far? I also understand that the intent is partially to use this conference to gather feedback for its improvement.

With some clarification on the nature of the provided evaluation framework, I think the paper meets the threshold for acceptance in its current state, but I hope the authors will take this feedback into account, as it could be a stronger paper without too much effort."
278,"### Summary

This paper proposes a new strategy to solve “medical event prediction”. They motivate the problem, describe its main challenges and propose a solution that addresses these challenges. The proposed method consists of two main steps - (i) medical concept memorization - to adapt the vocabulary of the LLM, and (ii) contrastive learning to capture inter and intra-visit relations in medical events. This method achieves SOTA performance on MIMIC-III and MIMIC-IV datasets.

### Strengths:
- Provides a clear motivation for the problem and outlines the main challenges in solving the problem.
- Novel and effective LLM pre-training tasks are proposed to directly address the above challenges
- A diverse set of baselines are considered is evaluating the proposed method

### Weaknesses:
- No ablation study in experiments to understand which of the pretraining tasks leads to performance gains.
- Does not consider any Clinical LMs as a base model in experimentation. Clinical LMs like BioGPT, MedPaLM may already understand medical vocabulary. This may remove the need for the “memorize” phase in the proposed method.
- Poor readability in certain sections. Eg: explanation of proposed method can be improved by correcting grammatical errors and adding equations/figures.    

### Other Feedback:
- Other baselines also evaluate on MIMIC-III and MIMIC-IV. Consider releasing your train/test split and characteristics of the dataset in your next iteration of this work. This could help standardize research in the “medical event prediction” field.
- Since your method is built on top of a generative LM, it could be inherently better than other methods at quickly (with limited data) understanding new medical events. It would be interesting to see experiments on predicting medical events in a few-shot/zero-shot setting.
- The authors noted that some other methods modeled external information as knowledge graphs. There are some advantages to these methods (eg: updating information) and those should be further explored in future work. For example, authors could add pre-training tasks using the knowledge graphs, similar to the way the ontology of medical codes is currently used."
279,"This paper introduces a novel parameter-free online gradient boosting (OGB) algorithm for adversarial nonparametric regression with convex losses. By leveraging chaining trees and adaptive pruning, the algorithm achieves minimax optimal regret, adapting to local Lipschitz patterns. The approach is theoretically sound and innovative, expanding classical boosting to an adversarial online setting with computational efficiency. However, the paper’s dense theoretical presentation may limit accessibility, and the experimental validation is confined to an appendix. To improve the paper, consider simplifying the mathematical explanations, consolidating related work, and expanding on empirical results and real-world applications in the main text. Adding visual aids and a brief practical discussion would further enhance readability and impact.

The OGB method proposed in this paper is targeting optimal minimax regret performance with a focus on local adaptivity. The contribution is excellent and technically novel. Also, leveraging the concept of chaining trees and pruning based on local Lipschitz profiles is both innovative and well-grounded in nonparametric regression theory. To sum up, it is a theoretically sound, novel theoretical work."
280,"This work is focused on Verlet flows and parameterization of the coefficients in both the state space and the augmenting space. The theory is not new however the implementation for CNFs can be interesting. It would be interesting to see, what is the optimisation and design approach if the authors claim that it's better than parametrization against neural network. It would be wise to present evidence of training and benchmark comparison."
281,"Pros:
- Well-written, clear description of the strategy and methodology used.
- Figures support the understanding of the approach clearly.
- Efficient pipeline compiled in C++.

Missing information:
- Distillation details regarding the prompt encoder and mask decoder are not given.

Problems:
- The text font in figures is not Times New Roman.
- Typo in section 2.4: Further instead of Futher.
- Missing vertical lines for tables in the **Experiments** section."
282,"### General Assessment
The manuscript titled “Modality-Specific Strategies for Medical Image Segmentation using Lightweight SAM Architectures” presents a comparative study on deploying medical image segmentation models optimized for CPU computation. The authors have made a commendable effort to improve upon existing deep learning models for medical image segmentation; however, there are several areas that require attention to enhance the quality and credibility of the research.

### Specific Concerns and Recommendations

#### Figures and Tables
- **Figure Inclusion**: The checklist mentions ""Figure 2"" and ""Figure 3,"" which are referenced in the text but not included in the provided manuscript. The final submission must ensure all referenced figures are present to support the discussed results.

#### Table Accuracy and Consistency
- **Table 4 Validation**: The Dice scores and Normalized Surface Dice scores for the EfficientSAM and LiteMedSAM models must be verified for accuracy and consistency. Discrepancies in the scores require clarification to ensure the reliability of the reported results.

#### Comparative Framework Analysis
- **Table 5 Framework Comparison**: When comparing model performances across different frameworks (PyTorch and ONNX), the accuracy of the data presented must be ensured. Any significant differences should be accompanied by a thorough explanation to understand the underlying causes.

#### Runtime Data Correlation
- **Table 6 Experimental Setup Consistency**: The runtime data listed in Table 6 must correspond with the settings described in the experimental setup and methods section. Any discrepancies could affect the perceived efficiency gains of the proposed models.

#### Final Testing Results
- **Section 4.4 Placeholder**: The statement “This is a placeholder” in the section ""Results on final testing set"" indicates that actual testing results are missing. These results are crucial and must be included before the final submission.

#### Limitations and Future Work
- **Section 4.5 Clarification**: While the manuscript outlines limitations and suggests future research directions in Section 4.5, it is imperative that these discussions are grounded in the findings of the current study. The proposed future work should be feasible and explicitly connected to the results and conclusions drawn from this research."
283,"This paper focuses on the benefits of sparse activations in infinitely-wide networks and studies how weak correlations in the weights can improve the generalization performance. The weights have been typically assumed to be independent so understanding the effect of correlated weights is currently limited. To this end, this paper first proposed a variant of NNGP for correlated weights, showing that the inducing of correlated wights is capable of improving the generalization performance in the sparse regime. The corresponding theoretical explanation is further provided by extending the recent advances in generalization theory. Eventually, the optimal weight correlation give a target sparsity has been introduced, improving the practical usage of this paper. 

Overall, this paper is presented with high quality and clarity. I enjoy much when reading the introduction and review of sparse NNGP.

The novelty of this paper mainly lies on (1) the formulation on extending of NNGP for correlated weights; (2) the empirical evidence for the benefits of weights correlation to feature sparsity, and to generalization performance; (3) Theoretical proof.

My major concern is why this paper mainly focus on random weighted sparse networks? Can the findings be generalized to trained sparse networks, the most closely one is ""The lazy neuron phenomenon: On emergence
318 of activation sparsity in transformers""; and sparse training regime i.e., SET (https://www.nature.com/articles/s41467-018-04316-3), SNIP (https://arxiv.org/abs/1810.02340), ITOP (https://arxiv.org/abs/2102.02887). 

I also list several minor cons of this manuscript here:

(1) The authors mention that ""Therefore, sparse random models do not perform well with deep architectures [12]."" However, IMHO, there are empirical papers showing that when models get deeper and wider, sparse random models actually perform better, i.e., random pruning https://arxiv.org/abs/2202.02643, and random sparse GNNs: https://arxiv.org/abs/2211.15335. While the randomness in this manuscript is slightly different from the papers I mentioned, it is necessary to at least discuss and explain this two seemingly counter-arguments.

(2) I encourage the authors provide a contribution summary in the early of the paper to improve the readiness of this paper.

(3) When this paper mentions in the intro ""Currently, there is no theoretical explanation of how deep network models such as the Transformers benefit from sparsity"" as their motivation, I expect to see any analysis of Transformers in the main paper. However, it is not presented. I encourage the authors to say something about Transformers or simply remove this statement."
284,"## Summary
---
This paper introduces SQL-RL-GEN and SQL-RL-GEN*, two novel approaches for text-to-SQL generation using reinforcement learning and large language models. The work addresses the challenge of generating SQL queries from natural language while minimizing computational resources. The authors propose using a reference reward function generated by SQL-RL-GEN to guide the training process, which is then utilized by SQL-RL-GEN* to fine-tune a base LLM. The paper demonstrates improved performance using only 1,000 training samples and a relatively small 248M parameter model.

## Strengths
---
- Strong empirical results showing improved accuracy (2-7%) over state-of-the-art methods while using only 1,000 training samples
- Resource-efficient approach that achieves good performance with a small base model while demonstrating versatility across different datasets

## Suggestions for Improvements
---
- The paper lacks a comprehensive analysis of model parameter counts and computational requirements compared to baseline methods like SQLNet and Seq2SQL, making it difficult to fully assess efficiency claims
- A more thorough comparison with other reward-based approaches in text-to-SQL generation would strengthen the paper's contribution
- More detailed analysis of failure cases and limitations would help guide future research in this direction"
285,"A) The authors can use more dimensions in the study to compare LF, HF and MF. From Figure 1 and Figure 2, at some times, the error of HF is much smaller than that of MF, but it's FLOPS is not much greater than that of MF. Therefore, the author needs to explain the superiority of MF from more perspectives.
B) The description of solutions in Section 2.3 is a bit confusing. I hope the author can provide further explanations.
C) The discussion of related work is very thin, there have been many efforts to learn from low fidelity complex PDEs and solve or forecast PDEs at other resolutions and fidelities. Here are some recent examples:
- ""Non-linear operator approximations for initial value problems."" In International Conference on Learning Representations (ICLR). 2022.
- ""Coupled Multiwavelet Operator Learning for Coupled Differential Equations."" In The Eleventh International Conference on Learning Representations. 2022.
- ""Multiwavelet-based operator learning for differential equations."" Advances in neural information processing systems 34 (2021): 24048-24062.
Particularly, the initial value problem has been actively considered in the neural operator literature and so it should be accurately discussed.
D) As the author stated, the performance of the model needs to be verified on more PDEs.
E) What are the main factors that affect the multi-fidelity and why the CNNs are the right approach? I think this is an interesting study but need to be put I the general context so that we can see when we should use it in combination with other techniques."
286,"Paper Summary:
The authors address the Segment Anything in Medical Images On Laptop challenge via their proposed DAFT strategy: A data-aware fine-tuning strategy in which the authors divide the different modalities and leverage knowledge distillation. They then fine-tune the model on all domains and then a second time on the domain of interest, resulting in one model per image domain (except for the 3D domains which they pooled).

Paper Strengths:
- The paper investigates a range of different settings to come up with their approach. The authors ablate their setting against a variant that only uses general fine-tuning and a well-motivated model selection approach. 
- The authors report strong results on the validation dataset outperforming the baseline in many imaging domains. 
- The paper is written clearly and provides a good amount of details which helps in understanding the approach of the authors.
- The paper uses a ""flood of improvements"" in which they use various engineering approaches to improve the inference speed of the model.

Paper Weaknesses:
- The motivation as to why the authors use meta-models to select the respective image domain remains a bit unclear. While the now-used approach to only use the image name depends on a substring matching problem which may be dangerous, the authors do not discuss the usage of a simple domain classifier (e.g. ResNet18 with domain classification output)
- The distillation process is not explained thoroughly: which data did you use as input to the teacher model? Furthermore, if you distill, why did you opt to distill from LiteMedSAM over standard MedSAM. 

Further Ideas:
- The paper could be further improved by a discussion about the characteristics of the image modalities in which the proposed approach worked really well and in which scenarios the approach failed."
287,"The paper, ""Automated Generation of Hospital Discharge Summaries Using Clinical Guidelines and Large Language Models,"" explores an approach to automating the creation of hospital discharge summaries. Leveraging LLMs few-shot prompted by clinical guidelines, this method does not require extensive training datasets and can handle full-length physician notes (by utilizing large LLMs), guided by clinical best practices. The system, tested with GPT-4-turbo and MIMIC-III physician notes, was evaluated by clinicians, achieving a micro accuracy of 0.81. The paper discusses methodological limitations and future improvements needed in the evaluation framework.

Pros:
1. Utilizes LLMs in conjunction with clinical guidelines to automate the creation of discharge summaries, offering a solution to reduce manual effort.
2. The methodology does not rely on extensive datasets for training, potentially making it more adaptable and easier to implement across different hospital systems.
3. Involves clinicians in the evaluation process, ensuring that the automated summaries meet practical clinical needs and standards.

Cons:
1. The reliance on a single dataset (MIMIC-III) for testing may limit the generalizability of the findings to other healthcare environments or patient demographics.
2. Accuracy and Completeness: While achieving a micro accuracy of 0.81 is promising, there may still be concerns about the accuracy and completeness of the generated summaries, especially in complex cases. A more in-depth error analysis of identifying which group of patients (which health conditions, which demographics, etc) end up with a better model performance can significantly improve the work.
3. Even though the paper acknowledges limitations in its evaluation framework, suggesting further refinement and broader testing is necessary to fully assess the system's effectiveness and reliability."
288,"The paper tests the performance of different VLMs in solving math problems using various zero-shot prompting techniques. The techniques themselves are nice and elaborate, as well as the variety of datasets and VLMs used. Therefore the results are convincing.

Having said that, I think that the paper is lacking in a few key aspects:
1. The VQA datasets were extensively studied in the past - I would present the performance of the best models that were trained for these tasks. This will allow us to see the gap the VLM is expected to close.
2. The paper only focuses on problems such as counting. I think that it's more interesting to ask the VLM to approximate the number of objects rather than give an exact number. A human won't be able to ""see"" the number of objects as well. The human observer will have to count, using his finger, which is a completely different task than seeing the image and ""counting"" how many items there are. 
3. Use RAG, few-shot - if you want to simulate something more similar to a human process.
4. There is not enough description of the dataset, types of questions, and error analysis, so it's hard to understand why the model was wrong when he was wrong (or right)."
289,"**Summary:**

The paper proposes SleepFM, a foundation model for sleep, trained using contrastive learning on a self-curated dataset consisting of EEG, ECG, and respiratory signals. The authors evaluate their model on downstream tasks like sleep stage classification and apnea event classification, showing performance superior to end-to-end trained CNN models. They also perform retrieval tests, showcasing their model’s ability to retrieve one modality’s closest embeddings from the test set based on another modality’s embeddings. The embedding layer in their model consists of three CNN encoders for each type of signal data, and the model is pretrained on a contrastive learning objective. The paper also evaluates the impact of pairwise contrastive learning vs leave-one-out contrastive learning objective, showing better performance on the downstream tasks using the latter. For classification, the model uses the embeddings from the pretrained model and uses them to train a logistic regression classifier to evaluate downstream performance. The model shows good performance compared to the baseline in the paper across a wide set of experiments.

**Strengths:**

1. The authors introduce a fairly large-scale multi-sensory dataset of simultaneous measurements of EEG, ECG, and EOG signals, focused on training a foundation model from scratch. This dataset seems well-curated, and based on the downstream results, the embeddings from the pre-trained model have a positive impact on the performance on given tasks.

2. The motivation for using a contrastive-learning-based pre-training methodology using all three types of time-series data is sufficiently articulated and well-grounded with prior work in the paper. 

3. The pre-training, fine-tuning, validation, and test splits for the dataset are well-defined, mitigating the risk of any data contamination in the evaluation pipeline.

4. Owing to the availability of various types of paired time-series data in the pre-training dataset, the comparison of pairwise contrastive learning vs leave-one-out contrastive learning as the training objective was relevant and interesting.

5. The k-shot analysis of the model’s performance for classification was relevant, explicitly showcasing that contextual information learned during pretraining can improve downstream performance.

**Weaknesses:**

1. I am not sure I would call this model a “multi-modal” model, since the model is primarily trained on paired multi-variate time-series across different domains. EEG, ECG, and EOG data all lie in the time-series modality and are similarly modeled using the same type of encoders.

2. Experimental comparison with other recent statistical and deep learning methods for the given downstream tasks is necessary to have a more holistic understanding of the proposed model’s performance.

**Other recommendations:**

1. In future work, interpretability experiments to show what the model in learning would be interesting, and evaluating the model potentially in zero-shot settings would be appreciated as well.

2. Leave-one-out contrastive learning is not a new approach, and prior works from other domains [1,2] should be cited for the same.
  
    [1] Sanchez-Fernandez, A., Rumetshofer, E., Hochreiter, S. et al. CLOOME: contrastive learning unlocks bioimaging databases for queries with chemical structures. Nat Commun 14, 7339 (2023). https://doi.org/10.1038/s41467-023-42328-w
  
    [2] Xiao, T., Wang, X., Efros, A. A., & Darrell, T. (2021). What Should Not Be Contrastive in Contrastive Learning. International Conference on Learning Representations. https://openreview.net/forum?id=CZ8Y3NzuVzO"
290,"The paper presents a novel planner that merges a data-driven planner, such as Plansformer (referred to as System 1 or fast thinking), with a traditional planner like LPG or FD (termed System 2 or slow thinking). The concept behind this approach is to employ System 1 to generate a plan, while a metacognitive system is employed to assess whether to go ahead with the plan based on the confidence score of this plan. When the confidence score is low, the system resorts to utilizing System 2 to formulate a plan for the given problem.

Questions: How are the initial values for the parameters H, T1, T2, T3, and epsilon determined? Are they subject to any learning process?

Could you please explain the reason behind PF, a fast-thinking approach, showing slower execution times than LPG as indicated in Table 2?

Given the challenges of representing intricate domain knowledge using System S2 in practical planning situations, isn't it better to adopt approaches where System S1 (based on past experiences) and System S2 (based on available incomplete knowledge) work together as in Rana et al. in their paper ""Sayplan: Grounding large language models using 3D scene graphs for scalable task planning"" (arXiv preprint arXiv:2307.06135, 2023)? Is the approach of trying S1 and then resorting to S2 in case S1 fails feasible as S2 itself may not be available for the domain."
291,"## Summary
This paper investigates the effectiveness of using LoRA fine-tuning compared to full-parameter fine-tuning LLMs for adaptation to the healthcare domain. Authors report results for both smaller 7B Llama-2 as well as larger 70B Llama-2 fine-tuned models. They also compare against close-source state-of-the-art models like GPT-4 and Med-PaLM-2. The findings and described methods are a useful reference for healthcare machine learning practitioners who want to fine-tune a general-domain LLM for health-care tasks.

## Pros
* Evaluation is comprehensive across multiple datasets
* Evaluation is carefully done with decontamination pipeline
* Compares one of the most popular parameter efficient fine-tuning technique, LoRA, against full fine-tuning and state-of-the art models (GPT-4 and Med-PaLM) in healthcare domain
* Models are publicly released and available
* Datasets are publicly released and available
* Manuscript is well written and easy to follow

## Cons
* Methods section describes that LoRA may be applied to only attention layers vs. all layers. PE-FT results in Table 1 are for LoRA applied to all layers. It would be nice to also show the performance for LoRA applied to only attention layers since authors have mentioned this is a common approach. However, this is more of a ""nice to have""."
292,"### **Summary**
The paper presents counting reward automata (CRA), a novel framework for modeling reward functions in reinforcement learning (RL) as formal languages. The paper shows that CRA can express any recursively enumerable reward function, unlike previous state machine-based approaches limited to regular languages. The paper also proposes Counterfactual Q Learning that exploits the structure of CRA to improve sample efficiency and convergence guarantees. It demonstrates their effectiveness on complex tasks that require long-horizon planning and counting.
The paper’s strengths are:
It introduces a general and expressive framework for reward function specification that can handle a wide range of problems, including context-free and context-sensitive languages.
It provides theoretical and empirical evidence that CRA can improve the performance and scalability of RL agents compared to existing methods.

### **Strengths**
- General and expressive framework for reward function specification that can handle a wide range of problems, including context-free and context-sensitive languages.
- Comprehensive theoretical and empirical evidence that CRA can improve the performance and scalability of RL agents compared to existing methods.
- Leveraging natural language and large language models to facilitate the intuitive and human-readable specification of CRA from task descriptions.

### **Weakness**
- Lacking clear comparison or discussion of the trade-offs between CRA and other neuro-symbolic or hierarchical RL methods that also aim to address long-horizon tasks.
- Missing analysis or evaluation of the robustness, generalization, or interpretability of CRA-based agents.
- Somewhat toy examples"
293,"The paper presents a comprehensive study on the application of self-supervised learning (SSL) in computational pathology, focusing on the pre-training and downstream performance evaluation of visual foundation models on large-scale pathology tasks. The study compiled a massive academic pathology dataset, consisting of over 3 billion images from 423 thousand digital microscopy slides, and compared the pre-training of visual transformer models using masked autoencoders (MAE) and self-distillation models (DINO). The downstream performance was evaluated on six clinically relevant tasks from three anatomic sites and two institutions, demonstrating the benefits of pre-training on pathology data for downstream performance compared to pre-training on natural images. The DINO algorithm achieved better generalization performance across all tasks tested, signifying a significant advancement in computational pathology research.

Pros:

The study addresses a critical gap in the application of SSL algorithms and foundation models in the medical domain, particularly in computational pathology.

The compilation of the largest academic pathology dataset to date, consisting of over 3 billion images, demonstrates a significant contribution to the field.

The comparison of pre-training methods and evaluation of downstream performance on clinically relevant tasks provides valuable insights for the development of performant models in computational pathology.

The study's findings indicate a phase change in computational pathology research, paving the way for more performant models based on large-scale, parallel pre-training at the billion-image scale.

Cons:

The study could benefit from a more detailed discussion on the limitations and challenges of SSL algorithms and foundation models in the medical domain, particularly in clinical workflows.

While the downstream performance was evaluated on clinically relevant tasks, the study could further emphasize the potential impact of these findings on real-world clinical applications.

The document lacks a detailed discussion on the ethical considerations and potential biases associated with the use of large-scale pathology datasets and SSL algorithms in healthcare.

Overall, the work demonstrates high quality, clarity, originality, and significance in advancing the application of SSL algorithms and foundation models in computational pathology. The study's comprehensive approach, large-scale dataset compilation, and valuable insights into pre-training methods and downstream performance evaluation contribute significantly to the field of computational pathology. However, further discussion on ethical considerations and potential biases, as well as the translation of findings into real-world clinical applications, would enhance the overall impact of the work."
294,"The paper considers the theoretical problem of essential equivalence between Transductive Learning and Probably Approximately Correct (PAC) Learning in supervised learning problems. Here, two learning models are essentially equivalent if and only if their sample complexity only differs by an additive polynomial in terms of the error rate $\epsilon$ (in both transductive and PAC learning) and the tail probability $\delta$ (in PAC learning). The authors establish a series of results on essential equivalence between these two models, with a focus on agnostic setting.

Key contributions of the paper:

1. Realizable learning setting: This paper extends the prior results of Aden-Ali et al. (2023a) on realizable setting to cover the entire class of (pseudo) metric losses and provides a unified proof.

2. Agnostic learning setting: The authors show that transductive learning and PAC learning are essentially equivalent for agnostic setting with bounded losses and in binary classification, and further conjecture that this is true for most natural label spaces and loss functions.

Overall, I think this is a good paper. This paper is well written and the main results are presented in a way that is easy to understand. By extending existing results to agnostic settings, the authors address an area where prior results were limited, especially in binary classification. Their technical approach, including symmetrization arguments and the OIG framework, is also well-presented and easy to follow."
295,"This paper proposed a novel reinforcement learning method for solving the task of generating SQL statements from questions in natural languages. Authors introduce two models: (1) SQL-RL-Gen model generates a reward function. This function improves the training process. (2) SQL-RL-Gen* model uses the generated reward function to tune the LLM model. Experimented with a limited amount of training data, this new method achieved better performance than state-of-the-art methods. 

Questions: 
1) Is it possible to enumerate ""all possible text prompts"" in real applications? 
2) It seems that authors used (Schulman et al 2017)'s method to generate reward functions. Is the method too old? What is the limitation of the method? 
3) In experiments, authors repeated 10 times on the same sample before moving on. Did (how often) LLM ignore feedbacks?"
296,"1. Summary and contributions: Briefly summarize the paper and its contributions
Outlines the development of an LLM called SoftTiger. This is a finetuned version of an open-source LLM named TigerBot. This is achieved using supervised finetuning tuning from a dataset of general text, a previously released clinical dataset and a novel clinical workflow dataset. The novel dataset is made up of instruction pairs for 3 tasks performed on the MIMIC-IV dataset with the outputs generated by GPT-4 and validated by 5 physicians. The results show that the finetuned models gained accuracy on automated evaluation benchmarks.

2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance to the community.
- It is an early example of finetuning large (70B) open-source LLMs across multiple GPUs.
- After carefully evaluating the trade-off between clinical complexity and helpfulness, 3 clinical data structuring tasks were chosen. This gives the work a clear potential for clinical impact.
- A very good section outlines the administrative burden on physicians.
- Making IPS or FHIR structure the output is optimal for potential future integration into current e-health systems

3. Weaknesses: Explain the limitations of this work along the same axes as above.
-MIMIC data user agreement prevents the sharing of the data or derivates with 3rd parties. Therefore, the SoftTiger and dataset should not be publicly released. They could be hosted on PhysioNet though.
- Similarly, MIMIC data should not be sent to 3rd party LLM providers as seems to have occurred in Table 5 unless via Azure or Amazon (see https://physionet.org/news/post/gpt-responsible-use). Please state clearly in text or “Ethical Considerations and Reproducibility Statement” if these services were used.
- Evaluation and training data only uses MIMIC-IV, which are discharge summary notes from the ICU department of a single health centre. This should be noted as a limitation.
- GPT-4 is used to produce the clinical training and evaluation set. This is then corrected by clinical review. No mention of the performance of GPT-4 on the task is made or the inter-annotator agreement. Furthermore, justification (most likely from a data governance perspective) is given on why GPT-4 cannot be used for this task directly if it can produce the labels for the task.

4. Correctness: Are the claims and method correct? Is the empirical methodology correct?
- In the introduction, it is claimed the 2 primary challenges for LLM clinical adaptation are finding a ‘helpful clinical task’ and input length constraint. I do not believe this to be true. Numerous clinical tasks could be performed by LLMs, e.g. diagnoses, discharge summary writing, reporting of adverse drug events etc... Barriers such as effective and safe evaluation, data privacy and governance, and integration into healthcare providers’ electronic systems would seem equal if not greater to this constraint. 
The second constraint of input length is notable but only for open-source models (closed-source models have context lengths >100k), a distinction that is not made. However, the trained model is only extended to 8k and claimed as a source of novelty. Current open-source models such as mistral have been trained with an 8k context window.
- Claimed that note length usually follows power law without proof or citation
- The approach is claimed to be “light-weight” but requires 64xA100s GPUs
- It is claimed that as TigerBot has a larger vocabulary size than Llama-2, it has a larger clinical vocabulary. However, as TigerBot is multilingual this claim only seems true if evaluating on multilingual data also. The claim that TigerBot has a greater English clinical vocabulary needs further explanation or proof.
- It is not obvious that the addition of the general-purpose or Asclepius datasets will improve performance on the clinical workflow tasks.
- “Dictionary of abbreviation expansion to standardize abbreviations” is known not to work due to the redundancy of terms. For example, “hr” could be expanded to hour or heart rate, depending on the context.

5. Clarity: Is the paper well written?
- “We then evaluate TigerBot and Llama-2 chat models using next-token prediction.” It is not clear to me how this evaluation works. Is this exact matching? Further explanation is required.
- Not clear how Llama-2 and TigerBot were extended from 4k to 8k inputs.
- It is claimed that “it is beneficial for worldwide adoption to build multilingual models”, which is true. But it is not clear if the fine-tuning dataset is also multilingual.
- Fig.1 shows some very helpful information, but it is not clear which task is related to which plot point due to the use of repeated colours. Furthermore, the Fibonacci scale is not explained.
- Not clear if, in normal practice, discharge summaries are the only source of information used to complete the 3 subtasks trained and evaluated in this work.
- Figure 2 is a direct screenshot from tensorboard or similar. Removal of the UI buttons, and adding full and axis titles would improve this figure. Not clear what the faint lines are in the figure
- Table 3 should be moved higher up the training data section and would be more instructive swap the size column for number of examples in each dataset.
- Figure 3’s final column is all 0% and so does not need to be included. Moreover, the information may be more helpfully presented as a table of min, median, max for input, output and total

6. Relation to prior work: Is it clearly discussed how this work differs from previous contributions?
- This is the first open-source LLM finetuning to output on FHIR IPS, FHIR Clinical Impression and FHIR Encounter from medical discharge summaries.

7. Reproducibility: Are there enough details to reproduce the major results of this work?
- The number, speciality, nationality, and seniority of clinicians surveyed to produce Fig 1 is not stated
- Would be useful to link or add in the appendices the exact FHIR structures of the 3 subtasks.
- The training framework section is limited. No training hyperparameters are given. The acronyms PP and DP are used without explanation.
- The settings, prompt, and model version used to generate the clinical workflow dataset using GPT-4 are not stated"
297,"This paper proposes a new method to combine relational dependency networks (in the form of relational probabilistic decision trees (RDTs)) with neural networks in order to leverage both structured and unstructured data for decision-making in noisy environments. Overall, the paper is well-motivated, and the methods are clearly described. While only a short paper, it might still benefit from more elaborate experiments that also compare to the state-of-the-art methods. 

Some open questions/limitations that could be addressed 
* Why would the logical penalty function (end of page 2) include phi that depends on unstructured data? This way, the unstructured data could invalidate the logical penalty function. Indeed, the penalty function on page 3 doesn’t contain any contribution from the unstructured data anymore. 
* It is unclear how we can balance the contributions of neural refinement and the RPT. 
* It would be interesting to see how the model behaves with different levels of noise in the structured data as well as the labels. I assume that with zero noise, an RPT would be sufficient to achieve high accuracy. When (at which noise level) does it become helpful to consider a combination of RPT and NN? 
* Where do we get the labels for the RPT? Could this be provided by another NN?"
298,"The authors train self-supervised vision foundation models on a very large set of pathology data and show that the embeddings they produce yield much better performance on downstream tasks compared to those trained on general image data, specifically ImageNet. The writing is clear, the presentation is thorough, and the results are strong. The work has potential for broad impact. I didn't read the supplementary sections in detail, but the thoroughness is appreciated. I am not familiar with pathology, so I will leave it to other reviewers to assess the selection of downstream tasks.

There are elements of the experiments and presentation that could be improved. While it should be straightforward to improve presentation, I understand that, since the experiments themselves are costly to run, additional experimental runs may not be able to be included in this submission, which is fine - it can be taken as feedback for further development of the work.
1. It is not necessary to show all the results across epochs. It makes the figures unnecessarily large and difficult to interpret, and it masks the effect of model selection, e.g. using a validation set to determine which checkpoint's model to actually use. In particular, the overlapping lines make Supplementary Figure 1 a bit hard to read. Perhaps just one figure to make the point about saturation and overfitting would be enough. The rest could be tables or bar charts or similar for the selected models.
2. It does not seem appropriate to draw conclusions about the effect of data quantity from the loss curves shown. I don't think you can reasonably disentangle the effects of training time and data quantity in these results. The best approach would be to train additional models on subsamples of the data set, but I understand this is costly.
3. It's unclear why not all models are shown in Figure 2.
4. Supplementary Figure 1 contains the main results of the study. These should be in the main paper. Just a table would be fine.
5. It's not clear what is tRes50 vs Res50.
6. The baseline model should have the same architecture (ViT) as the experimental models in order to isolate the effect of the pre-training data. It is even acknowledged by the authors that ResNet may be overfitting due to the architecture itself.
7. It is explained that DINO-ViT-large is excluded due to training cost, but why is there no MAE-ViT-small or MAE-ViT-base?
8. I would expect that the data cannot be released, but why can the pretrained models not be released? Regardless, the intention to set up an API to get embeddings is appreciated.

Minor nitpicks:
1. In the pre-training section, you mention that your data is an order of magnitude larger than any previous effort. It would be nice to cite the largest previous effort here.
2. Please define pseudo-epoch and explain why it is used instead of standard epochs (I am guessing it is to increase checkpoint frequency?).
3. Typo, first paragraph of discussion section: ""SLL""
4. In the discussion section, you say you trained DINO only on ViT-small, but you report results also for ViT-base."
299,"## General comments
Upon review, the manuscript on Verlet Flows should be accepted without revisions.
The paper demonstrates the use of Taylor-Verlet integrators in continuous
normalizing flows, presenting a method for exact-likelihood generative models. The
theoretical basis, methodological approach, and experimental validation are well
articulated. The overall contribution is deemed significant for the field of
generative modeling. The work is ready for publication, offering valuable insights
and advancements without the need for further modifications.
However, if the authors wish, they may increase the font size of the plot labels and
ticks of Figure 1."
300,"**Summary** This work studies and designs a system that exploits sparsity in the input layer as well as the intermediate layers in a resource-constrained scenario. The system uses a single masked representation of each image during training and then employs independent subnetwork training algorithm. The authors show that a single masked representation of each image can match the performance of randomly drawing masks at each training iteration. 

**Pros** I must first make a disclaimer that I don't have a background in designing efficient distributed learning system, therefore, I am unable to evaluate the authors' claim on efficiency, etc. I personally find it surprising that a single masked representation of each image can work almost as well as drawing random masks at each training iteration since this is basically like making the model only see part of the image during training. And this is the key the authors use to design the system since this sparse representation can bring down the computational cost. 

**Cons** With that being said, I do feel the experiment section is lacking for this work, even from a layman's perspective. Based on what I see, most of the experiments are about accuracy. If the authors are trying to claim their system is efficient, I believe experiment results on, say, training time, memory, etc should be shown. Also, it is not clear how this work is compared to previous work on efficient learning, as the authors didn't benchmark their approach to other existing methods."
301,"Clarity: 
The authors present a well written paper stating the current litterature and why their work is an advancement in the field of EEG pretrained models stating that their model achieves better performance and is able to generalize across different tasks via finetuning. 
Originality: The work combine many state of the art methods in the space of foundation models and apply them to a large scale foundation model trained on EEG data. A left out dataset reserved for validation that has no connnection with the TUH training dataset. 
Significance: The performance shows a clear advancement within this space compared to previous supervised and self-supervised models. 
Quality: the authors sometimes miss explanations on datasplitting and other common practices that need to be present to ensure. The fundamentals should be included in the script.

Major points
1.	Figure 1 should have a better explanation emphasising subfigures a and b 
2.	the finetuning paradigm is not clearly defined. You should consider explaining the way you constrained updating the weights during end-to-end fine-tuning if you did so to avoid misconceptions.
3.	In table 2 you present the AUROC and AUPRC of only a subset of the datasets you have avaliable and you show that with the model architecture you have created there is little difference between traning self-supervised and trainning supervised. I would want you to show the performance on the Neonate dataset to demonstrate the difference between supervised or self-supervised traning.
4.	There is very little disscussion of the results and interpretation of for example figure 3 showing the intrepetation of the naïve Bayes model. Here you seem to highlight the areas for seazures but you do not specify any output from the model. 
5.	You need to specify the datasets used for pre-training, fine-tuning, and testing to ensure that there is no lekage from training to testing 
Minor points
1.	In table 1, there is little reason to present all the EEGFormer variants as they have very similar performance. I would suggest that you proceed with only the EEGFormerl  for simplicity and just state the other tests in the text.
Pros 
•	The authors find high effectiveness in their model training a codebook to represent EEG with subsequent finetuning to solve interesting tasks such as: abnormal EEG detection, classifying EEG artifacts, classifying EEG slowing events, seizure detection, and neonatal seizures detection.
cons
•	Authors do not show the perfomance of finetuning styles for models on all datasets 
•	Auphors do not provide the way they split the training, test, and validation data giving no indication that the 
•	The authors presend very little discussion on their results, thus making the intrepetation of their results hard to understand out of the gate.
•	The embeddings seem to be highly dependent on large amounts of fine-tuning in order to perform well."
302,"This is a review of the manuscript, ""CHAROT: Robustly controlling chaotic PDEs with partial observations,"" submitted to the ICLR 2024 Workshop on AI4DifferentialEquations In Science. The paper describes an attention-based memory architecture that can augment actor-critic reinforcement learning (RL) algorithms in the context of chaotic partial differential equations (PDEs). The authors compare the utility of three controllers for the purpose of forcing a Kuramoto–Sivashinsky (KM) system. Tested controllers include:
1. a simple (memory-less) multilayer perceptrion,
2. a controller with long short-term memory, and
3. a modified transformer, a.k.a. ""CHAROT"".
The CHAROT controller exhibits superior performance to the other two across a range of KS parameters, spanning the weakly- and fully-chaotic regimes. Information in the appendices describe the effects of sensor density, hyperparameter selection, architectural choices, etc.

The paper is well-written and the results seem convincing. As is often the case with such work, it is unclear whether the dimension of the three controllers has been held constant across the tests. I do not necessarily doubt the utility of the chosen architecture, but it seems unreasonable to compare controllers with highly divergent numbers of trainable parameters, i.e., because the size of the model typically has a first-order effect on performance. Moreover, I do not see how the proposed architecture has been tailored to deal with chaotic systems, in particular. Perhaps the authors can motivate their architectural choices with regards to features of chaotic systems.

Lastly, I question the relevance of such methods to real flow control problems. It seems to me that developing a reliable training protocol (including the requisite scale-resolving simulations) is far and away the dominant challenge in that context, and highly-turbulent systems resist selective control actions (turbulence has a way of ""taking over""). It might be useful for the authors to sketch a practical application of the proposed control strategy, including the training data and expected sensor data."
303,"This work presents a noise guided trajectory based system identification method for inferring the dynamical structure from observation generated by stochastic differential equations.
The paper is well written, and some conclusions are drawn from various numerical tests for highlighting the superior performance of our learning algorithm."
304,"## Summary
In this work, the authors present EEGFORMER, a foundational model for electroencephalography (EEG) data. They present a new pretraining method for EEG data, that works as follows: first, EEG signals are segmented into patches and passed into a Transformer encoder. Then, they apply a vector-quantized model to convert the patch representations into discrete indices, which as subsequently fed into a Transformer decoder, with the objective to reconstruct the input. The authors apply EEGFORMER in 5 downstream tasks, showing good performance and transfer. Moreover, they show that the representations learned can also be highly interpretable. 

## Strengths
- The self-supervised approach the authors propose is promising, and has the potential to efficiently utilize the vast amounts of raw EEG data available.
- The idea to encode the EEG signals into quantized vectors can push the model towards learning interpretable representations, e.g. similar signal patches may be mapped into the same quantized encoding, that may then help us interpret predictions, as the authors show in an experiment.
- The empirical evaluations demonstrate good performance on all downstream tasks tested, and seem transferable. 

## Weaknesses
- It would be good to further explore the transferability and interpretability of EEGFORMER's representations, by performing further experiments on additional cases and corpora, to verify if the authors' observations truly generalize.  

## Overall Assessment 
The paper proposes a novel foundational model and pretraining method for EEG data, and shows strong downstream results and promising interpretability of the representations. It has the potential to pave the way for utilizing large amounts of unlabeled EEG data for various downstream tasks."
305,"Summary
------------------

The paper explores ensemble prediction techniques for Fourier Neural Operators (FNOs) in medium-term weather forecasting. It introduces a novel approach using Laplace approximation for uncertainty quantification in FNOs, addressing a gap in existing research. It evaluates various approaches, including input perturbations and statistical loss functions, and introduces a novel uncertainty quantification technique using Laplace approximation for FNOs. Through experiments on a one-dimensional PDE dataset, the paper demonstrates the effectiveness of the proposed method compared to traditional ensemble methods. The findings suggest that the proposed approach improves uncertainty estimation in FNOs, enhancing the reliability of medium-term weather predictions. Further research in ensemble prediction techniques for neural operators is needed, particularly in scaling the methods to larger networks and high-dimensional data. Overall, the paper contributes to advancing the field of ensemble prediction in weather forecasting and related domains.

Evaluation
------------------

**Quality:** The paper is good quality, offering interesting insights on ensembles of FNOs, which could have some impact on weather forecasting using neural operators and other (related) methods. 

**Clarity:** The paper provides a clear introduction to the problem domain and previous research efforts. It explains the concepts of neural operators and FNOs concisely and, in my opinion, makes it accessible to general readers, which I appreciate. The methods for UQ and ensemble prediction are well-described, with equations and algorithms nicely structured. Experimental results are presented logically, supported by tables and figures - good visuals overall.

**Originality:** The paper contributes to the field by exploring ensemble prediction techniques for FNOs, an area with limited prior research. So, its originality resides in applying a new method to a challenging problem. The introduction of Laplace approximation for uncertainty quantification in FNOs is somewhat novel, as recognized by the authors and properly cited previous related works, e.g. Magnani et al. The comparison with other ensemble methods adds value to the research, offering insights into the relative performance of different approaches. 

**Significance:** The paper's findings have significant implications for weather (and eventually climate?) forecasting and related fields in other domains and disciplines dealing with complex dynamic systems. By improving uncertainty quantification in FNOs, the proposed approach apparently enhances the reliability of medium-term predictions. The evaluation metrics and experimental results demonstrate the practical utility of the method.

Pros
------------------
1. Introduces a novel method using Laplace approximation for UQ in FNOs.
2. Provides a rigorous evaluation of the proposed method through experiments on a one-dimensional PDE dataset.
3. Demonstrates improved uncertainty estimation in FNOs compared to traditional ensemble methods.
4. Enhances the reliability of medium-term weather predictions, addressing a critical need in weather forecasting.
5. Contributes to advancing the field of ensemble prediction techniques for neural operators.
6. Opens avenues for further research in ensemble prediction methods for larger networks and high-dimensional data.

Cons
------------------
1. Requires prior precision tuning for the Laplace approximation, which may add complexity to the implementation. Discussing the limitations would be welcome, yet maybe this could be left for the workshop.
2. Uncertainty quantification in longer training runs using the proposed method appears to be underconfident.
3. Training FNO-Ensemble is computationally expensive compared to other methods. Some comments and ablation study could be added (eventually in the Appendix) and briefly commented (in the main part, just 2 sentences would suffice).
4. The generalization of the proposed method to higher-dimensional datasets needs to be investigated further. Some words of caution about that (substantial) jump to more challenging yet realistic cases would be welcome.
5. The comparison with other SOTA ensemble prediction methods could be more comprehensive. I understand this is not possible in such a short paper format, and consult with the sentence ""Such further comparisons and higher-dimensional experiments are left for future work."", some extra lines in Table 1 or appendices wouldn't hurt and would add much value to the analysis.

Clarifications
------------------

- In the intro, ""real-world data,"" I suspect the authors refer to ERA5, which is reanalysis data, not observational data. Please rephrase if so.

- ""of input-output functions D = {v_n, u_n}"" --> switch to output-input or exchange {u_n,v_n}

- in eq.2 one would expect sigma_n to be constant and not included in the loss, or is it a time-varying process? Please clarify as this would change the solution and ensemble in exponentially varying ways, no?

- clarify in eq.2 to what step prediction K the cdf ""G f ˆ (u n ),σ n"" refers to.

- is the computational cost an issue for the different approaches? could you include cputime estimates in table 1? i noticed that FNO-Perturbed and FNO-FL perform the same in RMSE but maybe there's other advantage (Q is not a definite score to declare a winner here, esp. in long training runs).

- a discussion on calibration (and eventually some metrics) would be welcome. sentences like ""to be underconfident."" should be substantiated with numbers - this tool can help: https://github.com/uncertainty-toolbox/uncertainty-toolbox

All these clarifications could be placed in the paper's experimental section or in an appendix/supp.mat section (which I don't find).

Typos & grammar
------------------

- Inconsistent capitalization in ""uncertainty quantification"" (sometimes capitalized, sometimes not).

- ""It exhibits better uncertainty quantification"" could be rephrased for clarity (e.g., ""it shows improved uncertainty quantification"").

- ""Moreover, we formulate a new Laplace approximation for Fourier layers"" - ""formulate"" could be replaced with a more precise term like ""propose"".

- ""The training set consists of 520, the validation and test set of 100 solutions"" - should include ""solutions"" after ""520"" for clarity.

- Inconsistent use of tense throughout the paper (e.g., switching between present and past tense). Choose one and maintain consistency.

- Some sentences are overly long and could be broken down for clarity and readability.

- Punctuation errors, such as missing commas or periods, should be corrected for better readability.

- In the conclusion, ""arguably strictly more powerful"" could be simplified for clarity.

- clean the bib; missing entries, capital letters. This is a nice tool btw: https://flamingtempura.github.io/bibtex-tidy/."
306,"Summary
The paper proposes an RL framework for solving LCL problems on graphs, leveraging local verifiers instead of ground-truth labels. This approach avoids algorithmic biases and supports non-unique solutions. The proposed framework significantly outperforms supervised baselines on four LCL problems.

Strengths
- The writing is clear and well-structured.
- The proposed framework achieves impressive performance across diverse LCL problems, surpassing supervised methods by large margins.
- The proposed framework is novel and does not require ground-truth labels or pre-defined algorithms.

Weaknesses
- In the experiments, does the VARL model have a comparable number of parameters to the baseline models? It is unclear how much of the observed improvement is due to having more parameters.

Suggestions
- It would be better to describe your reinforcement learning algorithm in the main text rather than in the appendix, as the paper proposes a reinforcement learning framework.
- The first sentence of the Experiments section, ""To test our ..."", should be revised to ""We test our ..."" for grammatical correctness.
- It would be better to evaluate the proposed framework on large-scale datasets."
307,"### Summary
-----
The paper leverages Monte Carlo Tree Search (MCTS) to generate process supervision data for enhancing the step-by-step reasoning capabilities of large language models (LLMs). Improving reasoning mechanisms in LLMs has been a longstanding challenge. Process supervision has shown better performance than methods focusing on producing correct outcomes. This work aims to train LLMs without relying on reward models, which are inherently complex, by augmenting data generated by the model itself. The proposed method samples and collects data from the search tree using MCTS, then performs supervised fine-tuning (SFT) on the LLM until convergence by minimizing the log-likelihood of the relative correctness of reasoning steps. The paper is well-written and easy to understand.


### Strengths
-----
- The experiments are comprehensively conducted with replications, standard error reporting, and notable performance improvements compared to baseline methods.
- The paper introduces a novel approach by suggesting data augmentation methods rather than relying on reward-based approaches, enabling greater training efficiency.
- The transferability evaluation demonstrates the model's ability to generalize effectively to unseen data.

### Suggestions for Improvements
-----
I kindly suggest clarifying the following points to improve the paper:
- Could the notation for the tree $\{(x^i, p_j^i, s_{j,k}^i, r_{j,k}^i)\}$ be embedded into Figure 1 for better visual alignment and clarity?
- Is this method the first approach to tackle data augmentation within the process supervision paradigm? If not, could references to related works be provided?
- What are the possible cases of distribution shift (e.g., label shift, covariate shift, domain shift) in Eq. (2), and can such shifts be minimized by the proposed method? Alternatively, does the term serve as a penalty term just to account for distribution shift?
- What could be the possible reason for the quick convergence observed? Could the use of self-generated data be a contributing factor?
- A more detailed explanation of the MCTS method itself would enhance readers' understanding. While the introduction mentions its use for annotation in previous works, the application in the proposed methodology appears to differ. Additionally, what are the strengths of MCTS compared to other baseline tools (if any exist) leveraged in handling reasoning process?"
308,"The paper proposes using the Swin Transformer for efficiency and the addition of point and scribble prompts to assist the model training and inference.
1. The link to the code is missing.
2. Adding how the weights of the prompt encoder and the mask decoder in the second stage of the model training would be helpful."
309,"This paper studies the problem of transforming a sample (approximately) from a source model to a target model, without knowing parameters of the source model. The authors subsequently establish reductions between various important high-dimensional statistical problems. The arguments and proof techniques are sound.

Clarity:

- The paper is clear and concise. I did not come across a missing definition. Section 1 and 2 are well written.

Originality:

- To the best of my knowledge, the paper studies a broader question than those usually tackled by papers concerning average-case reductions. The contributions are original, and the consequences of their base findings extend results in other papers.

Significance:

- The problem the authors tackle is significant, and dates back to the early days of statistics. I believe the results presented in this paper can/will be used to evidence computational hardness in a variety of new high-dimensional statistical problems (or at least, concerning different base distributions).

Recommendations and Questions:

- In Section 5.1, I recommend citing ""Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression"" by Arpino, Venkataramanan (COLT 2023), as the authors in that paper study computational hardness in the mixtures of linear regressions (MLR) and phase retrieval (PR) problems as well (although only in the sparse signal regime). From my understanding, they use the trivial (noiseless, infinite SNR) reduction from MLR to PR to evidence computational hardness of sparse phase retrieval through low-degree methods, and hence your reduction which allows for non-infinite SNR seems to help extend that work.
- How do your results relate to the paper: ""Hypothesis testing with low-degree polynomials in the Morris class
of exponential families"" by Dmitriy Kunisky? From my understanding, they also establish a partial ordering of distributions in a certain model with respect to computational hardness (via the low-degree method). Would be good to cite."
310,"Summary: The authors investigate best practices for designing Transformer architectures for weather forecasting. They identify key design choices in existing methods to build a streamlined architecture, with thorough ablation analysis to verify the benefits of their approach.

Pros:
- Strong performance on WeatherBench2 indicates that this is a state-of-the-art model in a competitive environment.
- Identifies key design choices in deep weather prediction models
- Analysis of scaling properties
- Clear and easy to follow methodology and presentation

Cons:
- Further analysis of spectral properties and temporal consistency would be very helpful to gauge the efficacy of the randomized forecasting objective.
- Further analysis of the costs/benefits of training and evaluating on different resolutions than the baselines would make the results more convincing.
- A comparison with more recent models like FuXi would be very interesting!"
311,"Thank you for considering me as a reviewer for this manuscript. After careful consideration, I feel that I may not be the most suitable reviewer for this particular work. My expertise does not sufficiently cover the areas related to the theoretical perspective of sufficient dimension reduction, which seems to be a foundational concept in the paper. The work is also defined build on several other Fréchet SDR work that I have never heard of. As such, I am concerned that I might not be able to fully grasp the motivation and nuances of the method presented. It would be in the best interest of the author and AC to not take into account of my comments on this work."
312,"This work presents Neural Context Flows (NCFs), a specialization of Neural ODEs to dynamical systems with unobserved (time-invariant) parameters. NCF introduces a (latent) context vector which modulates the behavior of the neural ODE. At training, a separate context vector is learned for each realization of the dynamical system along the parameters of the neural ODE. Importantly, during training, the neural ODE is linearized (by a first-order Taylor expansion around another context) with respect to the context, which seems to improve generalization with in-domain and out-of-domain parameters of the dynamical system.

### Strength
* The subject is interesting and well suited to the workshop.
* The goal of the paper is clearly presented and well introduced.
* To my knowledge, the approach is novel.
* The empiriacal results seem good, although I did not check them in details.

### Weaknesses
* As mentioned in the discussion, NCF lacks theoretical backing. At least an intuition as to why it works would profit the manuscript. My guess is that the linearization with respect to the context imposes a smooth modulation landscape.
* The quadratic cost of the training loss is also a concern, although it can likely be fixed with a Monte Carlo estimate (randomly selecting a fixed number of $j$ for each $e$ instead of all).
* Alternating between optimizing the weights $\theta$ and the contexts $\xi$ could lead to instabilities."
313,"Summary:
The authors propose to incorporate an Information Bottleneck criterion into the training of diffusion models, to facilitate the generation of samples that contain an explanatory signal within the high-dimensional data space. They define the explanatory signal through a mask that focuses the denoising process only on those regions that are 'relevant'. The proposed IB approach is compared against several other attribution methods as a means to generate these masks.

Pros:
- Interesting and original framing of the question how to obtain explainable samples from diffusion models.
- Theoretically well-motivated objective, with both qualitatively and quantitatively convincing results.
- Scientifically relevant datasets.
- Motivation and theoretical background were presented in a way that was easy to follow.

Cons:
- The application of the baselines is left somewhat ambiguous, are they incorporated into the DDPM training instead of IB?
- The text would benefit from a more in-depth discussion of the experimental setup, perhaps in favor of a less theoretical perspective on IB?
- The utility of the method for scientific application is hard to judge from the provided experiments, could the authors present a case study of how they intend to utilize this method to address scientific questions?"
314,"Verdict:
An interesting proposal with very little to back up the effectiveness of the method. The main novelties of this work seems to be: 1) the treatment of inputs and outputs as ""neural fields"" or in other words, inputs and outputs parameterized as functions. 2) vecrtorization of this neural fields 3) usage of transformers with these NN-valued inputs/outputs While 1 and 3 are not novel in itself see e.g. [1,2], I have not seen 2) yet. Where the paper breaks apart in my view is the sufficient evaluation and scientific documentation. The examples on which the architecture is evaluated are mere 1d toy examples. Moreover, parameter counts and hyperparameters are not disclosed, which makes it difficult to evaluate the effectiveness of the method is in comparison to the baseline models. Moreover, it is unclear to me what the trade-offs of this method are from the discussion presented in the paper.

For the above reasons, I am inclined to reject the paper.

Minor comments:
- I find the terminology ""Neural fields"" somewhat unnecessary in the realm of SciML. These are simply PDE solutions represented by Neural Networks. It's unfortunate that ML in general tends to favour trendy names such as Neural Fields, and would recommend sticking to clear-cut terminology wherever possible
- I would name it Appendix rather than tailpiece

References
[1] Yifan Du, Tamer A. Zaki - Evolutional Deep Neural Network (https://arxiv.org/abs/2103.09959)

[2] Louis Serrano, Lise Le Boudec, Armand Kassaï Koupaï, Thomas X Wang, Yuan Yin, Jean-Noël Vittaut, Patrick Gallinari - Operator Learning with Neural Fields: Tackling PDEs on General Geometries (https://arxiv.org/abs/2306.07266)"
315,"The authors proposed a reduction argument for computational hardness of weakly learning one-layer hidden neural networks via hardness of learning neurons and then the hardness of CLWE, which is a testing problem conjectured to be computationally hard. The major technical novelty comes from Theorem 8. The authors first constructed a CLWE instance by a slight Gaussian perturbation. Then the authors do a sample 2-split. One is used to train the learner; the other one is used to classify/validate. Now the author run the validation separately on the rest of the unused sample and some freshly harvest independent samples from the null. If the algorithm can weakly learn neurons and original samples are from CLWE’s alternative, then it seems the population risks should be lower, empirical risk concentrates well and the testing and reduction are done. Then the author constructed a neural net that can product the same the samples as the neurons and concluded the one-layer neural net’s learning hardness. 

The scope of the topic is a good fit for ALT. In regards of writing, this is clearly a solid theoretical paper. 

Some improvements or overhaul on the presentation can definitely be made by using more consistent notation among D,P,Q,h,A,\xi,z,y and their subscript and prime version. The notations are so convoluted given the technical complexity involved.  I will leave the confusions in writing to the end and move on to the technical part, some of which are probably also presentation related.

On the technical side:
I don’t understand the difference between \xi and \xi_0. I imagine \xi_0 is inherent inside phi and \xi is injected? If phi is Lipschitz and everything’s gaussian, injecting inside and outside should be similar? What’s the difference between F_xi and P_xi, only with no mod 1? Then how important is this mod 1? This is not explained clearly. An expert in CLWE setup might understand this but to an average person with no prior working knowletedge of it, this is far from clear. And where exactly in your algorithm did you use this injection and get P_phi? xi_i are what you injected? Or is P_phi just a proof artifact? 

The novelty on the construction of the NN is not clear. This is perfectly fine by itself since it reduces the proof into proving the neuron case, but less so if that’s a defining differentiating feature from Song et al. Do you believe there’s something inherently hard to apply Song et al.’s approximation or it’s just you don’t know how. Have you tried other approximation results, or you don’t find any other works to be directly applicable? I did not see you mention any papers other than Song et al. If you have tried, maybe cite them and say they won’t work and why, which definitely boost the novelty! Right now, I tend to believe the novelty here on this construction is on the more incremental side. The neuron’s reduction is interesting, but it suffers presentation issues. Technically, this work is solid, not trivial but nothing ground breaking.

The following are some other confusions I had when reading this work. 
Page 1: Long line of research with no citation? 
Page 2: ``which arguably limits the generality of such an unconditional lower bound” Should cSQ be conditional lower bound as the authors suggested earlier? ``Specifically, in terms of conditional lower bounds… the so-called correlation Statistical Query (cSQ)...”
I suggest either add a few lines briefly explaining ``Cryptographic assumptions” somewhere early in the presentation, or refer the author to CLWE definition explicitly when refering to cryptographic assumtions. They were mentioned too many times, some are even with no explanation. ``cryptographic assumptions (specifically the learning with rounding assumption)” 
Page 3: I suggest you modify ``the value of any such brittle algorithmic method in learning or statistics is unfortunately unclear”. Saying ``practically a non-negligible amount of noise always exists in these cases” or something like this should be enough to convey the idea, which I totally agree. I’m not affiliated with any of those authors nor offended. But what you wrote still seems unnecessarily rude and arrogant. 
Fcal^NN_k has no equation number but is refered later on as (3). And in its definition, what is w_j and W?  Reader can guess via dimension but specifying it should be better. 
Def of CLWE: Writing a sequence of decision problems {CLWE}_d is confusing (also the class notation). You are never considering a sequence of problems in this paper. You have one instance. Just say you are in the regime where d grows, and others depend on d may just be cleaner or am I missing something here?"
316,"Brief Overview of the Paper
The paper presents a novel approach to predicting Myocardial Injury after Non-cardiac Surgery (MINS) using a Retrieval Based Disease (RBD) prediction framework. This method innovatively addresses the challenge of analyzing complex, unstructured clinical data by transforming it into a coherent text description for improved decision-making accuracy. The authors claim their Language Model (LM)-based model outperforms traditional Machine Learning (ML) models, offering promising results that suggest further validation is needed in larger datasets or new clinical scenarios with different disease prevalence.

Quality
Technical Soundness and Description
  The methods and accompanying figures presented by the authors are technically sound and well-described, providing a clear overview of the proposed RBD framework and its advantages over traditional ML approaches. The incorporation of various types of clinical data into a unified text-description model showcases a significant advancement in handling and interpreting complex data for disease prediction.

Addressing Data Imbalance:
  While the authors acknowledge the challenge of working with an imbalanced dataset due to low disease prevalence, the paper would benefit from a more detailed discussion on how to address this issue. Suggested improvements include implementing resampling strategies, such as oversampling the under-represented disease class, to mitigate the imbalance's impact. Additionally, incorporating Receiver Operating Characteristic (ROC)-Area Under Curve (AUC) comparisons among the models could provide deeper insights into their performance, especially in managing false positives and negatives in an imbalanced dataset context.

Clarity and Justification
 The statistical methods used in the paper are clearly explained and justified. The authors' approach to analyzing the data and the results presented are sound, offering a solid foundation for their conclusions.

Recommendation & Significance 
This paper introduces a compelling and innovative approach to disease prediction using LM-based models, addressing significant challenges in analyzing unstructured clinical data. While the results are promising, addressing the highlighted areas for improvement, particularly around data imbalance and statistical analysis depth, could significantly enhance the paper's impact and validity. Further validation of the proposed model in broader clinical settings is recommended to substantiate its effectiveness and applicability in real-world scenarios.

Additional Comments
Strengths:
- The novel RBD framework represents a significant innovation in leveraging LM-based models for disease prediction, especially in dealing with complex, unstructured clinical data.
- The initial findings suggesting superior performance of the RBD framework over traditional ML models are promising and warrant further investigation.
Weaknesses:
- The paper lacks a comprehensive strategy to address the issue of data imbalance, which is crucial for validating the model's effectiveness across different clinical scenarios.
- The statistical analysis would benefit from more detailed comparisons between models and a clearer justification for methodological choices, such as the selection of k-values.

Recommendations for Improvement:
To strengthen the paper's statistical analysis, it would be beneficial to include a direct statistical comparison of the models' recall and F-1 scores. This comparison could highlight the proposed model's effectiveness more clearly against traditional ML approaches. Additionally, the rationale behind selecting a k-value of 5 for certain analyses requires further explanation. Expanding on this decision could enhance the reader's understanding of the methodology and its implications for the study's findings."
317,"The paper is really well-written and provides all the necessary details. I very much enjoy reading it!

It is also very impressive to see that classical methods can achieve promising results on some tasks in the deep learning era. 
I only have two minor ments:

- There are lots of classical methods. Could you please add a paragraph to describe the motivations to determine the classical method for specific segmentation tasks? 
- Could you please also comment on graph cut and active contours, which are also popular traditional methods?"
318,"The authors have introduced UQ-PINN's for uncertainty quantification of the inherent noise present in the system. They have introduced a simple framework by adding multiple outputs to the PINN and modifying the regular MSE loss function with negative loss likelihood.

The method is not exactly novel and is widely used in traditional probabilistic deep learning community.
The example shown in the paper is quite simple and straightforward. It might be better if more complicated examples with PDE's are shown. It is natural since PINN's were first introduced to solve PDE's."
319,"The paper explores advancing artificial intelligence by addressing its limitations in generalization through the Abstraction and Reasoning Corpus (ARC), a benchmark for logic-based tasks requiring human-like reasoning. It critiques the narrow capabilities of current AI models, highlighting the neurosymbolic DreamCoder system for its structured reasoning and LLMs like GPT-4 for their adaptability while advocating for hybrid approaches that combine their strengths. The authors propose enriching AI capabilities through data augmentation, including synthetic datasets and human trial observations, and developing math-inspired neural architectures that embed logical rigor. They also emphasize human-AI collaboration, suggesting interactive frameworks where humans and machines jointly solve ARC tasks by leveraging complementary strengths. 

Key points to address:
1. The paper heavily theorizes without presenting empirical results or concrete benchmarks for its proposed hybrid models or math-inspired architectures. This omission weakens the argument for their efficacy.
2. Drawing analogies with AlphaGo's approach and mathematical discovery seems speculative without real evidence that these strategies would generalize to ARC, which differs fundamentally from Go in structure and problem-solving requirements.
3. While the proposal for human-AI collaboration is cool, it lacks practical implementation details, such as how interactive interfaces would function or how the collaboration pipeline would be evaluated.
4. The paper overlooks the unique challenges of ARC, such as its focus on abstract transformations that defy straightforward data-driven solutions. This weakens its proposals for data augmentation and math-inspired architectures, which may not align with ARC’s core demands.
5. The paper blends philosophical aspirations of AI generalization with technical proposals without a clear roadmap for achieving its goals."
320,"Summary of the paper:
The abstract describes the use of NLP to detect healthcare provider behaviors aligned with common factors theory in psychotherapy. Common factors theory emphasizes building empathy, trust and positive relationships through provider skills like reflective listening and appreciation. While the clinical importance of this is paramount, I have concerns with the content that has been presented in the abstract. 

Major Comments:

This is a great problem statement. However, I have a couple of major comments:

1. There is ambiguity in the description of the exact methods, vague terms such as machine learning and natural language processing are used. 

* They mention the use of “synthetic and generative technologies to expand specific labeling strategies and data curation by generating and validating rare use cases” but don’t describe this - what was the generative model that was used, how did they verify its realism to simulate rare use cases, how much synthetic data was generated relative to non-synthetic data etc. More details need to be provided since this can have significant impacts on the quality of their model.

* “Machine learning methods were used to create natural language processing models based on conversational training data”. What was the base NLP model, did the authors fine-tune a model such as LLaMA? What specific machine learning method was used - NLP fine-tuning strategy needs to be described.
2. The authors mention that they will report results of benchmarking their model on the HOPE dataset, but they don’t do so within the paper. 

The overall clarity of the abstract is low due to the above concerns.

Minor Comments:

1. They have not adhered to the AAAI submission format.
2. They use the phrase “using machine learning with natural language processing”. NLP is technically a subfield of ML, and this statement needs to be revised to reflect that."
321,"Summary of Contributions:

The work proposes a new way to efficiently fine-tune SAM for medical image segmentation, i.e. a custom lightweight ConvNet head after the SAM encoder. The SAM encoder undergoes parameter efficient fine-tuning by using Low Rank Adaptation. The authors claim and demonstrate that fine-tuning SAM is better than fine-tuning MedSAM, i.e. for successful adaptation, medical pretraining is not necessary.

Strengths:
1. The authors challenge the popular belief that foundation models should be pre-trained with data from the domain where they are to fine-tuned, and successfully demonstrate that (in their own words) “generalist models like SAM need not be abandoned in favor of models with medical pre-training” 
2. The work experiments with various decoder networks for fine-tuning SAM and MedSAM.
3. The paper is easy to follow and the Block Diagrams are representative of the methods provided in the work.
4. Preprocessing steps, training hyper-paramters, along with memory requirements have been provided.

Weaknesses:
1. The work does not provide quantitative comparisons with specialist models (i.e. models trained from scratch on the given dataset, say UNet, which inspired the Convent decoder)
2. The work only provides fine-tuning results for a single dataset. Ideally two more medical image segmentation dataset (such as BraTS, VerSE, etc.) should be included.

Questions, Suggestions, Comments:
1. What are the input and output dimensions for the images?
2. It is not very clear that how the decoder from UNet is used, more specifically: the UNet decoder uses a prior from the encoder after each upscaling step (in the form of residual connections). What priors (if any) are being used in the proposed decoder ConvNet?
3. Addressing the weakness will definitely improve the quality of the work, and make the author’s claim more substantiated."
322,"Paper Summary:

The authors propose an approach to pruning the model size by performing a series of ablations starting from a LiteMedSAM baseline model. To achieve this goal, there are several levers to adjust: First, reduce the block depth of different blocks of the image encoder; second, they tackle the mask decoder by reducing the transformer depth from two layers to one layer and reducing its MLP dimensionality. On top, the authors reduce the number of attention heads from 8 to 4. Finally, the depth of the IoU-head was lowered from 3 to 2. All these changes are reflected in the architecture SwiftMedSAMv1. The authors also propose SwiftMedSAMv2, in which they prune even more aggressively.

Paper Strengths:

Pruning the model to an ideal size is a meaningful idea as it not only reduces training but also inference time.
Using additional allowed datasets (e.g., AutoPET III) to improve the balance of available images is a reasonable approach.

Paper Weaknesses:

The paper lacks key information in certain sections:
What do the authors exactly mean by “block depth”?
Which component are the authors referring to when talking about the IOU-head depth? Is this the MLP in the final stage that predicts the IoU scores?
Addressing these components via a figure would greatly facilitate the understanding of what the authors changed. Fig. 1 is not really helpful as it shows the architecture on too high a level.
The information on how the authors trained their pruned models is also rather short. Did they start with randomly initialized weights or use some form of image distillation? Did they use the weights of the pruned larger models?
Results: It’s a bit unclear how the results of the baseline as evaluated by the authors lead to such bad results. Assuming they used LiteMedSAM as provided by the organizers of the challenge, their reported results on the CT and MRI domains are drastically different from what others have reported (e.g., 40.71 for dice vs. ~92). In other domains e.g., Ultrasound, the results are similar. This raises the question of the evaluation protocol which the authors used and should be clarified.

Minor: spelling mistake in the first sentence of Sec. 2.2


Overall, I like the idea of pruning the model leading to a better tradeoff of accuracy and efficiency, but I feel the paper needs some improvements to better convey this idea and provide the necessary details to better understand what the authors did. Also, the large difference in the reported results is concerning."
323,"The authors show a new method learn the delay parameter alongside 'normal' parameters of neural delay differential equations by proposing  a new algorithm to calculate the adjoint for neural delay differential equations. With two simple examples, the authors prove that their algorithm can retrieve the data generating delay parameter and normal parameters.

This work could be a great starting point for developing neural delay differential equations further. The authors give some examples for systems governed by delay differential equations and I agree that the topic is highly relevant. Here, the relevance could have highlighted better by finding prominent examples from different domains. The work is original and can advance the use of neural differential equations to new areas by allowing for delay parameters. A point that may be missing is an ablation experiment that checks if a delay of 0 is found when the neural delay differential equations is confronted with data generated with an ODE. 

Pro:
- extends neural ODEs to delay differential equations
- seems to be the first time that adjoint for neural delay differential equations is described
- delay parameter can for the first time be estimated in neural delay differential equations

Con:
- some sloppiness with respect to formatting citations - (Author, Year) formatting is not used when appropriate. This makes reading a bit tedious at times."
324,"Strengths:
1. Compares constraint satisfaction in LLM which is important for consistency.
2. Authors try both prompt and decoder-based techniques.

Weakness:
1. Literature review missing key papers
    a. Synchromesh - constrained generation for Code Generation (https://openreview.net/forum?id=KmtVD97J43e)
    b. Format5 - constrained generation and pretraining (https://arxiv.org/abs/2310.17306)
    c. DataVinci - constraint guided semantic data repair (https://arxiv.org/abs/2308.10922)
2. The paper does not introduce any ideas and is an amalgamation of existing techniques. The findings like in-context examples help improve performance are already well known.
3. The paper should have more examples of success and failure cases for different techniques. Especially since it is an analysis paper.

Questions:
1. What was the prompt used for decoding strategies. Based on the poor result on standalone and improvement with prompting it looks like the prompt is to blame.
2. It would be interesting to see if finetuning can help here.

Formatting
1. related work section has a todo in header
2. missing related work as highlighted in weakness
3. for the dog example from commonGen, it would be good to show an example of a high score result."
325,"## Summary
This paper proposes a pipeline to segment multi-modality medical images efficiently on CPUs. This pipeline consists of three main components: a ViT-Tiny based SAM model, extensive data augmentation and sampling, and model quantization. The authors improve the training data diversity and richness through various data augmentation transforms; to address the modality imbalance in the training dataset, they oversample and augment modalities with less training data. However, there is no such information in the paper on which modalities are oversampled and what the augmentation ratio is. Furthermore, for model quantization, it is unclear whether the authors convert a pre-trained model into an 8-bit integer or fine-tune it before quantization. After this quantization, to address the accuracy drop caused by the low precision of an 8-bit integer model, authors fine-tune the quantized model using a calibration dataset, but there is no clue in the paper what this dataset consists of. Overall, this paper lacks methods and implementation details, many statements miss citations or references, and academic writing skill needs to be improved. The GitHub link and code are not provided. The completeness and reproducibility are NOT guaranteed.

## Detailed Comments
**Abstract:**  
1-	A good summary of the proposed method. But the DSC or NSD of the validation set were not included.

**Introduction:**  
2-	In the first paragraph, “making it difficult for traditional segmentation methods” - what is traditional segmentation methods? There is no reference or citation. The next sentence, “Secondly, traditional segmentation models typically require significant computational resources…”, no reference or citation to “traditional segmentation models” makes this paragraph less coherent and hard to read. The last sentence mentioned “reducing healthcare costs” seems meaningless.

3-	In the second paragraph, it seems the authors would like to introduce medical image segmentation models, but the models they mentioned – SAM, MobileSAM, and EfficientViT-SAM are not proposed to segment medical dat. Although the authors listed some SAM based models, they did not introduce these models. Related works were inadequately demonstrated.

**Methods:**  
4-	In the first paragraph, “This imbalance might lead to poor generalization of the model when segmenting different modalities, as the model may tend to favor processing modalities with larger data volumes” This statement needs more demonstration and citation to prove why and how.
5-	The authors conducted a statistical analysis but did not mention what dataset was used, and no tables or figures show the analysis results. The authors included the same data augmentation methods in both the pre-processing session (the second paragraph in session 2.1) and training protocol (session 3.2), was the augmentation done twice or pre-processing and augmentation were duplicated.

6-	“In the field of medical imaging, data augmentation is more crucial than image formatting.” What is the point of mentioning image formatting here? “Random rotation enhances the model’s adaptability to irregular scan data, while random contrast adjustment helps generalize the image performance under different voltages, currents, and radiation doses.” This is a strong argument. Did you conclude this from your experiments? If not, please cite.

7-	""ViT-Tiny Replacement for Image Encoder"" needs more explanation on why you need to replace SAM encoder with ViT-Tiny. What is the problem with SAM encoder, is it slower or heavier than ViT-Tiny?

8-	The method figure (Fig 1) does not include all necessary information in the methods and is less informative. What data were used to fine-tune the quantized model?

**Experiments:**  
9-	The authors did not mention what and how much data they used for model development, as they said, “external public datasets”.

10-	In data sampling strategy, the author used modality weighting techniques, but they did not list what weights were assigned to what modalities.  
11-	In the optimal model selection criteria part, the authors did not introduce what dataset they used to assess model generalizability and what the result is. In the Table 2, the authors listed SAM and MedSAM for training. It is confusing why this sudden appearance of MedSAM, as in the previous content authors only said they used SAM. Importantly, what is the training set and how much data used are unclear.

**Results and discussion:**  
12-	In the Table 3, typo for “Dermotology”. Table 3 using % for unit but DSC and NSD scores are out of 1, not 100. Unclear qualitative results, it would be better if having their ground truth to compare. ""As shown in Table 4, our method generally achieves faster segmentation speeds compared to baseline methods, especially when dealing with 3D image data."" There're cases that proposed model performed worse than the baseline so ""generally"" might not be appropriate here.

Checklist table  
-	Wrong number of authors  
-	There is no table 7 and table 8 in the paper.   
-	Please add table number for Efficiency evaluation and figure number for Visualized segmentation example.  

Please add Abbreviated paper title and First Author Name to headers."
326,"This paper is the first to study self-directed learning for the classification of vertices in a given graph. The underlying assumption is that the (connected) graph is fully given, and that each class of vertices forms a convex cluster. Initially, it is assumed that there are only 2 clusters. The authors designed a self-directed learning algorithm that runs in polynomial time in the number n of vertices and whose number of mistakes is upper-bounded by a function logarithmic in n and polynomial in the Hadwiger number of the given graph.

Some variations on the problem are discussed. For example, when convexity of clusters might be violated, the mistake bound of the proposed algorithm increases by at most 4 times the smallest number of vertices to be moved into the opposite cluster so as to fulfil the convexity criterion. In addition, for learning special classes of graphs, a few simple observations can be made on the number of mistakes. The multi-cluster case is also discussed (mainly in the appendix). Finally, it is observed (again with a very simple argument) that efficient self-directed learning is always possible with at most as many mistakes as there are boundary vertices (on the boundary of the clusters).

I consider the observations made from Theorem 9 onward fairly straightforward; the main result is clearly Theorem 8. However, I still think that the paper is worth publishing, and I am not a fan of requiring big muscle-flexing in order to have a paper accepted. The proposed algorithm makes use of structural properties of the underlying graph (sparsity in the sense of lack of K_z minors except for small values of z); these insights might be helpful for others working on learning graphs.

Still, some issues bothered me a bit:
(1) I appreciated the discussion of potential application domains, but was wondering when we can really assume a small Hadwiger number. For example, in social networks, might it not happen that the Hadwiger number becomes large enough to make the mistake bound large?
(2) Secondly, the authors state that their algorithm runs in polynomial time, which is certainly true. However, it needs to calculate shortest paths between lots of pairs of vertices, which, in large graphs like social networks may not really be feasible. I was wondering whether the authors have looked at the problem of trying to bring down the computational cost of their algorithm as far as possible. Perhaps a lot can be shaved off in terms of runtime at only a small cost in terms of mistakes of the self-directed predictor?
(3) For domains in which the Hadwiger number is prohibitively large, can one perhaps still salvage the algorithmic idea if one assumes that all vertices that belong to a large clique are in the same cluster? Such assumption may be fairly realistic in social networks, perhaps. I think this could lead to an interesting extension of the results presented in this paper.

Some minor comments for improvement of the writing:
- The term ""$\varepsilon$-good"" suggests that $\varepsilon$ is an external parameter that can be set independently of everything else. But, if I understood correctly, $\varepsilon$ is determined by the given subset $U$ of the vertex set. Would it be more appropriate to call a vertex $U$-good instead of $\varepsilon$-good, and then simply call it good if $U$ is clear from the context? I also noticed that the authors go back and forth between the terms \emph{good} and \emph{$\varepsilon$-good}.
- In line 6 of page 8, it says ""indicating that no mistakes have been made yet."" technically, shouldn't it say ""indicating that no mistakes have been made yet (except possibly when predicting the label of $a$).""?
- line 5 of Section 1: total number *of* classes
- overfull hbox on p 4
- p 5, last line: delete ""the"" before $U$
- in Def 5, $\varepsilon(U)$ is introduced and never used; elsewhere it just says $\varepsilon$.
- Observation 6 is hard to parse, probably due to syntax mistakes. I think the authors wanted to write ""and *any* node participating in the biggest number of good quadruples is *an* $\varepsilon$-good node.""
- 4 lines above Thm 8: when *the* induced
- 1 line below Thm 8: to *an* algorithm
- 5 lines below Thm 8: that *an* $\varepsilon$-good node *exists*
- 6 lines below Thm 8: ""such as"" should be ""such that""
- p 8 line -3: among *the* first; also delete ""an"" after ""at least""
- p 8, line -2: delete ""among first"" or fix grammar
- p 9, line 1: will *make* no mistake
- line after Prop 12: circular-arc *graphs*
- references often have capitalization missing in journal names and conference names.
- p 21, last line: at least *an*
- p 27, line -5: We *predict*"
327,"Pros:
- The paper is well-structured and provides a comprehensive overview of the development and evaluation of the approach. The methodology is also clearly presented.

Cons:
- In the introduction, you have mentioned MobileSAM, EdgeSAM, EfficientSAM, and EfficientViT-SAM as examples of lightweight models and stated that these models suffer from performance drops. This is not true for EfficientViT-SAM. Its benchmarks show that it has competitive results compared to SAM-ViT-H, which is the largest variant of SAM.
- In the data preprocessing section, there are 11 modalities in total, it seems like you have missed the microscopy type."
328,"The paper discusses leveraging Large Language Models (LLMs) for survival analysis in medical prognostic and diagnostic applications. It explores methodologies for estimating medical risk, with a focus on survival analysis, addressing the challenges posed by censoring in medical studies. The document reviews current LLM strategies for survival analysis, detailing their limitations and strengths, and proposes an open-source implementation for comparing these strategies. It aims to develop evidence-based recommendations for the effective use of LLMs in estimating patient survival outcomes.

**Pros**
- Comprehensive Approach: The paper provides a thorough overview of using LLMs for survival analysis, covering various methodologies and their adaptations for this specific application.
- Practical Contributions: By offering an open-source implementation, the work facilitates practical experimentation and comparison of different LLM strategies for survival analysis.
- Addressing a Crucial Challenge: The paper tackles the significant challenge of censoring in survival analysis, proposing methods to improve modeling under such conditions.

**Cons**
- Generalization of Findings: The paper might benefit from a broader evaluation across different types of LLMs and datasets to ensure the findings' generalizability.
- Detailed Evaluation Framework: While it proposes an open-source implementation for comparison, the paper could provide a more standardized evaluation framework for assessing the performance of LLM strategies."
329,"Summary:
This paper presents an innovative approach to improving the performance of Vision-Language Models (VLMs) in mathematical reasoning and visual understanding tasks. The authors propose a task-specific captioning pipeline that extracts keywords from the question, generates targeted image captions, and uses these captions as prompts to guide the VLM in solving complex problems. The approach is evaluated across multiple datasets and demonstrates promising improvements in accuracy and robustness.

Key Contributions:
* Generates targeted captions using keywords extracted from questions and integrates these captions into the reasoning process.
* Evaluate the pipeline's performance against adversarial approaches to ensure reliability.
* Tests on datasets involving geometry, counting, algebra, and mathematical reasoning to assess generalizability.

Strengths:
* The proposed captioning pipeline improves VLM performance on mathematical and reasoning tasks compared to baseline methods.
* By generating targeted captions and providing task-specific guidance, the method encourages VLMs to focus on visual content, improving their reasoning abilities.
* The pipeline demonstrates consistent improvements across multiple datasets and tasks, including geometry, counting, and algebra.

Limitations:
* The study evaluates only Vision-Language Models (VLMs) and does not compare its approach to other state-of-the-art methods outside the VLM domain, which could provide a more comprehensive understanding of its effectiveness.

* The experiments are confined to smaller datasets and open-source models, which limits the generalizability of the findings to larger-scale, state-of-the-art VLMs such as GPT-4.

* Generating captions from the query introduces potential challenges, as the quality of the captions depends on accurate keyword extraction and relevance to the question. Poorly structured queries or ambiguous keywords could lead to suboptimal performance.

* The study does not provide an analysis of the errors made by the framework. Identifying and categorizing these errors—whether they arise from the captioning process, reasoning steps, or question formulation—could offer critical insights for refining the approach.

The paper would benefit from additional refinement and improvements before it is ready for publication."
330,"Summary:

The paper introduces APPS, a method for data-driven differential model discovery that employs active learning. APPS avoids the need for the pre computation of large training dataset by actively querying informative data regions, guided by phase portraits of candidate ODEs.

Major concerns:

- The extension of context-free grammar to represent an ODE as a sequence of grammatical rules is not entirely new, but it builds on previous works on symbolic model discovery; it should be better highlighted what the authors' contribution is in this regard.
- Exploration of generalization properties appears limited, particularly with respect to long-term prediction and extrapolation beyond the training time interval. In addition, it would be useful to analyze the generalization capabilities of the framework with respect to discretization steps different from those used during the training procedure.
- A more in depth analysis of training times related to algorithm 1 would be appreciated.
- The test cases considered are not ""large-scale."" I recommend that the authors apply the proposed framework—or at least discuss and comment on its application—to an ODE derived from the semi-discretization of a PDE, enabling it to handle systems with 1,000 to 10,000 or more DoFs."
331,"In this submission, the authors introduced novel tools for deriving enhanced $H$-consistency bounds in various learning settings, including multi-class classification, low-noise regimes, and bipartite ranking. They established substantially more favorable guarantees for several settings and demonstrated unexpected connections between classification and bipartite ranking performances for the exponential and logistic losses. In general, finer and more favorable $H$-consistency bounds are established.

The authors have clearly stated the new enhanced $H$-consistency bounds as well as its applications to different scenarios. The structure is clear and the authors provided all the detailed mathematical proof for all the theoretical analysis.

The only weakness and suggestions is that the authors should explain more about the difference or improvement between the proposed enhanced $H$-consistency bounds in this submission and the previous $H$-consistency bounds in the related works. Theorem 1 and theorem 2 generalize previous results from (Awasthi et al., 2022a,b), which can be recovered as special cases when $\alpha = 1$ and $\beta = 1$. The authors argue that compared to earlier approaches, these new tools offer more precise $H$-consistency bounds in familiar settings and extend them to new scenarios where previous methods are insufficient.

Could the authors explain more about the reason why earlier approaches didn't work well for some settings and previous methods can not be applied to some scenarios? The new enhanced $H$-consistency is applied to multi-class classification, binary classification, and bipartite ranking with exponential loss, logistic loss and hinge loss. The authors should make a detailed comparison between the applications of the earlier approaches to these setting and the proposed enhanced $H$-consistency to these scenarios. The authors should provide the counter parts of Theorem 5 to 12 with applications of the previous $H$-consistency. Through this comparison, we can see the improvements of the novel method in this submission and significantly validate the argument made by the authors."
332,"**Paper summary** 

SAGE is a self-explainable graph learning framework designed to mitigate distribution shift by “compressing” rather than discarding irrelevant structures, ensuring more stable explanations. By enforcing a lower bound on attention weights and refining parameters through branch optimization, SAGE achieves strong predictive performance and meaningful, interpretable explanations on multiple benchmark datasets.

------

**Originality**

* **Strengths**: SAGE introduces a novel method to control information compression by setting a probabilistic lower bound on edge attention scores, avoiding abrupt distributional shifts caused by pruning. Its branch optimization technique refines model parameters in a straightforward manner without disrupting the main training loop. 

* **Weaknesses**: While the concept of controlling information compression is interesting, it is somewhat incremental over other IB-inspired methods. The idea of partial preservation might be seen as a heuristic. The method relies on a hyperparameter r that must be manually chosen and may vary with the dataset.

--------

**Quality**

* **Strengths**: The technical derivation is sound. The model employs a GIN backbone, Gumbel-softmax reparameterization for edge selection, and a KL divergence penalty that encourages attention weights to approximate a predefined distribution. The experiments show performance gains that support the claim.

* **Weaknesses**: The theoretical motivation for why this partial compression (instead of fully dropping edges) leads to better performance could be explored more deeply. The paper lacks rigorous theoretical analysis of how the chosen distribution boundary (r) interacts with distribution shift. The reasoning is intuitive but not extensively justified.

---------

**Significance**

* **Strengths**:  Improving self-explainable GNN performance is a meaningful contribution. The problem of distribution shift is a recognized challenge. Addressing it by partial compression of noisy information could open a new line of thought for balancing model fidelity and interpretability. For practitioners looking for interpretable GNNs with minimal performance loss, this could be valuable.

* **Weaknesses**: Although the results are good, the performance improvements are not always dramatic except on certain datasets (e.g., NCI1). The method’s broad applicability and how it compares to a wide range of other explainability techniques (e.g., recent state-of-the-art methods) is not thoroughly discussed. More extensive baselines or complexity comparisons would add to significance.

-----------

**Questions and suggestions for the authors**

* The performance appears sensitive to the choice of r. Could the authors provide a heuristic or automated method to select r without heavy tuning?
* Could the authors provide more theoretical insights into why partial compression of noisy edges leads to improved performance and reduced distribution shift? For instance, can they characterize how r affects the divergence between compressed and original distributions mathematically?
* The experiments compare with a handful of methods. It would be helpful to see how SAGE compares against a broader range of self-explanatory or post-hoc methods to strengthen claims about general efficacy.
* The training process includes branch optimization and involves Gumbel-softmax sampling. How does the runtime and computational complexity scale with the size of the graph? Are there any memory constraints?

______

**Limitations** 

The authors mention distribution shift but do not provide a formal definition or metric beyond similarity in embeddings and a heuristic. While they test multiple datasets, the approach relies on a hyperparameter (r) that must be tuned. The branch optimization step increases computational load and might not be feasible for very large graphs. Another limitation is that while the model provides attention scores as explanations, the granularity and faithfulness of these explanations depend heavily on how well the attention aligns with truly causal substructures. There is also no strong theoretical guarantee provided that the edges identified are indeed the “correct” explanations.

_______

**Ethics**

There are no obvious direct ethical concerns related to the method as it stands. The paper does not deal with sensitive data or produce sensitive content. The approach is a method improvement and not directly involved in human-facing decision-making applications at the evaluation stage. No unethical dataset or methodology usage is apparent. Thus, no ethical issues need to be flagged for special ethics review."
333,"$\textbf{Model Definition.}$ The model proposed by the paper is as follows. A learning algorithm is required to behave optimally within a markov decision process (MDP). At every step, the learning algorithm can either make a decision on its own, or query an oracle that provides information of the optimal action associated with the current state of the MDP. While the algorithm is required to make no mistakes, the goal is to minimize the total number of oracles used within T time steps.

$\textbf{Results.}$ When the oracle is noiseless, the authors show that the query complexity of the learning model is sharply characterized by the eluder dimension of the class of possible policies of the MDP. 
When the oracle is noisy but outputs the optimal action with probability larger than those of the other actions, the authors show that the agent could still succeed in making 0 mistakes with high probability, but the query complexity will depend on the specific structure of the noise added. 
In particular, the authors give upper and lower bounds for both Massart noise (when the probability of the optimal action exceeds by those of the others by a fixed margin $\Delta$), and Tsybakov noise (when the optimality gaps for certain states can be arbitrarily small as long as these states rarely occur in possible trajectories of the learner). The bounds do not match with each other tightly but have the same linear dependency on the eluder dimension of the policy class. Overall, I think the work is a good contribution and leave many interesting future directions.

$\textbf{Strengths.}$ The model is natural, well motivated, and has close connections to other parts of learning theory including reliable learning, active learning and imitation learning. The algorithm is a natural implementation of the principle of disagreement-based learning, and is shown to be optimal in the realizable case. 

$\textbf{Weakness.}$ There are gaps in the query complexity bounds for both the Massart noise and Tsybakov noise model. For Massart noise, the upper bound is given by the product of the eluder dimension and a statistical estimation term (related to the failure probability $\delta$ and the optimality gap $\Delta$) while the lower bound is the sum of the two. Similar issues also appear in the bounds for Tsybakov noise. The authors note that similar issues persisted in the active learning literature for a number of years, suggesting that resolution to the gap may require novel and non-trivial technical ideas.

$\textbf{Question.}$ The authors conjecture that the lower bound for Tsybakov noise is more likely to be optimal. Do the authors believe that the lower bound for Massart noise is optimal?
Suppose the policy class is completely unstructured. Doesn’t that mean the agent is forced to learn the best action for each state from the oracle separately, which requires $|S| / \Delta^2$ many queries. It seems to me this is suggesting a stronger lower bound may exist?"
334,"# Overview

The manuscript proposed a new framework to improve the performance of active learning, specifically for medical image classification. 
The proposed method is based on a self-supervised model and a label-irrelevant patch augmentation scheme. Plenty of ablation studies have been conducted, showing the effectiveness of the proposed method.

# Comments

(+) Thorough ablation studies are carried out, providing a deep understanding of the method's effectiveness.

(+) The method is articulated clearly and supported with descriptive diagrams, enhancing comprehension.

(-) Regarding the results presented in Table 2, it's unclear if the FLOPs consider the overall costs or only account for individual rounds. Given that every AL round necessitates training a new adapter based on the current encoder, the overall computational overhead seems not negligible.

(-) The abstract mentioned, ""to reduce redundancy in the learned features and mitigate overfitting during the AL process"". Are there specific results that demonstrate the successful mitigation of overfitting?

(-) The advancements illustrated in Table 1 appear to be relatively mild. Incorporating error bars might offer a more effective assessment of the proposed method."
335,"This paper takes on an analysis of ""comparative learning"" which is an extension of PAC learning proposed recently by Hu and Peale (2023). The setting of comparative learning (as explained by Hu and Peale, 2023) considers a pair of hypotheses classes, a source class $\mathcal{S}$ and a benchmark class $\mathcal{B}$; so that realisable learning can be thought of as the case where $\mathcal{S}$ constrains the potential hypotheses that the ground-truth labeling is generated from, and agnostic learning can be thought of as the case where $\mathcal{B}$ constrains the set of hypotheses that the model output by the learner will be compared against. While classical PAC learning considers a single hypothesis class, Hu and Peale (2023) argue that separating these two roles of the hypothesis class, by postulating two separate classes as described, can lead to new ways to study and characterise the sample complexity in more learning tasks than can be studied via the classical (single hypothesis class) PAC learning setting.

As far as I can see, this paper under review proposes to further the work of Hu and Peale (2023), particularly the sample complexity analysis. The paper presents upper and lower bounds on the sample complexity of comparative learning, for each of two setting called respectively 'proper' and 'improper' learning. The said bounds depend on error and confidence parameters, and some quantity depending mutually on classes $\mathcal{S}$ and $\mathcal{B}$. The work of Hu and Peale (2023) proposed the mutual VC dimension, and this paper under review proposes some new quantities (mutual graph dimension, mutual star dimension, and so on).

I have assessed the paper against the following criteria, with the following outcomes:

- **Quality/clarity/readability:** The paper is relatively well-written. Some minor typos (see my detailed comments below). However, I have no big concerns with respect to these criteria. I am confident to say that the paper is readable for the most part.

- **Originality and significance:**  It would appear that this work is re-packaging and perhaps a small increment from that of Hu and Peale (2023). I am open to stand corrected on this, please let me know if I missed something. However, at least this indicates that there is a need to articulate in a clear way the novelties and/or improvements with respect to Hu and Peale (2023). For instance, if what this work calls ""proper"" comparative learning  (where the learner is required to output a predictor from the benchmark class) is a novel setting here, which was not considered by Hu and Peale (2023), this needs to be highlighted prominently and so does the associated analysis.
Also I'd like to ask the author(s) to address this question: Is not the gap between your upper and lower bounds on sample complexity for comparative learning, in terns of dependence on the error parameter $\epsilon$, the same as the gap in the corresponding bounds of Hu and Peale (2023)? It would appear that your lower bound scales with $\frac{1}{\epsilon}$ and your upper bound scales with $\frac{1}{\epsilon^2}$. The same can be observed in the bounds of Hu and Peale (2023). In that case, the sentence about this gap in your abstract, and similar later in the introduction, is misleading and potentially creating the wrong expectation, as in making believe that this gap is closed somehow by the work under review, which appears to not be the case, judging from Theorem 4. 

- **Correctness:** I have a concern about this. For the setting called ""general comparative learning"" it appears that the lower bound (Theorem 13) scales with $\frac{1}{\epsilon^2}$ and the upper bound (Theorem 12) scales with $\frac{1}{\epsilon}\log(\frac{1}{\epsilon})$. Just from this one could conclude that there is something wrong with the arguments. 

- **Suitability for ALT:** The problem setting appears to be of interest to the ALT readership.


As requested, here is a **summary of pros and cons:**

>**Pros:**
>- Mostly well-written paper, and content is suitable for ALT.
>- Addressing an interesting problem, and motivations and discussions are generally believable.

>**Cons:**
>- Unclarity about the scope of the contributions and novelties with respect to Hu and Peale (2023).
>- Inconsistency (?) between lower bound and upper bound for the general case (see ""correctness"" above).

Last, I share a list of **detailed comments:**
- Abstract and other places say ""error parameter 1/ϵ"" which looks strange. Error parameter is ϵ, I think. Perhaps this is meant to say something about the dependence of the bounds on 1/ϵ, either linear or quadratic.
- Introduction, line 4: ""all possible data-generating distributions."" (replace ""processes"" with ""distributions"")
- Four lines further down: delete ""so called""
- Couple of lines down: ""data-generating distribution"" (replace ""process"" with ""distribution"")
- Next paragraph: After commenting on PAC bounds being vacuous for deep learning, perhaps add a comment that there is some recent literature on PAC-Bayes bounds giving non-vacuous values for deep learning such as ""Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data"" (Dziugaite and Roy, 2017) and ""Tighter risk certificates for neural networks"" (Pérez-Ortiz et al., 2021).
- Next paragraph: Maybe "" label-separatedness"" should be "" label-conditional separatedness""?
- Next paragraph, first line: ""the PAC learning framework""
- Next paragraph: The case of comparative learning that is proper w.r.t the benchmark class is clear (meaning the learner is required to output a predictor from the benchmark class). However, the last line of this paragraph says ""For both general and benchmark-proper comparative learning"" and it is not very clear at this point what defines the case of general comparative learning. (At the beginning of Section 4 this is clarified: general comparative learning means that no properness requirement is imposed. I suggest to make this clear a this point in the introduction.)
- Next page, second bullet: ""as well as general benchmark-proper learning"" - now this created a complete confusion since before there were two cases, one being the benchmark-proper comparative learning case, and the other one being the general comparative learning case. So, then what is this ""general benchmark-proper learning"" ?
- Section 2, first line: ""standard statistical learning theory framework""
- Next paragraph: The note that ""a partial classifier can be viewed as a function $h : \mathcal{Z} → \mathcal{Y}$, where $\mathcal{Z} ⊆ \mathcal{X}$ is a subset of the domain"" needs to clarify that $h$ and $\mathcal{Z}$ are connected. In fact, $\mathcal{Z} = h^{-1}(\star)$. The point being that it needs to be clarified that $\mathcal{Z}$ is not the same for all partial classifiers.
- The names ""partial class"" and ""total class"" are creating more confusion than clarity, I think.
- Better to say ""class of partial classifiers"" than saying ""partial class"" (the latter evokes a subset of a function class).
- Next: Notation $S$ for dataset is clashing a bit with notation $\mathcal{S}$ for source hypothesis class. As in, the visual effect of seeing $S$ and $\mathcal{S}$ tends to suggest some connection between the denoted objects. Could this be avoided?
- Definition 1: There is inconsistency between the use of $n_{\mathcal{S},\mathcal{B}}(\epsilon, \delta)$ and $n(\epsilon, \delta)$.
- Also, I suggest the notation should be $n_{\mathcal{S},\mathcal{B}}(\epsilon, \delta)$ with $n$ and not the one with '$\mathrm{n}$' written currently in the notation $\mathrm{n}_{\mathcal{S},\mathcal{B}}(\epsilon, \delta)$.
- Same comment for the sample complexities with upper script 'gen' and 'prop' and 'ERM'
- Next page, in Observation 1: Maybe rewrite as ""There exist classes $\mathcal{S}$ and $\mathcal{B}$ of binary total functions such that"" (if that is the intended meaning)
- Corollary 8: Reformulate similarly to previous bullet, to clarify the intended meaning.
- ...
- The conclusion was kind of lame. Could an effort be made to produce an informative conclusion, with a recap of the highlights of the work, discussion of scope and limitations, and the take-aways messages that readers should remember from this paper.

%%% === POST REBUTTAL

The rebuttal has addressed my main concern. I've increased my score."
336,"Strengths:

1. Innovation in Weakly Supervised Approach:
WS-iFSD integrates weakly localized objects using Grad-CAM, magnifying class and image data, offering a fresh, data-efficient approach to iFSD.

2. Performance Preservation Strategy:
Freezing the backbone and detection head during meta-training to preserve base-category detection performance is strategically sound.

3. Strong Empirical Evaluation:
Substantial improvements on benchmarks (MS COCO, PASCAL VOC) validate the method’s efficacy over current approaches like ONCE.

Weaknesses:

1.  Novelty of this paper is questionable. The entire framework seems a combination of existing techniques, e.g., Grad-CAM for weak object localization, backbnoe freezing, etc. Despite the effectiveness, the contribution of this paper is incremental.

2. Weak Localization Concerns:
The reliance on coarsely inferred bounding boxes could be a source of training noise and might impact detection reliability in complex environments. More analyses on the impact form those noise is beneficial.

3. Computational Complexity:
Absence of a detailed discussion regarding computational costs with increased class and image data might overlook practical deployment challenges.

3. Generalization Discussion:
Further exploration of the model’s generalization and performance across diverse detection scenarios and datasets is needed to establish robustness.

Overall:
The manuscript presents an interesting, data-augmented weakly supervised approach for iFSD, demonstrating marked improvements over established benchmarks. However the novelty and contribution of this work is questionable, and delving deeper into potential challenges with weak localization and computational complexities, along with a broader analysis of applicability, would fortify the research. The work is promising but warrants further exploration in specified areas for comprehensive insights and applicability."
337,"This paper presents a new neural network pruning library implemented with Jax, called JaxPruner, with 3 tenets: (a) fast integration (b) research first (c) minimal overhead. The library has implemented the major components of network pruning, supporting different pruning criteria, schedules, etc. They also establish the results of many baseline pruning schemes. This work does not introduce new pruning algorithms. Instead, they offer a new pruning library with Jax, which could benefit the Jax & pruning community.

Pros:
1. First and foremost, there is no prevailing Jax library for neural network pruning. This paper bridges this gap, which could benefit many researchers, esp. those using Jax as their deep learning framework and are interested in pruning.
2. JaxPruner is featured by fast integration, research first, and minimal overhead, which could make the library easy to use.
3. JaxPruner also provides strong baselines in either traditional pruning (pruning a pretrained model) or sparse training (pruning at initialization).

Cons:
1. Some of the results look interesting but lack enough discussion. E.g., in Tab. 1, for the row ViT-B16+, SET and RigL beat Dense with the prolonged training, while underperforming Dense in the row ViT-B16. This is quite unusual. Did the authors double-check and confirm the results are correct? If so, why? More explanations or discussions are highly suggested. Unreliable baseline results could mislead the community.

2. The library implemented many baseline pruning schemes like RAND, SAL, and MAG -- these are different pruning criteria. There is another major group of pruning methods that use sparsity-inducing penalty terms for sparsity, such as [*1-*4]. I was wondering whether this group of methods can easily fit into JaxPruner?

3. Typos: Line 258. with block sparsit; -> sparsity.

- [*1] 2016-NeurIPS-Learning Structured Sparsity in Deep Neural Networks 
- [*2] 2018-ICLR-Learning Sparse Neural Networks through L0 Regularization
- [*3] 2021-ICLR-Neural Pruning via Growing Regularization
- [*4] 2022-ICLR-Dual Lottery Ticket Hypothesis"
338,"The paper proposes a privacy-preserving synthetic data generation technique PriSHA for assisting training clinical foundation models. The DPGAN and PATE-GAN are combined to make the synthetic data generation private, enhancing the privacy protection capability. The experiments show slight improvement of PriSHA over PATEGAN Standard.

Strength:
1. Privacy protection and using synthetic data is very important in healthcare.
2. The combination of existing techniques makes sense.
3. The experimental results are surprisingly good enough.

Opportunity to improve:
1. I am not sure if I buy in the concept that the proposed method can ""address distribution shift"". To me, it is just combining two data sources, except that one is synthetic. It is quite different from deliberate design of tackling distribution shift. It would connect more dots to federated learning.
2. I understand DPGAN provides noises to protect training data, while PATE-GAN trains student models to generate synthetic data for model training. I am puzzled why the improvement is not over standard DPGAN, but over the PATE-GAN. More importantly, I do not think synthetic data can significantly improve model performance in general, since it is impossible to generate OOD data out of the scope of the existing data distribution to provide new information. This should be not be the main focus of the study.
3. There is no evaluation and real-world cases showing whether the privacy is protected well enough. This is the main motivation of this paper, but it is poorly presented and rarely discussed."
339,"This paper proposes an unsupervised representation learning method for generative and discriminative tasks. 

(1) The quality is high, given abundant experiments and visualization on classification accuracy and representation learning. 

(2) The clarity is good. The paper is overall easy to follow. 

(3) The originality is fair since the proposed methods, such as the CTRL-binary program and self-supervised learning with data augmentation, are extensions of existing works, although the authors properly cite them. 

(4) The significance is good, and I think this work attracts the community."
340,"Summary:
The authors set out to benchmark various Deep Learning Weather Prediction (DLWP) models on a two-dimensional Navier Stokes problem, which they claim is a suitable problem for evaluating models of this kind. 

Strengths and Weaknesses:

[+] The paper is well-motivated and clearly presented. It provides a clear description of the problem and the proposed study.

[+] The authors benchmark a wide variety of DLWP models, including not only different models but also different variations of models such as two- and three-dimensional TFNOs. The inclusion of a baseline (Persistence) and a multiple of parameters for each is very comprehensive. 

[-] The claim that the chosen test problem is suitable for identifying a backbone for DLWP is let down by the author's own comments. Take, for example, footnote 1: ""Due to the rectangular nature of our data, we consider the original FourCastNet implementation based on Guibas et al. (2021) instead of the newer Spherical Fourier Neural Operator (SFNO) "". For suitable benchmarking, it does not seem this is a sufficient problem; it would be better to go to 3D and on global scales (e.g., Figures 2, 6, & 7 in the appendix show small-scale features unlike the problems solved in something like the original FourCastNet paper) in addition to including a wider variety of problems. While this is an okay first step, drawing conclusions from such a small problem set does not seem prudent. 

[+] The supplementary material is beneficial and includes many important and helpful details. However, the call for papers states: ""Reviewers are not obliged to read supplementary materials when reviewing the paper."" Some information in the supplementary material is necessary to corroborate the main text claims (see next comment). 

[-] It would be nice to see training and inference times in addition to accuracy. The authors make the statement: ""In terms of accuracy, memory consumption, and runtime, our results illustrate various tradeoffs, and they show favorable performance of FNO, in
comparison with Transformer, U-Net, and GNN backbones."" in the abstract. However, I do not see any reporting of memory consumption or runtime in the main text, and inference time comparisons are also vital for real-time predictions. The authors do state when certain models run out of memory, but that is not comparative; it is only a binary statement. 

Conclusion: In terms of models, the study is comprehensive; however, in terms of problems, it is not. Given this is a workshop paper, that is to be expected in only 4 pages, however, the conclusions derived from such a limited study have to be taken with a grain of salt. I think benchmarking lends itself better to a longer format venue, so comparisons and conclusions can be fair. Still, there are insights that can be made here, such as studying memory and saturation limitations on an individual model level; therefore, I am recommending a weak acceptance and would like to see this expanded upon in a full-length article."
341,"Summary of the paper:
The abstract describes the use of NLP to detect healthcare provider behaviors aligned with common factors theory in psychotherapy. Common factors theory emphasizes building empathy, trust and positive relationships through provider skills like reflective listening and appreciation. While the clinical importance of this is paramount, I have concerns with the content that has been presented in the abstract. 

Major Comments:

This is a great problem statement. However, I have a couple of major comments:

1. There is ambiguity in the description of the exact methods, vague terms such as machine learning and natural language processing are used. 

* They mention the use of “synthetic and generative technologies to expand specific labeling strategies and data curation by generating and validating rare use cases” but don’t describe this - what was the generative model that was used, how did they verify its realism to simulate rare use cases, how much synthetic data was generated relative to non-synthetic data etc. More details need to be provided since this can have significant impacts on the quality of their model.

* “Machine learning methods were used to create natural language processing models based on conversational training data”. What was the base NLP model, did the authors fine-tune a model such as LLaMA? What specific machine learning method was used - NLP fine-tuning strategy needs to be described.
2. The authors mention that they will report results of benchmarking their model on the HOPE dataset, but they don’t do so within the paper. 

The overall clarity of the abstract is low due to the above concerns.

Minor Comments:

1. They have not adhered to the AAAI submission format.
2. They use the phrase “using machine learning with natural language processing”. NLP is technically a subfield of ML, and this statement needs to be revised to reflect that."
342,"### Summary
This paper gives a private density estimator for mixtures of k d-dimensional Gaussians in the agnostic setting. The estimator is computationally inefficient and its sample complexity is polynomial in d and k. This is the first estimator for the agnostic setting and has slightly improved sample complexity than the existing estimator for the realizable setting. 

### Techniques
To achieve this, the authors prove that: a) there exists a list globally stable learner for Gaussians with d samples and list-size d^{d^2}, b) the class of k-mixtures of any list globally
stable learnable class with m samples and list-size L is also list globally stable learnable with mk samples and list-size L^k, and c) any list globally stable learner can be converted into an agnostic private density estimator with a number of samples that grows poly-logarithmically with the list-size L and linearly with the samples m. 

List global stability has been introduced in prior work, which also shows how to convert a list globally stable learner into a private learner with a number of samples that is logarithmic in the list-size L, for the task of classification. The reduction follows the same high-level structure in this paper (run the list globally stable learner on subsamples, filter-out bad candidates from the lists, privately detect repeated candidates in the remaining lists) but the individual parts differ significantly and a new analysis is necessary. The analysis combines known techniques such as propose-test-release, the choosing mechanism, robust compression schemes, and ideas connecting the several parts together. 

### Score
Overall this paper gives the first estimator for privately learning mixtures of gaussians in the agnostic settings, which is especially challenging. Specifically, it is a task which does not seem particularly amenable to known techniques in private density estimation. The paper proposes techniques that seem more generally applicable, such as the reduction of private density estimation to list globally stable learning, and the recipe to retrieve list globally stable learners for mixtures of classes."
343,"The paper is interesting as it explores the application of vector symbolic architecture and abductive rule learning in abstract reasoning tasks. It would be better if the following points can be clarified:

(1) Equations 2 and 3 are confusing with respect to the variable names used. For instance, what is c1, c2..c6,..c12? It is mentioned that c_i represents v_a at (i,j), so shouldn't it be cij? I suppose i and j represents row and column numbers respectively. 

Equation 3 is also not clear - what is I and j - is it number of rows and columns again?

(2)  The three variants of ARLC - ARLC_progr, ARLC_learn and ARLC_p->1, ARLC_progr has some knowledge about the rules, and ARLC_p->1 which is initialised with programmed rules shows lower accuracy than ARLC_learn which learns all rules from scratch (see Table 2) - how do you explain this? Why doesn't Tables 5 and 6 show results on ARLC_p->1? It would help if you could give more details on manual programming of weights and rule initialisation to better understand the different variants. 

(3) While comparing ARLC_progr and ARLC_p->1 with LLM, isn't it fairer if we provide some knowledge about the rules to LLM as well? How would the LLM perform if it has access to the rules?

(4)Can we apply other interpretable rule learning frameworks like ProbFOIL (Inducing Probabilistic Relational Rules from Probabilistic Examples, Luc de React et al) for the task? What is the significance of VSA? It would be easier to understand if the paper could also show examples of the rules learned by the approach."
344,"This work introduces the concepts of local and differential kernels for neural operator architectures. This is to help address a specific limitation of spectral neural operators, in that they only perform a global convolution in Fourier space.

I think the ideas are quite interesting, and the results suggest the techniques work well to improve FNO and SFNO. I appreciate the detailed discussion about related approaches to achieve a similar result in Appendix A Related Work. A minor suggestion to improve the clarity of the work would be to summarize in plain words what exactly goes into the local differential kernel (e.g., ""we center the kernel by subtracting its mean and re-scaling the result by the reciprocal resolution) and integral kernel early on, perhaps in Figure 1's caption. Another would be to include a detailed Limitations section."
345,"I like the idea similar to iterative syllogism as multihop reasoning. The writing could have been more formal to maintain the flow of reading. Overall, it is a novel proposal with a good code demonstration."
346,"This paper presents a novel approach to infer the dynamics of systems from sparse time snapshots without access to individual trajectories. Using an iterative projection mechanism inspired by Schrödinger bridges, the authors propose a method that iteratively refines a dynamical model to match observed marginal distributions over multiple snapshots. This approach is tested on simulated data from ecology, demonstrating its potential to adequately reconstruct the underlying dynamics.

Pros:
- The paper aims to address a significant challenge in dynamical systems, especially in areas where only marginal snapshots are available.
- The work may be of interest and relevance to a broad audience in the workshop, with the new tool for understanding dynamical systems potentially applicable and useful to other fields.

Cons:
- The robustness of the methodology to data sets with different characteristics (e.g., data sparsity, and underlying system dynamics with different parameters, as well as different levels of noise as mentioned by the authors) is not extensively explored. 

Comments/Questions:
- The choice of experiments in ecology may seem disconnected from the initial example focused on mRNA dynamics, which confuses me as I expect a direct application of the proposed method to the scenario described in Sec 2. I would suggest that the paper clarify the rationale behind the choice of actual experiments and explain how they relate to the broader goals of the research.
- I question certain assumptions about the sparsity and distribution of data points on which the methodology is based. Please consider clarifying these.
- Does the method require some parameters such as the choice of reference dynamics or optimization settings, and I wonder if there is some exploration of sensitivity analysis for the parameter selection.
- What are the computational requirements for the proposed method and what is the efficiency or scalability of the algorithm compared to the baseline method?"
347,"The paper investigates the use of Dynamic Sparse Training (DST) strategies in Continual Learning (CL) scenarios. The authors explore different initialization and growth strategies and their impact on performance. They conduct experiments on CIFAR100 and miniImageNet datasets with varying sparsity levels and task numbers. The choice of initialization and growth strategies depends on the sparsity level and number of tasks. The adaptive approach proposed by the authors shows promising results in enhancing performance.

Advantage:

1.	The paper explores the performance of different DST strategies in the context of continual learning (CL) tasks.

2.	It proposes an adaptive approach for selecting DST criteria per task, which improves performance compared to fixed strategies.

Disadvantage:

1.	It's unclear how the DST strategies discussed in the paper perform when applied to other neural networks like VGG and MobileNet. Additional experimentation or information on their applicability to different architectures could be valuable.

2.	The paper focuses on sparsity levels of 80%, 90%, and 95% to draw conclusion that at a low to moderate sparsity level, ERK initialization is more efficient and at a high sparsity level, uniform initialization is more robust. It would be beneficial to know how these strategies perform at other sparsity levels, like higher than 95%, lower than 80%, or more fine-grained sparsity levels between 80% and 95%. Also, it would be better to know how these strategies perform on other works, will other works demonstrate the same conclusion?

3.	I wonder if the proposed adaptive method can be applied to other works, will the performance of adaptive method be better than random or gradient in other works?

4.	It would be better to show how the frequency of topology updates affects performance."
348,"The authors present an extension of the BootStop algorithm, termed MULTIStop, by adding additional constrains derived from domain experts’ knowledge. The authors use an 1D conformal bootstrap equation to show their method.

I think the paper is well written and make clear the main goals. I am not familiar with the example the authors use of the conformal bootstrap equation. I provide my comments below from an optimization perceptive and from a parameter non-identifiability angle of view. 

(a)	The authors seem to be familiar with the BootStop Algorithm and here they propose their method MULTIStop which includes additional constrains to improve the accuracy of the bounds on their coefficients. I would expect to see at least a comparison with the BootStop algorithm to showcase that indeed the additional constrains are useful.

(b)	The authors choose to work in regimes for which there is literature suggesting what might be good OPE coefficients and scaling dimensions. I think this is the rational thing to do but it seems that when they use those “good initial conditions” the obtained coefficients C_n^2 are still off. Can they please comment on why this is the case?

(c)	The authors in many parts of the paper discuss about degeneracy; in my understanding this has to do with non-identifiability of the systems parameters (the coefficients that give a particular response cannot be determined uniquely –> the systems can reach the same state from different parameters) is this is case?

If yes, then in my opinion showing comparisons in terms of the inputs/parameters/coefficients space is misleading. I would expect so see how the norm (the output) ||E(Δ,C^2)||_2 looks like or how close to zero Equation 1 becomes for the obtained coefficients. If the values of their coefficients produce an error comparable to the prior work or smaller, then their method is successful. Based on the results showing in the manuscript is unclear to me the efficacy of their method.

(d)	As a follow up to my previous comment, I think this is why they observe much smaller errors when they use the sum C_4^2 + … + C_8^2. This might be an effective coefficient of the model and thus it has one-to-one correspondence with the output of interest.

In my opinion, the lack of a comparison between their approach and BootStop and the comparison only in the input space and not also in the output/response/behavior space makes the current paper marginally below acceptance threshold. 

Minor Comment:
Abstract: Check the English of the second sentence."
349,"**Overall Evaluation**  
This paper approaches domain generalization through the lens of the Mixup technique, highlighting current challenges including the entanglement of domain and class information and the potential pitfalls of introducing noisy data. Its novelty is underscored by demonstrating the adverse effects of synthetic data generation, especially in terms of domain and class entanglement, not only in conceptual illustration but also within real-world datasets.  

*Pros*  
1. The negative effect of entanglement on domain and class information is well proposed, and its remedy is straightforward.  
2. Analytical evaluation of distribution coverage and inter-class distance further strengthens the motivation of FIXED.  
3. Experiments across two modalities are well-conducted and show convincing results.  


*Cons*   
1. While the primary focus of this paper is on differentiating between domain and class information, offering a deeper discussion on how their relationship (i.e., entanglement) can occasionally serve beneficial roles in real-world scenarios—such as instances where a class originates from distinct domains—would enhance the paper's motivation.  
2. Sensitivity analysis could be more comprehensive. The impact of adversarial learning weight and the required distance to boundaries may be intertwined. Presenting their joint effects using a 2D heatmap or 3D plot could offer deeper insights.  
3. The visual presentation in this paper, including figures and tables, needs improvement. The space for Figure 4 and Table 3 should be expanded for clarity. Figure 5, in particular, appears too small and could benefit from resizing. However, these can be easily addressed in their final version."
350,"Paper proposes a plasma dynamics model with a parametric module to estimate particle and energy diffusivities. The dynamical system is solved with NeuralODE and the parameters are optimized through standard backpropagation through the solver. 

Since it is not my expertise, I am unable to judge the novelty and correctness of the proposed dynamics model in Section 2.
Feedback on the simulation results in Section 4:
1. Predictions for particle density $n$ seem quite far from the ground truth in the tested examples (e.g., in Figures 4, 9). But there is a mismatch with the reported MSE values in Table 1: they seem too low when compared with these predictions and the fact that $n$ is in the range of $10^{19}$. 
2. The baseline (original model) seems to be the one with randomly initialized parameters. If that is indeed the case, it is a weak baseline. A better comparison would be to use other dynamical models proposed in the literature or purely neural network based methods. Without an appropriate baseline, it is hard to judge whether the reported predictions/errors are reasonable for this task."
351,"**Paper Summary.**

This paper presents an interactive prompt-engineering technique for Large Language Models, through a system they call ChatLogic. Rather than asking the LLM to give its chain of thought reasoning in English, the system prompts the LLM to output its reasoning in a logic programming language (specifically, pyDatalog).  The system then exploits this by doing light semantic and syntactic corrections on the program via script-run interactive sessions with the LLM.  The result is that doing this consistently improves the LLM's ability to do multi-step reasoning, measured by accuracy on standard multi-step reasoning datasets.

**Novelty.**

There are three core ideas going on in the ChatLogic system:
  1. Rather than prompting an LLM to give its reasoning in English, prompt it to output logic programming code
  2. Use a scripted LLM interactive session to self-correct *semantic errors*
  3. Use a scripted LLM interactive session to self-correct *syntax errors* (until the program is executable)
  4. Use different types of prompt templates (specifically, zero-shot vs one-shot) at different stages

As the authors mention, the closest system to theirs seems to be LOGIC-LM (Pan et al. 2023), which does both (1) and (3) to perform multi-step reasoning.  Note that (Chen et al. 2023) also considers both (2) and (3), but in the context of self-correcting LLM code in general rather than to improve an LLM's multi-step reasoning.  Scanning the prompt engineering literature, I cannot personally evaluate whether (4) is novel.

The novelty of this work is that, compared to LOGIC-LM, ChatLogic additionally uses (2) semantic self-correction to improve multi-step reasoning.  What's especially interesting about their semantic self-correction is that it is done by translating the output code back to natural language.  The system then uses scripted prompts to align the code with its reverse-translation.  As far as I'm aware, this technique is new.

**Quality.**

The LLM+ChatLogic system performs consistently better than the baseline LLM at standard multi-step reasoning tasks.  It also performs better than an LLM with zero-shot chain-of-thought prompting (Kojima et al. 2022).  These are solid preliminary results.  However, since ChatLogic appears to share a similar approach with LOGIC-LM, it would be good to see how these two systems compare.  The authors also claim in Table 1 that ChatLogic's mix-shot approach is more accurate than one-shot prompting, but they did not include one-shot prompting in their results --- perhaps because it is already worse than zero-shot prompting, but it should be included nonetheless.

In the Evaluation, and throughout the paper, the authors claim that LLM+ChatLogic significantly outperforms, but there are no confidence intervals or discussion of statistical significance supporting this claim.  Additionally, the claim made on page 3 that ""our approach reduces hallucination significantly"" is entirely unsubstantiated, since the authors do not measure hallucination (higher accuracy does not necessarily entail fewer hallucinations).

Another strength of the paper is that the authors include an ablation study, showing that both the semantic *and* syntactic self-correction components contribute to the system's performance.  I'm curious why they only test the executability of the code produced rather than the performance of each ablated system.  The authors also put a lot of emphasis on their mix-shot prompting (e.g. in the section ""Mix-shot CoT""), but I'm skeptical that this meaningfully contributes to the system's reasoning ability.  I recommend testing zero-shot and one-shot prompts against mix-shot in the ablation study.

**Clarity.**

The overall organization of this paper is quite good.  The authors clearly put a good deal of effort into their figures, which do an excellent job communicating the structure and advantages of ChatLogic.

Although the writing is clear in some places, many paragraphs and sentences are confusing and hard to parse.  Here are a few examples:
- (page 3) ""It is worth noting that an initially generated code, following successive revisions within the two modules (Semantic and Syntax Correction) through two or more iterations, enjoys a significantly heightened probability of serving as a direct code solution for multi-step reasoning, yielding precise results.""
- (page 3) ""Conversely, in scenarios needing broader analysis, such as text comparison, thematic exploration, or creative content generation, the model is granted greater autonomy to utilize its analytical prowess and generate innovative solutions, fostering its capability to navigate vast information and produce unique insights.""

Similarly, much of the Related Work is hard to follow.  I urge the authors to do a proper revision (prior to submitting a future version of this to a conference) by 1) breaking up these long run-on sentences and 2) setting up sign-posts at the beginning and end of paragraphs that remind your reader of the subject of the paragraph.

Some other issues of clarity: ""CoT"" is used before ""chain-of-thought"" is defined.  It would also be nice to see an example of how basic chain-of-thought is used when introducing the concept.  The authors also do not define zero-shot and one-shot CoT in a clear, outlined way (I had to read the literature in order to understand what the difference was).  Perhaps the authors should also note the terms ""zero-shot"" and ""one-shot"" were given specific definitions in the prompt-engineering context, which is *not* the same as their meaning in machine learning (this started with (Brown et al. 2020)).

Also, occasionally the authors dress up their work with flowery speech, e.g.
- (page 1) ""we have innovatively implemented a mix-shot CoT which significantly boosts LLM performance with minimal resource usage by utilizing diverse prompt engineering techniques.""
- (page 3) ""Our innovative mix-shot CoT approach represents a groundbreaking hybrid methodology""
- (page 5) ""The impressive performance demonstrates our work's effectiveness""
I believe that passages like these are misleading and obscure the actual contribution of the paper (see 'Novelty' above).

**Significance.**

Performing multi-step reasoning is a key challenge for LLMs.  Interest in prompt-engineering for this purpose has exploded recently, so this paper is certainly timely.  I believe this system, along with its cousin in the literature (Pan et al. 2023), provide a unique and important perspective on how to do this: we ought to prompt LLMs to give their reasoning as a logic program, so that we can do meaningful self-corrections that would otherwise be difficult in natural language.

**Typos and Nits.**
- (pages 1, 2) ""continuous"" -> ""continual""
- (page 2) ""Surprisingly, often produces..."" has no subject
- (page 3) ""Overflow"" -> ""Overview""
- (page 4) ""closed-world assumption principle"" -> ""closed world assumption""
- (page 5) ""NER"" -> better to spell out Named Entity Recognition for the reader
- (page 6, Figure 3) ""Anny is tiny"" -> ""Anne is tiny"" (in bottom-right corner)
- (page 7) ""GPT"" -> ""GPT-4""
- (page 7) The 'Base' in Tables 2 and 3 is slightly different from the 'Base' in Table 4.  You should clarify this in the caption."
352,"The paper introduces an approach for more efficient prediction of fluid flow, by downscaling the mesh sizes in CFD simulations. Both the approach and results are interesting and should be useful for the community. I believe the work is of a good quality to be accepted for the workshop. However, there are some major (as well as some minor) comments I would like to address before final acceptance:


•	The authors claimed the model is mesh-independent, however, it is not very clear why it is. Maybe a few sentences will help. 
•	Regarding the previous comments, as I see in the appendices (e.g., Figs. 4, and 10), none of the cases use unstructured meshes, which is in contradiction to one of the main claimed novelties of the work. 
•	The separation of data for training of testing has not been discussed clearly (although I see a discussion about it in the appendix). It would be nice if the authors could mention how they were separated. What was the difference between the properties of the system comparing the training and testing data? that is important since if for instance the geometry or boundary conditions were different between two datasets, the outcomes of the study would be a higher quality. 
•	Regarding the previous comment, what are the extrapolation capabilities of the PointSAGE method? Is it reliable to train it once and then use it for many other cases? 
•	There are some problems with citations. Maybe authors could use \citep instead of \cite in the cases with parenthetical citations. 
•	Table 2 shows that for some cases the PointNet approach is superior considering both accuracy and run-time. What do the authors think about it?"
353,"**Summary**:

The authors present a foundation model for chest x-ray interpretation with three novel components, they present an instruction-tuning dataset, a foundation model trained on this dataset, and a benchmarking tool to evaluate FMs across different CXR datasets. The authors showcase the performance of their FM across multiple tasks, such as Image Perception, Question Answering, and Text Generation. The authors compare their FM against other General and Medical-domain specific FMs across 8 tasks and 7 CXR datasets and indicate a general superiority of the performance of their models across the different tasks. 

**Pros**:

- *CheXagent's* performance looks impressive across the different tasks. They are able to demonstrate its utility as a Foundation Model by displaying generally superior performance across 7 tasks that are relevant to an FM in this domain.
- The authors comprehensively describe the steps they took to infuse the underlying LLM with medical and clinical knowledge. The overall architecture of *CheXagent* also appears convincing and matches intuition on their good performance across the different tasks.
- The authors have created a benchmark for evaluating FMs for Chest X-rays using their *CheXbench* benchmark. This is important for reproducibility and extending the evaluation across newer Chest X-rays datasets and Medical FMs. 

**Cons**:

- The authors mention they provided an evaluation of the disparities of the mode's performance across demographics, and important considerations for FMs when they envision their FMs being adopted into clinical practice by radiologists. 
- I would've liked the authors to provide a list of diseases they are evaluating the models on along with their distributions. It would have been interesting to provide some examples of the model's performance in the appendix. 
- I would've liked the authors to make a comment or two on interpretability, and how they think they could extend their work to incorporate measures of explainability and trustworthiness, which is fundamental to the adoption of AI models into clinical practice. Frequently, we run into the issue of developing ""black-box"" models in the field with not a lot of effort made into explaining how the model makes its decisions. 

**Quality**: 

The overall quality of the paper is good. 

**Originality**:

Even though it appears that some work has been done on building summarization models for Chest X-Rays using Multimodal LLMs like GPT, I believe the authors have done a good job highlighting the novelty of their work in compounding an extensive X-ray dataset, developing a novel FM specific to Chest X-ray and evaluating it across a variety of tasks.

**Significance**:

I believe this can be a significant model in the field of radiology if the authors are able to extend this work to take it out of its current ""black-box"" setting and integrate some form of explainability or generalization, and comprehensively evaluate how this model's performance differs across different demographics. 

**Miscellaneous Comments**:

- Have the authors considered incorporating the Noisy CXR dataset into CheXinstruct? The dataset presents the unique dimension of capturing label noise, and it would be interesting to observe how CheXagent performs under such a scenario."
354,"This paper studies the improving (rising) bandit problem, in which each arm’s reward follows an arbitrary increasing function with diminishing returns.  Several variations of this have been studied previously, and what distinguishes this one is (i) considering instances whose worst-case nature may depend on the time horizon T, and (ii) allowing randomization rather than only deterministic strategies.  The lower bound shows that the regret must be $\Omega(\sqrt{k})$ times the best single action, and the upper bound matches this (when the best total reward is known to within a constant factor) or gets within a log factor (otherwise). 

The results seem to be good additions to this line of works, but I also have some not-so-minor concerns mentioned below.  I am unsure about acceptance, as I would ideally (if available) ask for a “major revision with second round review”, but I wouldn’t mind if the paper were accepted. 

Main comments:
1) The paper does not do a good job of highlighting the distinction between average regret and high-probability regret.  It is absolutely essential for the results given – it looks like even asking for a probability-$\frac{1}{2}$ regret bound would mean we’re back to $O(k)$ suboptimality rather than $O(\sqrt{k})$, because the hard instance could be such that information about the optimal arm is only learned after time $\frac{2T}{k}$.  Along similar lines, the upper bound in Section 5 is based on identifying an $\Omega\big( \frac{1}{\log k} \big)$ probability good event, which means that the algorithm might actually fail most of the time.  The average regret notion considered may be of some theoretical interest (perhaps less practical interest?) but it does stand out as a weakness, and it really needs to be highlighted better.  This is made worse by the fact that Section 2 never explicitly writes down the performance goal mathematically (which should be a displayed equation with a a clear E[.] operation etc.) and these subtle issues are really hidden. 
2) The lower bound proof seems to be very similar to that of Patil et al., but I don’t see any mention of that in the proof.  I find this rather troubling, as it makes me uncertain about which methods in the entire paper are more novel or less novel. 
3) The main assumptions are also quite hidden in the text.  I suggest a LaTeX “Assumption” environment clearly stating the diminishing returns, f(0)=0, and anything else you assume.
4) Sometimes the mathematical steps themselves can also be clearer.  For example, $OPT_{T’} \ge \big(\frac{T’}{T}\big)^2 OPT_T$ should be stated as a lemma *with proof*, and cross-referenced whenever used (it’s not immediate to see this just “from diminishing returns”).  
5) Other parts of the writing can also be improved.  For example, Lemma 6 and the paragraph before it are quite confusing and events/quantities are written in an overly wordy way, sometimes in an incomplete or inconsistent manner.  It’s better to use mathematical definitions and symbols.  

Minor comments:
- The abstract and introduction mention that any algorithm must be $\sqrt{k}$ away from the “optimal reward”.  This is ambiguous, because “optimal reward” could be taken to mean the reward obtained by some actual algorithm for your problem.  I suggest wording to the effect of “total reward of the best single action”.
- I suggest merging Footnote 1 into the main text.
- The final sentence of Section 1 is very generic.  I think a better job can be done of highlighting the other bandit literature.
- p6: After “…from diminishing returns” you can also write “…and $2c_1 > 1$” to avoid confusion with two different steps being applied at once.  You can similarly give a hint about the second-last line in this proof (just before Theorem 8).
- Suggest brackets around the summand in (8)
- At the end of Lemma 9’s proof I don’t know why you write X+(2k-1)X instead of 2kX. 
- In Appendix C you should point out (if not already done) that you assume $\epsilon$ to be known to the algorithm.
- In Claim 13 you should say *adapted version* of Algorithm 1
- I think the word “noisy” should be mentioned in the main text, not just Appendix C.  Stating that you extend to noisy settings sounds stronger than just stating that you extend to cases without precisely diminishing returns."
355,"*Paper Summary*

The paper considers distributed, agent-based algorithms for generalized Nash Equilibrium (GNE). GNE is a natural extension of Nash Equilibrium (NE) in games where agents face coupled constraints, meaning the action set of each agent depends not only on their own actions but also on the actions of others. The paper investigates distributed algorithms that enable agents to reach GNE via limited communication. Specifically, the paper makes the following contributions:

1. The authors propose an algorithm, Online Feasible Point method, that once followed by each agent, the coupled constraints of the game are never violated. The essence of the algorithm is that at the beginning of each round each agent $i$ transmits a desired set $S_t^{(i)}$. Then each agent $i$ selects an action $x_i^t \in S_t^{(i)}$ that depends also on the sets $S_t^{(j)}$ that the agent has received.
2. The authors identify a subclass of GNE i.e. strongly benign GNEP at which the proposed Online Feasible Point method converges to.
3. The authors establish no-regret guarantees for each individual agent that follows the Online Feasible Point method.

*Strengths*
The problem of how agents can collectively converge to a GNE is an interesting and important topic. Consequently, I believe that the results of this paper are valuable. The Online Feasible Point method guarantees feasibility at each step, which is a notable contribution compared to previous works. Additionally, the notion of strongly benign GNE is well-justified and is connected to standard assumptions in the learning-in-games literature, such as strong monotonicity.

*Weaknesses*
I believe the presentation of the paper could be significantly improved. Overall, I found the paper difficult to follow. Additionally, the authors should compare their convergence results with previous convergence results for GNE, such as those by Yi et al. and Jordan et al. It was unclear to me how the notion of strongly benign GNE as well as the provided convergence rates compare with the previous results. Furthermore, the no-regret guarantees seem to hold only in cases where a strongly benign GNEP exists. However, in my opinion, in order for an algorithm to be considered compatible with the strategic behavior, no-regret guarantees should hold regardless the game structure.

*Overall Evaluation*
I overall believe that this is an interesting paper and should be considered for acceptance."
356,"This paper explains an approach to convert tabular data into textual descriptions that can be parsed by a language model for comparison against a specific medical definition.

Strengths:
1. The paper presents an interesting idea to supplement LM models with clinical definitions to inform model predictions. 
2. Multiple experiments were performed. Results were averaged and standard deviations were included.

Weaknesses:
1. Paper contributions are misleading. Paper says, ""approach transforms multifaceted pre-operative and intra-operative tabular data into coherent text-based descriptions, enabling the use of advanced Language Models (LM) for data interpretation."" This seems to imply that a model is used to output text descriptions from static data but really these descriptions are developed using medical experts. Also the conclusions say, ""The key contribution of our study is to increase the predictive performance of medical data with heavy class im- balances in disease prediction"". Class imbalance is not discussed as an objective in the introduction and it unclear how the method was designed for this purpose in mind.
2. More explanation of the RBF framework is needed. What is it? How does your method differ/expand on this approach?
3. Paper contributions as explained do not seem to present a novel methodology. The text and definition are developed by medical experts and then just parsed through an existing LM where the output embeddings are compared using cosine similarity. It seems that the main contribution is the approach to group sentence inputs to a downstream classifier for prediction rather than directly using an LM model fine-tuned for binary classification. Perhaps the embedding approach for grouped sentences is novel? More explanation and a perhaps refocused message is needed.
4. Based on confidence intervals the results do significantly improve over baselines in terms of precision and recall. 
5. Explanation of baseline LM training is vague. 
6. How was the relevance threshold selected? This should be a hyperparameter that is tuned based on a validation set and not manually selected based on model performance. More explanation is needed here.

Questions:
1. Is the definition of MINS provided to the other baseline LMs? If not, it would be difficult to compare how this approach compares to other baseline LMs and assess whether the downstream classifier is necessary.
2. What is the purpose of the different inputs channels? Is it to create embeddings over a set of grouped sentences rather than individual sentences? Did you experiment with just inputing the embedding over the entire patient description? This would be similar to using an LM binary classifier baseline.

Other:
1. I would recommend emphasizing the value of this approach to adapt to new/changing definitions easy without needing to relabel the data and retrain a model. That is a very valuable approach in healthcare."
357,"This work lays the groundwork for the creation of a Foundation Model (FD) on the domain of Chest X-Ray (CXR) images. It proposes a set of datasets (CheXinstruct) containing 5 task categories related to CXR, a FD trained on these datasets (CheXagent) and an evaluation benchmark (CheXbench), in which the model achieves outstanding results. This work is well presented, clearly expressed and is especially relevant with regards to the topic of Foundational Models for medicine. Although it builds on the contributions of many other works (which is properly cited in the text), it is an important contribution nonetheless and will allow further research and advances in the topic. For these reasons, I consider to accept this paper for the symposium.

I would like to include some comments for further clarification or correction of the work.

- When listing the tasks in the construction of the dataset, some tasks might not be familiar for all readers and a short description would be beneficial, e.g., view classification.
- In the model section, “To infuse the model […], we utilize **five** distinct text sources for training”, but then only three are mentioned. Either a clarification or a correction is needed here.
- For training stage 2, training a vision-language bridger, a reference is needed to clarify how this process takes place.
- Regarding the evaluation table, no confidence intervals are given, which makes it harder to compare between models with similar metrics, particularly for single- and multi- disease identification.
- No results are given for any other method in the Findings Summarization task; if it is in the author’s power to do so, it would be beneficial to have results for these methods.
- Regarding the outstanding results in terms of View Classification, a comment on the reasons for this dramatic increase are missing."
358,"Quality and clarity:
- The writing is generally clear and easy to understand

Originality and significance:
- Gatertron Foundational Model trained on clinical data, highly relevant for this workshop

Weaknesses
- Performance ranking makes it more difficult to visualize the actual performance of the model, more fine-grained evaluation metrics would be appreciated, like accuracy for QA, precision and recall for the extraction tasks, etc
- Where are the GatertronGPT evaluation results?"
359,"This paper studies linear models in tangent feature space with transformable features. This work indicates the relationship between linear feature adaptivity and structured regression using fixed features with low-rank constraints. The authors provide insights into how the features and kernel functions change. Experiments support the results. My concerns are the following.

1. This paper is not very easy to follow. The writing needs improvement. 

2. There are too many assumptions about the Adaptive feature model and the neural network model. I feel it makes the conclusion weak. 

3. The practical significance of the results is not very clear. For example, if the authors want to relate the results with LoRA, it is better to provide a Corollary with a discussion on LoRA.

----------------------------------------------------------------------------------------------------------------------------
Thank you for the reply, and sorry for the late reply. I am partially satisfied with the response. For the examples, I am referring to whether your assumption applies to some common neural networks, such as convolutional networks with ReLU activations. It is difficult to make a decision. I will keep my rate of 5 but decrease my confidence to 2."
360,"An interesting benchmark  with good reproducibility.  Clear explanation and well written demonstrating how LLMS can be used to detect instances of race-based medicine.

Pros - Potentially an interesting read for a clinical audience who would like to know the feasibility of having such a model being run 'in clinical practise'.

Cons - This style of paper is very common (prompt crafting + evaluation) and can be criticised as not contributing anything novel to the field."
361,"Paper simulates an epidemiological dataset by solving a known set of PDEs for spatiotemporal evolution of diseases over a domain resembling Germany. The synthetic data is then used to benchmark various NN-based methods for forecasting. 

1. More details on the dataset generation should be added to the paper: 
	1. What are some examples of the 25 different infection scenarios? 
	2. How were $\alpha(x, t), r(x, t), D(x, t)$ chosen? 
	3. Was the dataset generated with different sets of initial points infected?
	4. How many trajectories does the dataset contain?
2. Benchmarking experiments need more clarifications/rigor.
	1. Please clarify whether the results in Figure 2 and Table 1 show RMSE for a single trajectory in the dataset?  Figure 2 is missing axis labels. Table 1 is missing confidence intervals. 
    2. It would be helpful to show how the results change across the 25 different infection scenarios in the dataset (by varying $\alpha, r$, etc.)
	3. Robustness experiment is limited to a single level of arbitrarily chosen noise. It is unclear if the results hold across different levels of noise. 
    4. Another relevant robustness scenario for epidemic modeling is when data is sparse/missing from certain regions of the domain. It could be interesting to benchmark whether the GNN based methods are robust to this.

Overall, novelty of the methodology is limited but a large-scale spatiotemporal epidemiological dataset could be useful for benchmarking or pretraining models."
362,"**Explanation of paper**

This paper uses a specially designed neural network as a preconditioner for the Helmholtz equation, which a PDE typically solved via iterative methods. This paper is similar to Azulay and Treister (2021), the main difference being an implicit layer is added to the U-Net preconditioner, which improves performance.

**Pros**

* When I review papers for these workshops, more often than not I find serious flaws or methodological issues with the paper. In this paper, I see no major flaws and have no concerns. 
* This is a good, solid paper, that tackles a problem of relevance to this workshop while comparing to a strong baseline and using good empirical practices.
* While the Helmholtz equation is not commonly studied by the AI4DiffEq research community, it is an important PDE of interest to many disciplines of engineering and science. Furthermore, the strategy here of using a neural network as a preconditioner *is* very common in the AI4DiffEq community; those papers typically consider the (simpler) Poisson equation.
* These authors appear to be experts in preconditioners and iterative numerical methods for elliptic PDEs. Their presence at this workshop will likely add value to the community.
* I think the introduction is excellent, and is helpful both for explaining the context this work is in as well as for future authors working on the Helmholtz equation.

**Cons**

* I found section 3 of this paper hard to understand. Maybe this is my fault -- I was an emergency reviewer for this paper and didn't have enough time to understand the architectural details -- but I do think that these ideas could be explained with improved clarity. 
* If this were the main ICLR conference, I would feel the novelty of this paper (i.e., mainly an architectural modification) is not sufficient to justify acceptance; in other words, this paper is to some extent salami slicing. However, for a workshop, I think this is an appropriate contribution.

**Additional comments**
* ""Preconditionding"" in title is spelled incorrectly. Even if it were spelled correctly, the PDF uses the word ""preconditioners"" not ""preconditioning"". 

**Conclusion**

My recommendation is to accept this paper. It is a solid paper, which uses good empirical practices, a strong baseline, is clearly within the scope of the workshop, and has nice experiments and good results. If I were argue against acceptance, I would say that the results in this paper are not of enormous significance and I have some concern about salami-slicing. However, a vast majority of papers (98%+) are not of great significance, so I think that is perfectly okay for a workshop."
363,"Straight forward, early work. Could use experiments with harder datasets (as mentioned towards the end of the paper), more discussion on next steps, comparisons to other domains, etc. It's also short with more spacing than other papers suggesting more content should be added"
364,"This paper aims to improve the approximation of Koopman operators by introducing a new loss function based on mathematical properties of the infinitesimal generator of the Koopman operator. The paper is clear, and offers a simple, generic alteration for practitioners approximating Koopman operators.

The main pitfall of this paper is the lack of comparison to existing loss functions for the same task. At least one should be included in the main paper, in order to showcase the difference between existing literature and this paper.

Overall, the paper is well-written and makes a compelling case for practitioners to adopt this loss.

Some minor nitpicks:
- Each of the terms of this loss are immediately interpretable by someone with a background in the theory of linear dynamical systems, except for the terms $z$ and $z_i$. Therefore, consider pulling the definition out of the appendix so it's possible to understand the loss without going to the appendix. Especially since this is the central contribution of the paper.
- $\omega$ is used both as parameters in $\psi$, and as weight terms in the loss function, ie. $\omega_1, \omega_2, ...$ Consider using a different notation for these.
- In the third sentence after equation (2), the authors state ""If, $f$ is sufficiently smooth, ..."" without a reference to the smoothness conditions.
- In the second sentence after equation (6), the authors state ""It is straightforward to show that Koopman eigenfunctions $\phi(x)$ that satisfies $\mathcal{K}^t \phi(x) = \lambda^t \phi(x)$ for $\lambda^t \neq 0$ are also eigenfunctions of the Lie operator..."" The wording here is not entirely clear, is this to say eigenfunctions which satisfy “for all t there exists $\lambda^t$ such that $\mathcal{K}^t\phi(x) = \lambda^t \phi^t...$?“
- In the paragraph preceding equation (8), it reads ""Since our ultimate goal is to study nonlinear dynamical systems using linear theory, we do not need to restrict ourselves to Equation 7."" This is again not entirely clear. Why should one not limit themselves to studying linear functions of the Koopman eigenfunctions?
- In the first sentence following equation (10), ""$\mu, 2\mu, \lambda]$"" is missing an opening bracket.
- In the first sentence of section 3.3, ""exmplar"" is misspelled."
365,"### Summary:
Mechanistic Neural Networks (MNN) are introduced for equation discovery, PDE solving, and forecasting. The method implements a mechanistic encoder that translates the input into an ODE and, at the same time, predicts the step size to solve the equation. Subsequently, a neural Relaxed Linear Programming Solver integrates the inputs to generate a prediction. Results underline MNN's ability to discover equations and accurately forecast dynamic systems, while not qyite reaching the accuracy of state-of-the-art methods in PDE solving.

### Strengths:
- Exhaustive experimentation with additional details provided in the appendix.
- Wide applicability of the method to different tasks.
- Accelerating equation discovery by providing an efficient GPU implementation for parallel computing.

### Weaknesses and Concerns
- Unclear what the exact inputs and outputs of the model components are. E.g., what does $f_{\theta}$ receive and concretely produce as output and how are these outputs then composed as an ODE?
- MNN is compared against a fairly old version of SINDy. More recent variants, such as SINDy-PI [[1]](https://royalsocietypublishing.org/doi/full/10.1098/rspa.2020.0279) in fact are able to recover the coefficients of nonlinear systems.
- While MNNs can be applied to various tasks, it does not seem to reach FNO's performance in PDE solving.
- In Figure 2, it is unclear to me how MNN can technically learn the entire vector field accurately (including the region of the test sample) by only seeing the train sample. The SINDy solution looks realistic to me, given the method only receives the plotted training trajectory in the left end of the vector field. I understand that the two trajectories plotted in Figure 8 are more informative about the shape of the vector field, allowing MNN to better capture the overall vector field in that case, i.e., when receiving two trajectories. Can you explain why Figure 2 only contains one trajectory, verify whether MNN is only receiving that single trajectory, and explain how MMN technically can get such an accurate notion of the complete vector field then? Also, how does SINDy (or SINDy-PI) perform when receiving two trajectories during training?

### Questions:
1. On page 1, what are large batches and do you seek to learn many ODEs simultaneously from different families, or do you vary some coefficients in an ODE and call this many ODEs?
2. If the step size is a learnable parameter, how do you ensure that it does not converge to zero in order to generate the most accurate results?
3. Can you show some examples of what $\omega_x$ converges to? Would be interesting to see what step size it induces under what circumstances.
4. In Equation (2), can you specify what $b$ is and why the right-hand-side is not zero as in traditional ODE representations? That is, do you allow for a bias depending on $t$ and $x$ and how do you account for it and model it?
5. in ""Efficient Quadratic relaxation and extension"", is the LP method standing for linear programming? Maybe add as explanation.

### Minor comments:
- On page 1, space missing in ""When training ODE representationswe...""
- In the \textbf{} commands, when opening new paragraphs, you sometimes do and do not use a dot, which might be made consistent.
- At the beginning of Section 4, PDE solving printed two times in ""... discovery of equations, PDE solving, , PDE solving ...""
- Typo in **N-body prediciton** on page 4.

### Assessment
In balance I am undecided about this paper, which, on the one hand, offers exhaustive and reasonable results on a relevant topic, but, on the other hand, is vague about several aspects that need clarification.

### References:
[1] https://royalsocietypublishing.org/doi/full/10.1098/rspa.2020.0279"
366,"## Overview

The study introduces a novel attack framework, termed DLM and DLM+, that challenges existing federated learning systems where model weights are transmitted rather than the gradient. Unique to DLM(+) is its reliance solely on communicated model parameters and the loss function, making it more flexible than earlier gradient leakage attacks. Comprehensive tests underscore the consistent efficacy of the introduced methods.

## Comments

(+) The advancements brought about by DLM+ are notably superior to other attack baselines.

(+) A series of ablation studies have been carried out, which strongly attest to the efficacy of the proposed method.

(+) This work is well-motivated and proposes a threat to federated learning where only model weights are transmitted.

(-) Referring to line 172, while MNIST, CIFAR10, and CIFAR100 are employed to encompass different image sizes, both sets predominantly contain smaller images (28x28 for MNIST and 32x32 for CIFAR10/100). Integrating datasets with larger images might offer more comprehensive results.

(-) It would be valuable to understand if the presented method can be extended to situations where prior information about the loss function isn't accessible."
367,"The paper presents a study on the development of the CycleTrans model for predicting specific medications for patients based on their disease diagnoses. The model incorporates a cycle-embedding module to enhance symptom and drug embeddings, utilizes cross-attention and transformers to integrate patients' longitudinal data, and achieves high clinical precision and low drug-drug interaction (DDI) rate. The study also discusses the need for additional data, ethical concerns, and the unresolved issue of AI explainability in the medical field.

Pros:

The study introduces a novel model, CycleTrans, which addresses the need for precise medication recommendations based on patient diagnoses.

The incorporation of a cycle-embedding module and the use of cross-attention and transformers demonstrate a comprehensive approach to addressing the complexities of medication recommendation in clinical settings.

The model achieves high clinical precision and low DDI rate, indicating its potential for improving patient safety and treatment efficacy.

Cons:

The study acknowledges the need for additional data, particularly recent clinical domain data, to substantiate and validate the findings, indicating potential limitations in the current model's training and evaluation.

Ethical and moral concerns about AI-generated conclusions and the lack of clear AI explainability in the medical field remain unresolved, raising questions about the practical application of the model in real-world clinical settings.

The study does not provide a detailed discussion of potential biases or limitations in the model's predictions, which could impact its practical significance and real-world applicability.

Overall, the study presents a novel approach to medication recommendation in clinical settings, but it also highlights the need for further validation, consideration of ethical concerns, and addressing potential biases in the model's predictions. The work's significance lies in its potential to improve patient care and treatment outcomes, but its practical application may be limited by unresolved ethical and explainability concerns."
368,"The authors generally described their OneSAM step by step including the techiques they used such as SAM-HQ mask decoder, an adapter, data augmentation and so on while some details can be elaborated more. The ablation study's result is somewhat not clear, the baseline performance is lower than the one showed on the codabench. Also, the running time of 2D decreased a lot while the 3D objects running time even got longer, the author should explain possible reasons. More elaborations should be add for Qualitative result."
369,"Summary: The paper introduces an approach to enhance literature retrieval from PubMed for encephalitis-related research. The approach is based on a transformer-based embedding model that, given encephalitis-related queries, retrieves relevant PubMed literature. Unlike prior keyword-based searching systems, the proposed method is able to identify articles relevant to a query even when the exact keywords are absent from the query. To build the embedding model, the authors first created a dataset of thousands of query-document pairs. Specifically, they used ChatGPT to generate queries from the document as it has the ability to understand the variance of encephalitis representations and capture various semantics of the document.

Comments and suggestions:

1. Comparing the performance to that of a keyword-based searching system seems insufficient. Would it be possible to use a pre-trained, high-quality embedding model to perform this task? I believe recent advancements in retrieval-augmented generation (RAG) would provide useful techniques for this task.
2. How do we ensure that a general-purpose model like ChatGPT understands encephalitis literature, as I suppose it belongs to long-tail distribution?
3. The link for [1] is missing.
4. It is stated that the ""Biopython"" library is used. However, the ""Biopython"" library I know of is a biological sequencing tool (https://biopython.org/), which is not related to this paper.
5. The paper would benefit from a large-scale quantitative assessment of the model's performance. This addition would provide a clearer understanding of its efficacy compared to existing methods.
6. I think adding a graphical diagram would be beneficial to better outline the pipeline."
370,"1. Summary and contributions: Briefly summarize the paper and its contributions
- The paper outlines the development of billion parameter GatorTron, an encoder only, and GatorTronGPT, a decoder only, transformers. 
- They are trained from scratch on billions of text tokens, both from general text corpora as well as de-identified clinical notes. The paper then goes on to summarise the previously published performance of the models on commonly used clinical NLP benchmark tasks.

2. Strengths: Describe the strengths of the work. Typical criteria include: soundness of the claims (theoretical grounding, empirical evaluation), significance and novelty of the contribution, and relevance.
- It is a huge (maybe unique?) technical achievement for an academic institution to be able to train billion parameter LLMs on billions of tokens.
- The performance on various benchmarks is very strong.
- Highly relevant to this symposium

3. Weaknesses: Explain the limitations of this work along the same axes as above.
- The hypothesis of this work is that explicitly including clinical notes in an LLM’s training data aids performance on downstream clinical tasks. Therefore, a direct comparison to state-of-the-art general LLMs (e.g. GPT/Gemini/Llama/Mistral) in Table 1 would strengthen this argument. Especially since general LLMs have been through RLHF, unlike GatorTronGPT
- Not clear why the models were trained from scratch instead of finetuning a generally pre-trained model. Although the training corpus size is large, it is still dramatically smaller than that used in open-source models (such as Llama 2 trillion tokens)
- Does GatorTronGPT exhibit zero/few-shot learning abilities, or must it be finetuned?
- No section on ethical considerations or the ethics board approval. This is highly relevant given the large-scale use of (de-id) clinical notes and the subsequent open-sourcing of GatorTron. This is a great initiative, so it would be helpful to be explicit on how this was achieved so others could do the same in the future.
4. Correctness: Are the claims and method correct? Is the empirical methodology correct?
- “GatorTronGPT provides a solution to solve many diverse information extraction and classification tasks using unified text-to-text learning” seems like a strong statement.
- The methodology, although only briefly outlined due to page limit, is sound.
5. Clarity: Is the paper well written?
- The paper is well written. However, the last paragraph of the conclusion is oddly formatted and does not seem to follow from the previous one.
- The repetition of Peng et al b and c on seemingly the same datasets and tasks but with differing performance rankings in Table 1 is a little confusing.
- Would be useful to note the context window of the models.
6. Relation to prior work: Is it clearly discussed how this work differs from previous contributions?
- Yes, a clear explanation of general-purpose LLMs and their evaluation in biomedical/clinical NLP space. This then goes on to outline the limited specialised training of LLMs in the clinical domain. Although the motivation for this is implicit rather than explicit (training on clinical data may lead to better results on clinical tasks) 
7. Reproducibility: Are there enough details to reproduce the major results of this work?
- No, but that is out of scope, given the page limit. The author could add the compute required to train the models.
- Links to the open source model weights or training code would be helpful."
371,"This paper investigates the emergence of segmentation properties within Vision Transformer models (ViTs). Contrary to the prevailing belief that segmentation properties predominantly result from intricate self-supervised techniques like DINO, this paper illustrates that these properties can also manifest through architectural choices within the conventional supervised training paradigm. 

The authors provide a comprehensive overview of the background literature necessary for grasping the paper's content, rendering their work self-contained and accessible. Moreover, the paper carefully examines both qualitative and quantitative metrics to support its claims. Specifically, the authors demonstrate that their findings hold across diverse datasets, varying model sizes, and a spectrum of evaluation metrics. The inclusion of insightful ablation studies, notably the architectural modification of ViT (specifically, the replacement of MHSA with MSSA), adds depth to the analysis. A notable feature of this paper lies in its detailed description of the experimental methodologies, thoughtfully provided in the appendix (huge plus!). This greatly improves the ability to replicate the results and encourages their use as a basis for subsequent research endeavors. 

In conclusion, the findings presented in this work hold substantial value for the research community. They introduce and validate novel and intriguing insights previously unexplored, effectively challenging established beliefs regarding the emergence of segmentation properties in ViTs.

A few minor comments and questions for the authors:

1. Could you elaborate more on the thought process that led you to investigate this specific architecture choice? Usually, papers introducing novel architectures lack insight into this aspect, and such findings may appear as if they came out of nowhere. However, I believe that architectural improvements are typically the result of an iterative process, often involving failed attempts. Including a paragraph describing other architectural options you explored (if any) and explaining how and where they fell short in producing segmentation properties would be highly beneficial. This information could prove invaluable to fellow researchers seeking to build upon your work, potentially saving them time and effort by avoiding similar pitfalls.

2. Your paper demonstrates the emergence of segmentation in the attention maps of CRATE trained in a supervised manner on ImageNet-21k. Have you observed these same segmentation properties persisting after fine-tuning the model for other downstream tasks, or do these features tend to diminish during the transfer-learning process? It would be valuable to include an analysis of some of the transfer-learning datasets mentioned in Appendix C.2 to shed light on this aspect.

3. Figure 11. has a typo in the x-axis label: “Epocs” -> “Epochs”

4. Looking at Figure 11 (left), it’s evident that AP score saturates after 9th epoch. Could you provide the same analysis as in  Figure 11 (left and right) for at least one more model and one more dataset? This additional data would provide insight into when these segmentation properties typically manifest during training. Are they consistently present early in training, or does their appearance depend on factors like model size and dataset selection? 

5. Could you also provide the same analysis as in Figure 11 for classic ViT models. This would offer further insights into whether classic ViTs could have benefited from extended training or if they also saturate early in training, albeit with notably lower scores."
372,"Brief Overview of the Paper
The paper presents a novel approach to predicting Myocardial Injury after Non-cardiac Surgery (MINS) using a Retrieval Based Disease (RBD) prediction framework. This method innovatively addresses the challenge of analyzing complex, unstructured clinical data by transforming it into a coherent text description for improved decision-making accuracy. The authors claim their Language Model (LM)-based model outperforms traditional Machine Learning (ML) models, offering promising results that suggest further validation is needed in larger datasets or new clinical scenarios with different disease prevalence.

Quality
Technical Soundness and Description
  The methods and accompanying figures presented by the authors are technically sound and well-described, providing a clear overview of the proposed RBD framework and its advantages over traditional ML approaches. The incorporation of various types of clinical data into a unified text-description model showcases a significant advancement in handling and interpreting complex data for disease prediction.

Addressing Data Imbalance:
  While the authors acknowledge the challenge of working with an imbalanced dataset due to low disease prevalence, the paper would benefit from a more detailed discussion on how to address this issue. Suggested improvements include implementing resampling strategies, such as oversampling the under-represented disease class, to mitigate the imbalance's impact. Additionally, incorporating Receiver Operating Characteristic (ROC)-Area Under Curve (AUC) comparisons among the models could provide deeper insights into their performance, especially in managing false positives and negatives in an imbalanced dataset context.

Clarity and Justification
 The statistical methods used in the paper are clearly explained and justified. The authors' approach to analyzing the data and the results presented are sound, offering a solid foundation for their conclusions.

Recommendation & Significance 
This paper introduces a compelling and innovative approach to disease prediction using LM-based models, addressing significant challenges in analyzing unstructured clinical data. While the results are promising, addressing the highlighted areas for improvement, particularly around data imbalance and statistical analysis depth, could significantly enhance the paper's impact and validity. Further validation of the proposed model in broader clinical settings is recommended to substantiate its effectiveness and applicability in real-world scenarios.

Additional Comments
Strengths:
- The novel RBD framework represents a significant innovation in leveraging LM-based models for disease prediction, especially in dealing with complex, unstructured clinical data.
- The initial findings suggesting superior performance of the RBD framework over traditional ML models are promising and warrant further investigation.
Weaknesses:
- The paper lacks a comprehensive strategy to address the issue of data imbalance, which is crucial for validating the model's effectiveness across different clinical scenarios.
- The statistical analysis would benefit from more detailed comparisons between models and a clearer justification for methodological choices, such as the selection of k-values.

Recommendations for Improvement:
To strengthen the paper's statistical analysis, it would be beneficial to include a direct statistical comparison of the models' recall and F-1 scores. This comparison could highlight the proposed model's effectiveness more clearly against traditional ML approaches. Additionally, the rationale behind selecting a k-value of 5 for certain analyses requires further explanation. Expanding on this decision could enhance the reader's understanding of the methodology and its implications for the study's findings."
373,"The authors study the quantum version of Cover’s portfolio problem. While it is known that matrix multiplicative weights yield a regret bound of d^2 log(T) for this problem, this method lacks an explicit implementation and requires the computation of complex multidimensional integrals. Existing algorithms in the literature give polynomial-time computation but have suboptimal regret bounds. The authors build on recent work by Jezequel, Ostrovskii, and Gaillard to achieve an almost optimal regret of d^2 log(d + T) via semi-definite programming.

One potential weakness of the paper is that it is not clear what the computation time is. Although only semi-definite programming is required, the polynomial dependence in the quantum case could be significant. It would be useful to compare this with the mentioned complexity bounds of O(d^3) per round, which, while suboptimal in terms of regret, offer more straightforward computation.

Although the analysis follows several steps outlined in the work by Jezequel, Ostrovskii, and Gaillard, the paper introduces technical tricks and distinctions, which are clearly highlighted. I liked that contributions are presented transparently and without unnecessary claims. While I have not checked the proofs in depth, the overall context suggests the results and analysis are solid. The topic is very relevant, and the paper deserves to be accepted."
374,"The reviewer, while not an expert in statistical physics, offers comments from a machine learning standpoint. The paper explores a novel coarse-graining method via reparametrization, circumventing the 'Force-matching' (dimensional reduction) and 'Back-mapping' (reconstruction) steps by employing a reparametrization X = ρ(Z), where ρ : Z → X. However, the reviewer perceives this process as analogous to dimensional reduction in machine learning and akin to the 'Force-matching' concept, thus finding limited novelty from a learning perspective.

Nevertheless, the discussion on physical Hessians proves to be intriguing and thought-provoking. The author demonstrates that many physical problems exhibit a distinctive structure within the Hessian matrix, leveraging this property to devise an algorithm that projects onto the slow manifold."
375,"The paper studies error rates for weak to strong learning with voting classifiers. Previous to this work, error bounds for voting classifiers, such as the classical AdaBoost, have been shown to suffer from a double logarithmic factor in the sample size $n$, as compared to an optimal, non-voting classifier, whose error bound has no logarithmic factors. Here the authors introduce a new voting classifier and establish an error bound for it that has only a single logarithmic factor in the sample size (while paying in worse dependency on the weak learner parameter $\gamma$).

The new voting classifier is closely related to AdaBoost in the sense that at each iteration the algorithm draws a small (its size depends on $\gamma$ but not on $n$) random subsample according to the same distribution that would have been used by AdaBoost on the entire sample, and the weak learner is invoked on this subsample.  The voting is then done by a non-weighted combination of the classifiers produced over many iterations (whose number is logarithmic in n). The authors show that this method corresponds to a stable randomized compression scheme, which is a newly introduced random extension of stable compression schemes. The latter has been recently shown to enjoy better error bounds by a logarithmic factor, as compared to standard or permutation invariant compression schemes. The authors establish that their new randomized scheme enjoys this logarithmic factor saving as well, which allow them to conclude the aforementioned tighter error bound.

Overall, I found the paper and its contributions to be quite nice and the results seem technically correct. I only have a few minor comments.

1. As opposed to AdaBoost, the new method here needs to know the weak learner parameter $\gamma$, which is typically unknown, in order to determine the subsample size. Can this be avoided?

2. How do the hidden constants in the error bound of Theorem 1 depend on the distribution $D$? 
In Section 1.1, I believe ""The size of the compression scheme is the supremum over $S$ of $|k(S)|$"" should be for a given size of $S$.

3. The letter $k$ is used for both iteration number and compression encoding function, which is a bit confusing in first reading."
376,"The authors present an approach to predicting the next most likely disease /outcome for a patient given a previous sequence of outcomes. They focus on using the icd10 ontology. The model is an LLM that is further fine tuned on intermediate predictive tasks such as recalling definitions of codes. The authors trial their approach on open source EHR data and show good performance. The paper is a good contribution to an important area of research.

In general I have two criticism to offer.  My criticism is on the practicality of using approach in the real world. Based on my reading it assumes codes are accurately recorded by the clinician/clinical care team following a given patient episode. In reality clinicians are not very good at recording this level of data. At what stage of the clinical workflow would this 'model' would this be useful?  

Secondly I am not sure how much is lost by discarding the actual clinical text as part of the input data. This approach seems to model the problem as a sequence of codes (or sequence of bag-of-codes). LLMs are designed to interpret free text so why not take advantage of this?

Overall I think the paper presents an interesting and somewhat original solution to the problem. I think the clarity of writing can be improved and ideally some justification for design choices needed to be provided."
377,"The paper investigates the baseline language variations in normal aging to understand cognitive changes. It utilizes NLP tools and psycholinguistic metrics to analyze natural speech data, aiming to establish a normative benchmark for aging language, which could assist in assessing cognitive aging. 
Pros: The paper used NLP tools to objectively analyze natural speech data, overcoming the subjectivity in manual assessments of language abilities, besides, it provided a detailed year-by-year analysis of linguistic characteristics influenced by age and sex.
Cons: However, there are a few limitations to this study: it excludes individuals older than 80 years and those with less than 13 years of education, which could omit valuable insights from these groups. While the approach reduces subjectivity compared to manual assessments, the choice and interpretation of psycholinguistic metrics could still introduce bias.

Overall, the study presents a step forward in understanding language variation in aging."
378,"This work explores the Clustering with Bandit Feedback Problem (CBP), where a learner engages with an \(N\)-armed stochastic bandit system providing \(d\)-dimensional sub-Gaussian feedback, in order to uncover a hidden partition of the arms into \(K\) groups. Each group shares a common mean vector for all arms within it. The primary objective for the learner is to identify this hidden partition with a minimum observation budget and with an error probability below a specified threshold, $\delta$.

The paper established a Non-Asymptotic Lower Bound on Observation Budget for the observation budget required to achieve the specified error threshold, which matches the information theoretical lower bound up to multiplicative log factors. 

The main algorithm innovation is from combining the Sequential Representative Identification and Active Distance-based Classification subroutines together, being able to greatly reduce the number of observations compared to the batched sampling. This adaptive approach provides a substantial improvement in the performance, ensuring that clustering is achieved with minimal budget expenditure.

A notable finding in this paper is the demonstration that, contrary to the batch setting, there is no computation-information gap in the sequential setting. 

 The paper is very well written and organized.  I examined the mathematical proofs presented in the main body. Empirical evidences are included to support the claims. Overall, this paper makes meaningful theoretical advancements in understanding the observation budget and computational requirements for clustering in the bandit feedback framework.

Therefore, I recommend this paper for acceptance."
379,"This paper investigates the deployment of lightweight medical image segmentation models on CPUs to improve performance and inference speed. It utilizes models like EfficientSAM and LiteMedSAM for different modality, optimized through methods such as model quantization and the ONNX format. The authors demonstrate significant accuracy and runtime improvements across various medical imaging modalities, including PET scans, by integrating strategies like majority voting and fine-tuning with Sharpness-aware Minimization. The detailed methodology and experimental results indicate that the paper is thorough and provides sufficient information for reproducibility. 

As a summary, I recommend this paper for acceptance due to its significant contributions to performance and efficiency on lightweight MedSAM, with good details for reproducing."
380,"In this work, an approach for obtaining a sparse neural network for an LTI system is introduced. 
The short “workshop” version of this paper as submitted appears to be a high-level summary of a lengthier manuscript, with a large number of references to sections in the Appendix where the details and theory describing the approach is contained. Therefore, I cannot claim to fully understand the approach. 
Nevertheless, I do think the contained ideas are interesting. I appreciate the author’s position that finding+fitting a neural network to an LTI system is an interesting exercise in obtaining sparse and accurate neural networks in a toy setting. However, one concern about abandoning a data-driven approach in favor of a (more principled) gradient-free approach is that this may scale quite poorly to more complex settings.
Comparing this approach to Neural Architecture Search baselines would be helpful as well, particularly any which attempt to enforce sparsity."
381,"The paper discusses leveraging Large Language Models (LLMs) for survival analysis in medical prognostic and diagnostic applications. It explores methodologies for estimating medical risk, with a focus on survival analysis, addressing the challenges posed by censoring in medical studies. The document reviews current LLM strategies for survival analysis, detailing their limitations and strengths, and proposes an open-source implementation for comparing these strategies. It aims to develop evidence-based recommendations for the effective use of LLMs in estimating patient survival outcomes.

**Pros**
- Comprehensive Approach: The paper provides a thorough overview of using LLMs for survival analysis, covering various methodologies and their adaptations for this specific application.
- Practical Contributions: By offering an open-source implementation, the work facilitates practical experimentation and comparison of different LLM strategies for survival analysis.
- Addressing a Crucial Challenge: The paper tackles the significant challenge of censoring in survival analysis, proposing methods to improve modeling under such conditions.

**Cons**
- Generalization of Findings: The paper might benefit from a broader evaluation across different types of LLMs and datasets to ensure the findings' generalizability.
- Detailed Evaluation Framework: While it proposes an open-source implementation for comparison, the paper could provide a more standardized evaluation framework for assessing the performance of LLM strategies."
382,"Summary:
- This paper proposed an augmentation technique that improves performance for LLMs on ARC and shows that finetuning based on these augmentations further improves performance. 
Additionally, it proposes a reflection system that enables the effective combination of multiple independent solvers.

Strength:
 - Identification of augmentations that bring improvement to ARC for LLMs
 - Evaluation of effect of QLoRA fine-tuning on AugARC for different models
 - In-depth analysis of solution overlap of different algorithms
 - Proposal of a reflection system that is able to select the most promising outputs among a selection of candidates
 - Running eval on complete evaluation set, enabling ease of comparison.
 - Good limitations section

Weaknesses:
 - Similarities of test-time augmentation scheme in AugARC to the one propozed by (Bober-Irizar and Banerjee 2024)
 - Minor improvements over DSL search using massively more compute (Claude 3 Opus is rumored to be quite large)
 - Exact prompting scheme for the reflection model is unclear
 - Missing insights into performance of reflection model
 - Ablations seem to be missing some scientific rigour


Questions and points for improvement:
- Figure 3: reflection model chooses the wrong solution in example 1, but the caption claimes it is the correct solution
- On page 4, you mention that for the remainder of the experiments, you are going to use AugARC. Since you call AugARC a benchmark, it is then rather confusing when you talk about ARC performance.
  It would make more sense to just call AugARC an augmentation for ARC, since it is not really a new benchmark but just an augmentation scheme, that can be used for training and at test time.
- Figure 4 would be easier to read if sorted by performance.
- Why is Figure 5a symmetric? Shouldn't it be normalized by the total number of tasks of the model on the bottom? With the current approach, you show overlap only in one direction.
- Also in Figure 5, you claim that the models are ordered by how much gain they add, but this does not seem to be the case?
- Which version of Gemini Pro are you using, I assume 1.5?
- You propose to use permutations as augmentations. If I've read the paper correctly, you never finetune on a dataset that includes permutations, which leaves it unclear whether permutations also yield a benefit for finetuning.
  A larger number of permutations could also lead to an imbalanced dataset due to tasks with different number of examples, eventually hampering the effectiveness of the augmentation.
- Did you also consider permuting the colors of the puzzles?
- It is commendable that you have evaluated many different models. However, your inconsistent use of different models for the different ablations and their inconsistent ordering in figures leaves a weird impression.
  * Ordering in Figures 4 and 5
  * Not all models of Figure 4 are present in Figure 5, how are they selected?
  * GPT models used as Reflection models, but not used as solvers
- Some more detailed ablations on the main contribution, the reflection system, would be interesting.
  * The upper bound of improvement over DSL search would be 23, the reflection system reaches 6.
  * In the setting where you use 3 solvers, does the fine-tunes Llama-3 8B even provide any potential new solved tasks to begin with?
  * Ablation with third solver for reflection system not really comparable to two solvers due to different reflection model used.
  * Can a reflection model provide a benefit if it is the same model that has been used as a solver, or does it need to be a different model?
- This is interesting work as it improves performance on ARC, but how does this contribute to the original goal of ARC, to achieve more intelligent and more human-like artificial systems?
- The conclusion does not fully reflect the content of the paper
	

The augmentations proposed in AugARC, mainly the permutations, could lead to an imbalanced dataset, eventually hampering the effectiveness of the augmentations.

Minor:
 - Weird paragraph break at the end of page 1
 - page 4: ""Each solver solver independently and cannot...""
 - Weird paragraph break at the end of page 6
 - page 7: ""by 6 ARC tasks. the Reflection System...""
 - There is also research on LLMs for the bloom model series (Camposampiero, Giacomo, et al. ""Abstract visual reasoning enabled by language."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.)
 - Further, (Wang, Ruocheng, et al. ""Hypothesis search: Inductive reasoning with language models."" arXiv preprint arXiv:2309.05660 (2023)) could also be included in the related work, as it is quite relevant.
  Unfortunately, they only provide results on a random subset of the ARC evaluation set, so direct comparison is not really possible.
 - ARC has been renamed to ARC-AGI"
383,"The authors evaluated the performance of different fine-tuning strategies on medical QA tasks. A variety of medical QA datasets were used in the evaluation. Full parameter fine-tuning vs. LoRA fine-tuning were used and compared. Zero-shot performance were used to evaluate the model performance. 

Only two different size Llama-2 models were used as the base model. Some studies have shown that different base models may provide different level of improvement after fine-tuning, so it would be great to see the results from some other commonly available open source models such as mistral. 

The results were not new and mostly expected. Similar studies have been conducted and the results of this study were generally consistent with some previous findings. The authors did include more QA datasets for the evaluation, which made the results more comprehensive."
384,"The paper presents a comprehensive study on the application of self-supervised learning (SSL) in computational pathology, focusing on the pre-training and downstream performance evaluation of visual foundation models on large-scale pathology tasks. The study compiled a massive academic pathology dataset, consisting of over 3 billion images from 423 thousand digital microscopy slides, and compared the pre-training of visual transformer models using masked autoencoders (MAE) and self-distillation models (DINO). The downstream performance was evaluated on six clinically relevant tasks from three anatomic sites and two institutions, demonstrating the benefits of pre-training on pathology data for downstream performance compared to pre-training on natural images. The DINO algorithm achieved better generalization performance across all tasks tested, signifying a significant advancement in computational pathology research.

Pros:

The study addresses a critical gap in the application of SSL algorithms and foundation models in the medical domain, particularly in computational pathology.

The compilation of the largest academic pathology dataset to date, consisting of over 3 billion images, demonstrates a significant contribution to the field.

The comparison of pre-training methods and evaluation of downstream performance on clinically relevant tasks provides valuable insights for the development of performant models in computational pathology.

The study's findings indicate a phase change in computational pathology research, paving the way for more performant models based on large-scale, parallel pre-training at the billion-image scale.

Cons:

The study could benefit from a more detailed discussion on the limitations and challenges of SSL algorithms and foundation models in the medical domain, particularly in clinical workflows.

While the downstream performance was evaluated on clinically relevant tasks, the study could further emphasize the potential impact of these findings on real-world clinical applications.

The document lacks a detailed discussion on the ethical considerations and potential biases associated with the use of large-scale pathology datasets and SSL algorithms in healthcare.

Overall, the work demonstrates high quality, clarity, originality, and significance in advancing the application of SSL algorithms and foundation models in computational pathology. The study's comprehensive approach, large-scale dataset compilation, and valuable insights into pre-training methods and downstream performance evaluation contribute significantly to the field of computational pathology. However, further discussion on ethical considerations and potential biases, as well as the translation of findings into real-world clinical applications, would enhance the overall impact of the work."
385,"The paper proposes DeiSAM, a neuro-symbolic pipeline for segmenting objects in images based on deictic prompts, which are natural language descriptions that refer to objects depending on the context. The paper also introduces a novel benchmark, DeiVG, which contains visual scenes paired with complex deictic prompts. The paper shows that DeiSAM outperforms neural baselines on the deictic segmentation task and demonstrates its reasoning capabilities.

### **Strength**

- Using LLM to generate first-order logic is a very promising direction for the neuro-symbolic approach, which combines the power of neural representation into the commonsense reasoning in LLM. Similar approaches have been proposed in [1]. 
- The scope is clearly stated and empirical results against grounded SAM are impressive. 

[1] Hsu, Joy, et al. ""What's Left? Concept Grounding with Logic-Enhanced Foundation Models."" arXiv preprint arXiv:2310.16035 (2023).

### **Weakness**

- More details on LLaVa baselines will be helpful, as it belongs to another category. How do you compare the visual instruction tuning baseline with prompting?"
386,"The paper introduces and briefly discusses three main additions to the (Lite)MedSAM baseline:
* **Altered Model Architecture**: The authors replace the image encoder of the baseline model with EfficientViT-SAM-L0.
* **Increased Speed through Quantization**: They explore static and dynamic quantization, demonstrating benefits such as reduced inference time while maintaining accuracy compared to the non-quantized model.
* **Enhancing Input for Grayscale Images**: For single channel modalities, they show that applying transformations to the original input image and using those altered image as further input improves segmentation results compared to just duplicating the grayscale image over all 3 input channels.

Overall, the paper is clearly structured and understandable and checks all boxes in table 6. The authors plan to publish the model's code and have disclosed their training data, making it feasible to reproduce their experiments.

Positives:
* They show that a more lightweight image encoder can be used in the SAM framework.
* Simple yet effective enhancement for improving image segmentation quality for grey-scale images.

Negatives:
* The baseline scores from Table 4 do not match with the reported numbers here https://www.codabench.org/forums/1766/247/ 
  However it is not clear whether the baseline was tested on the validation set or on their hold-out part of the training set. Clarification would increase trust in the paper."
387,"# Summary

The paper derives an adjoint method that allows the delay in neural delay differential equations to be learned rather than specified *a priori*.
In experiments on simple systems, the proposed algorithm is able to recover the correct parameters and delay from the training data.

# Strengths
1. (**significance**)
The paper builds well upon previous work and addresses an interesting, impactful problem.
2. (**quality**)
The main contribution of the paper is the proof of the adjoint equation in Appendix B, which is thorough and well written, although I haven't verified its correctness.
3. (**clarity**)
More generally, the presentation of the paper, including figures, theorems, and algorithms, is excellent.
4. (**completeness**)
The authors set out extensive and clear opportunities for further work.

# Weaknesses
1. (**completeness**) 
While the question of ""limited and noisy data"" is mentioned briefly in the conclusion, I believe this is a fairly fundamental issue for the method that deserves detailed discussion.
In particular, the method requires interpolating between data points, since the delay $\tau$ is treated as a continuous variable while the data is necessarily discrete.
I strongly suspect that the interpolation may struggle in the presence of noisy, sparse, or irregularly sampled data.
Therefore, a complete discussion of the limitations of this method should include a detailed examination of this point.
1. (**completeness**) 
The experiments, while a reasonable proof of concept, are perhaps somewhat trivial, consisting each of only two learnable parameters in addition to the delay.
I would be very interested to know how well the proposed algorithm performs in the presence of many more parameters; for example, in [1] the authors train NDDEs with $\mathcal{O}(100k)$ parameters.
1. (**completeness**) 
Another discussion missing from the paper is the possibility of using a *discretize-then-optimize* approach to training NDDEs, i.e., applying automatic differentiation directly to the operations of the numerical ODE solver, as opposed to *optimize-then-discretize*, i.e., adjoint sensitivity analysis; see [2].
While I'm not certain of this, it may be possible to skip step 5 of Algorithm 1 entirely and do step 6 using automatic differentiation instead of using Eq. 4.
At the very least, I think it would be interesting to compare such an approach to the adjoint method derived in this paper.
1. (**completeness**) 
There appears to be some closely related work that is not mentioned in the paper [3].

# Other Comments
1. A link to a GitHub repository is included on page 3, but I didn't click on it and I don't think any identifying information about the authors can be determined from the URL.
1. One easy-to-fix problem with the paper's presentation is the bibliography, where some references are given to arXiv preprints instead of the relevant conference proceedings.

# Conclusion
Overall, I like this paper.
The weaknesses discussed above are intended only as constructive criticism that might benefit future versions of this work.

# Citations
[1] Qunxi Zhu, Yao Guo, and Wei Lin. Neural Delay Differential Equations. In *International Conference on Learning Representations*, 2021.

[2] Patrick Kidger. On neural differential equations. arXiv preprint arXiv:2202.02435, 2022.

[3] Xunbi A. Ji, Gábor Orosz. Learning Time Delay Systems with Neural Ordinary Differential Equations. IFAC-PapersOnLine, Volume 55, Issue 36, 2022."
388,"Overview

This paper investigating the effect of different DST components under the CL paradigm by performing a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImageNet benchmarks in a task-incremental CL setup.


Strengths

1. It is very interesting results that ERK and uniform sparsity experience different performance at different sparsity levels under CL setting. The results have practical significance to the further research on CL.

2. Adaptivity within DST is also evaluated under CL setting, and the design is very important in practical use of DST, not just in CL.



Weaknesses

1. The observation is sufficient, however, there is no detailed discussion on why the DST has such difference in CL in terms of its settings on sparsity.

2. The main body of the paper should have more discussion on the rationale of DST’s different settings."
389,"The paper proposes a privacy-preserving synthetic data generation technique PriSHA for assisting training clinical foundation models. The DPGAN and PATE-GAN are combined to make the synthetic data generation private, enhancing the privacy protection capability. The experiments show slight improvement of PriSHA over PATEGAN Standard.

Strength:
1. Privacy protection and using synthetic data is very important in healthcare.
2. The combination of existing techniques makes sense.
3. The experimental results are surprisingly good enough.

Opportunity to improve:
1. I am not sure if I buy in the concept that the proposed method can ""address distribution shift"". To me, it is just combining two data sources, except that one is synthetic. It is quite different from deliberate design of tackling distribution shift. It would connect more dots to federated learning.
2. I understand DPGAN provides noises to protect training data, while PATE-GAN trains student models to generate synthetic data for model training. I am puzzled why the improvement is not over standard DPGAN, but over the PATE-GAN. More importantly, I do not think synthetic data can significantly improve model performance in general, since it is impossible to generate OOD data out of the scope of the existing data distribution to provide new information. This should be not be the main focus of the study.
3. There is no evaluation and real-world cases showing whether the privacy is protected well enough. This is the main motivation of this paper, but it is poorly presented and rarely discussed."
390,"1.It's better to do comparison with the state-of-the-art decision deep networks/tree-based models
2.it's better to do experiments for large-scale datasets"
391,"This work introduces a self-supervised learning model for EEG data analysis aimed at improving transferability and interpretability. While the approach is innovative, several critical aspects require scrutiny:

1. The concept of leveraging self-supervised learning for EEG data is not novel. The paper must delineate clearly how EEGFormer diverges from and improves upon existing models like BrainBERT or SeqCLR in terms of architecture, learning efficiency, or application to diverse tasks.

2. While the paper asserts that EEGFormer offers interpretable outcomes (which I think it is explainability rather than interpretability), it also falls short of providing a comprehensive framework or quantitative measures for interpretability. The paper should incorporate case studies or comparisons with expert analyses to substantiate these claims.

3. The work needs to show a few-shot learning performance in order to be a foundation model."
392,"The paper studies online learning of quantum states with logarithmic loss (LL-OLQS). They extend the VB-FTRL algorithm by Jezequel et al. (2022) from the online portfolio setting to the LL-OLQS setting. This results in an polynomial-time algorithm achieving d^2 log(d+T) regret. This gives a different solution than Zimmert et al. (2022)'s universal portfolio and BISONS, improving the first's computational complexity and the second's regret bound. 

I believe the result is novel and the analysis is sound based on my own reading. However, I hope that more high-level explanation and the ideas behind the proof could be spent in the main text, instead of just laying out the proofs, even if the ideas may already be in the previous work of Jezequel et al. (2022).  For example, what's the motivation behind the choice of the regularizer, and why it addresses the problem better than previous work.  In particular, I find that in your analysis there is a term ""Gain_t"", which induces a negative regret cancelling out ""Miss_t"".  On the other hand, Zimmert et al. (2022) said that they use ""negative regret via linear bias"", whose goal is also to cancel out another positive term.  Are these terms in your paper and their paper related to each other? What's the key to improve their bound? Also, it is mentioned that your analysis ""avoid the affine reparametrization step"" in Jezequel et al. (2022). What's the implication of this? Does it simplify the proof, or improve the bound (i.e., if you don't use it you will get worse bound)? Please clarify. 

Besides, what is the more precise per-round complexity of your algorithm? It is only stated as ""the complexity of solving a semidefinite program"", but this doesn't seem to be sufficient for a fair comparison with previous work (like universal portfolio).  

Besides, I would prefer the paper to provide more intermediate steps of calculation, rather than saying ""a direct calculation gives..."" and omitting details. For example, at the bottom of Page 7.  It will just make the proofs easier to follow.  

Below are minor points: 
- ""VB"" always appears in the abbreviated form. It should be spelled out at the first time
-  Typo: Lemma 11  ""LB-convex""-->""VB-convex"""
393,"The paper proposes a lightweight MedSAM using post-training model quantization techniques and some other techniques. The results show a better performance than the baseline. Here are some advices:

1. Formatting issues. 

    1.1 The ORCID for all authors should be included in the paper. 
    
    1.2 The page headers should be rewrited from the template.
    
    1.3 The last line in Table 1 has an extra blankline.
    
    1.4 Table 3 exceeds the default layout. 
    
    1.5 In Table 3, it states “DSC(%)” but what follows is not in percentage form. 
    
    1.6 Fig.1 and Fig.2 has wrong font, as it has been requested that the font in   figures should be Times New Roman.
    
    1.7 Format wrong in Section 3.2, ""1. Data augmentation"". 

2. The description of how the post-training model quantization techniques is implemented should be expanded.

3. Ground truth segmentation results should be added to section 4.3 for better evaluation on the images."
394,"># Quality
The work is of high quality, as it demonstrates a thorough understanding of the clinical domain and the challenges of building a clinical large language model (CLaM). The authors justify their choice of foundation model clearly. They also address several modeling challenges, such as long context window, medical jargon, and abbreviation expansion. Their models are evaluated using both next-token prediction and blind pairwise comparison with other popular LLMs, showing superior performance in patient clinical data structuring.

># Clarity
The work is well-written and organized, with clear problem formulation, methods, results, and discussion. The authors provide sufficient details and explanations for their data collection, model training, and evaluation methods, including several appendices with examples of their models’ outputs, ethical considerations, and reproducibility statement.

># Originality
This work is indeed original as it introduces a novel family of CLaMs designed for patient clinical data structuring, a crucial yet intricate component of clinical workflows.

># Significance
This work is significant, as it addresses a critical problem of structuring clinical notes into clinical data, according to international interoperability standards.

># Pros
* This work tackles an important area of healthcare and has a lot of potential significance.
* The work evaluates the models using both next-token prediction and blind pairwise comparison with other LLMs"
395,"1. Summary:
The paper begins by highlighting the challenges posed by the growth and complexity of data to traditional centralized machine learning. Distributed learning has emerged as a solution, with federated learning (FL) being a notable application. FL aims to protect client data privacy by keeping training data local. However, even without uploading raw training data, FL can still be vulnerable to data leakage.

2. Main Contributions:
a) The paper identifies the possibility of recovering private training data in FL using only transmitted model parameters and loss functions, challenging the foundational security of FL.
b) Two novel attack frameworks, DLM and DLM+, are introduced. These frameworks are applied to FedAvg, a widely-used algorithm in FL. c) The results show that FL architectures that exchange model weights cannot fully protect client data.
d) The paper compares the proposed model leakage attacks with existing gradient leakage attacks, demonstrating the superiority of the new attacks. Additionally, two defenses against these attacks are introduced.

3. Pros:
a) The paper addresses a critical security concern in federated learning, enhancing the understanding of potential vulnerabilities.
b) Introduces two novel attack frameworks, DLM and DLM+, offering a comprehensive approach to understanding data leakage in FL.
c) Provides defenses against the proposed attacks, ensuring a balanced perspective on the issue.

4. Cons:
a) The paper's focus on transmitted model weights might limit its applicability to FL systems that don't rely on this method.
b) The complexity of the proposed frameworks might make them challenging to implement in real-world scenarios.
c) The paper assumes that attackers have access to certain transmitted data, which might not always be the case in secure FL implementations."
396,"This paper makes solid progress with the sample complexity of plugin approaches for average-reward MDPs (AMDPs).  

# Pros

- This is the first paper giving optimal sample complexity proofs to plugin-type approaches for the policy optimization problem in AMDPs. The plugin approach is the most natural and simplest model-based algorithm for the policy optimization problem in MDPs. Moreover, when the setting is AMDPs, the plugin approach is more natural (and maybe more useful from an algorithmic perspective) than the methods based on the reduction to DMDPs. Hence, the sample complexity of the plugin approach is a fundamental problem. This work, though still a little way from fully solving this fundamental problem, made considerable progress on this basic problem.

- This is the first paper giving optimal algorithms (in the sense of optimal sample complexity) w.r.t. diameter $D$ and/or uniform mixing time $\tau_{unif}$ of the AMDP without the requirement of prior knowledge of these parameters.

- This is the first paper proving the simplest plugin approach without reward perturbation is sample optimal for the full range of sample sizes in DMDPs. This is also the paper extending the range of $\epsilon$ to the full range for the optimal sample complexities w.r.p. $D$ or $\tau_{unif}$ in DMDPs.

- This paper has solid proofs with new techniques (though I only get to skim them through), and it is well-written.


# Cons

- The bound (Theorem 1) for the simplest plugin approach has dependencies on some unclear terms ($\|\widehat h^*\|_{span}$ and $\widehat \gamma^*$) and has the extra requirement of the connectivity of the estimated MDP.
  
- We usually want the bound to only depend on the parameters of the MDP ($S, A$, and like $H = \|h^*\|_{span}, D, \tau_{unif}$) or the problem ($\epsilon, \delta$ and $\gamma$ in DMDP). It will be have been better if we can bound $\|\widehat h^*\|_{span}$ and $\widehat \gamma^*$ (which appear in the bound of Theorem 1) by these parameters. A lower bound showing a higher order dependence on the common parameters, if it is the truth, is also welcome.
  
- It would also be ideal to eliminate the requirement that the estimated MDP is weakly communicative (e.g. prove it holds with probability $1-\delta$); or to prove for any polynomial samples $n$ and tolerance probability $\delta$, it is inevitable to exist a non-weakly-communicative estimated MDP with probability $> \delta$.

- The results (Corollary 5, Corollary 7, Theorem 8) that provide algorithms with optimal sample complexities w.r.t. $D$ or $\tau_{unif}$ don't require prior knowledge of these MDP parameters, but only in either one of the parameters.
  
- When the sample size $n$ is given, the accuracy of the resulting policy can be bounded by $n$, and the algorithms really don't depend on $D$ or $\tau_{unif}$.
  
- But when the required accuracy $\epsilon$ is given, the algorithms will actually depend on $D$ or $\tau_{unif}$. The algorithms are the plugin approach with the anchoring technique (and with or without reward perturbation), and the anchoring technique needs a parameter $\eta$, which is set to be $\frac{1}{n}$ in the algorithms. To obtain the certain accuracy $\epsilon$, since the proved sample complexities are in terms of $D$ or $\tau_{unif}$, the algorithms will have to set $n$ to depend on these parameters, which will be further used for $\eta$ in the algorithms. This leads to that the algorithms actually require prior knowledge of these parameters, if I understand correctly. Can the authors clarify it a bit?

- The hardness result (Theorem 14) seems a bit incomplete to me. It cannot show a necessity of $\|\widehat h^*\|_{span}$ term in the complexity.
  
- The hardness just proves a lower bound of $\epsilon = \omega(\sqrt{\frac{H \log(Hn)}{n}})$, but in a precise sense, to show, as said by the author, the $\|\widehat h^*\|_{span}$ term can not be removed from the bound in Theorem 1, what we need to prove is $\epsilon = \omega(\sqrt{\frac{(H+1)\log^3(\frac{SAn}{\delta (1 - \widetilde\gamma^*)})}{n}})$, if I understand correctly. Can the authors comment a bit on this?
    
- Another reasonable way to show the necessity of this term might be to prove a lower bound with this term directly appearing in the bound. It would be good if the authors can add some discussions on the challenges of obtaining such a bound.

- This paper only proves that the plugin approach, with the anchoring technique and with reward perturbation, achieves the optimal sample complexities w.r.t. $D$ or $\tau_{unif}$. But two important problems remain:
  
  - Can the simplest plugin approach itself achieve the optimal sample complexity? If not, what is the complexity? How may the techniques and results in this paper help us understand the near-optimality of the vanilla plugin method for AMDPs?
  
  - Can we improve the sample complexities for the plugin approach with the anchoring technique and with reward perturbation, by replacing the $D$ or $\tau_{unif}$ in the bound with the parameter $H = \|h^*\|_{span}$? It's known that $H$ is smaller than both $D$ and $\tau_{unif}$.

# Some typos

- Sec 1.1 Line 8: should be $\widetilde O(\frac{SA \|\widehat h^*\|_{span}}{\epsilon^2})$ instead of $\widetilde O(\frac{SA \tau_{unif}}{\epsilon^2})$

- After Corollary 5 Line 4: should be Corollary 5 instead of Theorem 5

- Theorem 13 Line 2: should be Algorithm 2 instead of Algorithm 9"
397,"This paper tackles an important research question: how to improve LLM's reasoning and multi-hop QA abilities. Its main contribution is a collection of newly annotated human-feedback data for over 2k examples about correctness, error types and explanations in two tasks: StrategyQA and Sports understanding. However, the algorithms proposed are not particularly novel and the evaluation is especially weak.

Specifically, there are some major weakness on the evaluation, which raises more questions on the approach/algorithm used:

1) The performance gains were inconsistent across the two datasets
2) When evaluating the prediction of the final correct answers, the main evaluation metric is the accuracy. Since the majority of examples in the test sets have errors in the reasoning chains (see Table 2 in paper), this is a highly imbalanced classification problem. The use of accuracy alone is misleading since the models can be biased towards majority ""incorrect"" predictions. Other metrics such as Precision and Recall or F1 score could be used and reported to get a better error breakdown picture of the model performance.
3) There is no human or expert evaluation to deeply assess reasoning improvement qualitatively. This could shed more lights on the actual improvement
4) The proposed approaches are basic. Meta-learning tailored for reasoning could be more effective.
5) No ablation study is done to remove different feedback signals (e.g, error type vs. error description vs. corrections) from training. Such experiments could reveal which forms of feedback are most valuable for enhancing reasoning performance."
398,"This work presents an AutoBasisEncoder, which is a framework designed for operator learning based on of set of functions optimized for the target function space and utilizes this pre-trained basis for an efficient learning process.
The paper is well written and some conclusions underscore the potential of AutoBasisEncoder to enhance the landscape of operator learning frameworks"
399,"Clarity: The authors clearly describe their goal of assessing two groups of reddit posters the posters in the Anxiety subreddit and that then post in the ADHD subreddit and the posters in the Anxiety that do not post in the ADHD subreddit. 
Originality: the task of determining a proxy of possible comorbid ADHD is novel. 
Significance: The link between the proxy of possible comorbid ADHD is not well discussed in the manuscript and therefore it is hard to say what impact this will have in the clinical world. However, it is an interesting approach to the use of foundation models within a ‘semi’-clinical setting. 
Quality: the study quality appears good as the method, data and performance of the model has been clearly stated. 

Major points
1.	You point towards it yourself in the data collection section of the paper where you note that the reddit posts are not a clinical diagnosis. I think you must discuss the implications on the significance of your work. 
2.	Why did you choose to remove data from posters that posted in the ADHD subreddit within 6 months after their post in the Anxiety subreddit?
3.	Clarify if you download posts from the ADHD subreddit. In the data preprocessing section, you state that you don’t use the posts and then that you use the posts from the ADHD subreddit. 
4.	You reference a figure 6 that is not present in the manuscript.
5.	You state that you visualize the phrases leading to “will post in ADHD” or “will not post in ADHD” for any given post but this is not presented anywhere. 

Minor points
1.	In section Data Preprocessing is it correctly understood that the posters who posted anywhere else than the Anxiety or/and ADHD subreddits were removed from the dataset? This should be clearer.
2.	You mention the base rate of the test set, but you provide no detail on the distribution of the training set or if you have done any weighted sampling or indeed how you sampled the test dataset.

Pros 
•	Well written and concise. 
•	Interesting subject sure to spark interest even for people who are not experts in psychiatric disorders.
•	Interesting application of existing models to proxy ADHD comorbidity 
•	In the appendix of the paper the limitations section does a good job of explaining the cons of the study in non-bias manner. 
Cons 
•	The ADHD comorbidity proxy is not well discussed. 
•	The authors state that they have visualizations multiple times where none are shown."
400,"Summary:

The paper describes the development of a segmentation network, designed in the light of the “CVPR 2024: Segment Anything In Medical Images On Laptop” Challenge. The submission lists three main contributions - enhancing grayscale images to optimize contrast and smoothness, leveraging the EfficientViT-SAM architecture and employing dynamic quantization. The authors show that this combination leads to improved performance and reduced inference speed compared to the baseline model on their heldout validation set.

Strengths:
- Preprocessing: The authors apply a novel preprocessing scheme to utilize the channel information for grayscale images.
- Performance: The proposed method improves significantly upon the baseline.
- Efficiency: The combination of the EfficientViT-SAM encoder and the use of dynamic quantization lead to a reduction of inference time of up to 80%.

Weaknesses:
- Missing details: The paper lacks many important details, which reduce reproducibility of the submission.
  - It is not completely clear how the model was trained, especially regarding knowledge distillation. Which checkpoint was used for knowledge distillation?
  - The grayscale enhancement is described, but it is not clear how a combination of this choice of channel input and the channel inputs for RGB images should be done. It seems like this actually leads to a slight decrease in performance of modalities with RGB input.
- Validation Set: The authors report results on a custom 1% validation split instead of the full training set instead of the official validation split provided by the organizers. This not only makes a comparison to other submissions basically impossible, but also the comparison to the baseline model LiteMedSAM, which was trained on a large portion of this data. The custom validation set seems to show significant differences to the official validation set, as can be seen by e.g. the performances on CT and MR.
- Results: The presented results for NSD are questionable. A reported NSD of ~0.98 with a DSC of 0.88 seems to be too high. Especially compared to results on the official validation set, these values appear to be way too high and raise questions regarding a correct implementation of the evaluation metrics.
- Dynamic Quantization: It seems like dynamic quantization actually leads to an improvement of performance compared to the unquantized network. As this is very unexpected, the authors should mention this and if possible try to give an explanation.
- Ablation study: It is not clear how the different contributions contributed to the final performance. What is the contribution of the grayscale enhancement on top of training on more data?"
401,"The paper explores an interesting application of neuro-symbolic methods from Cog-Sci as loss functions to enhance the subitizing capability of CNNs. The authors test generalization of their approach across transformations showing minor improvements in limited cases over cross entropy loss. The authors do not provide analysis quantifying the computational efficiency benefits of subitization versus traditional object detection and counting. An analysis of inference speed, latency, or computational cost is needed to demonstrate that the proposed HRR approach retains this advantage over counting detected objects. Without such analysis, the practical speedup of subitizing for real-time applications is unclear."
402,"The quality of the paper was reasonable. The key idea (significance) was to use a RepViT backbone instead of a TinyViT. 

The authors mentioned most details that are necessary for implementation. Some of the missing details include mentioning which particular variant of RepViT was used and defining what ""whole pipeline training means"".

The writing of the document was clear and formal. The only suggestion I would have is to replace the bullet points on page 3 with an numerically ordered list. 

The paper's training process and architecture was not novel, as it is similar to MobileSAM and traditional knowledge distillation. Some novel ideas include storing the teacher embeddings, training the student's prediction on augmented images to match teacher's prediction on the unaugmented images (during distillation and whole-pipeline training),  and cropping the training images to non-zero regions with annotations.

The ablation study only compared computational speed of structural reparamerization technique, which I beleive was important. Further ablations should include the changes in performance (e.g. comparing the improvement in performance from the novel ideas).   

The analysis of the sample results was very limited. 

In terms of training improvements, I would look into whether distilling the student and teacher on different views is actually beneficial, as there are works that indicate the contrary: ""Knowledge distillation: A good teacher is patient and consistent"" (https://arxiv.org/abs/2106.05237).


Measuring the carbon footprint of the training (CO2)"
403,"Summary: This paper propose a new method for weakly spupervised incremental few-shot detection. They use a metra-learning approach and introduce a weakly supervised class augmentation. They outperforms traditional methods by a clear margin on MS COCO and PASCAL VOC.

Strength: 
1. They have achived very good performance improvements in terms of both the number of new classess, and mAP.
2. It's interesting to study the problem that using images of a new category as the support set for meta-learning. In other words, I like the idea that applying meta-learning framework to incremental few-shot learning.
3. I like the idea that using a pre-trained classification model with Grad-CAM for data augmentation.


Weakenss:
1. In this paper, authors mainly compare their methods with ONCE,Feature-Reweight and MAML, what were proposed before 2020. I adverse authors to compare their method with some more advanced method.
2. Authors propose a new data augmentation method, and the Mis-Classification Filtering for remove the bad augmentations. However, no experimental results are probided to validate their effectiveness. Ablation on this is necessary.
3. In this paper, authors use models pretrained on ImageNet to introduce the knowledge from the new categories. Another popular method is to introduce new knowledge by using large-scale pretrained multi-modal models such as CLIP. To the best of my knowledge, the second  road has also achieved good performance. Does your method work better than such kind of method? Please discuss on it."
404,"Summary:

This paper discusses the performance improvements of visual language models (VLMs) in tasks involving vision and reasoning, such as image retrieval and visual question answering (VQA). This paper proposes an efficient, question-driven image description (captioning) pipeline to enhance visual question answering in a mathematical context. The approach extracts keywords from the question, generates targeted captions for each image-question pair, and uses these captions as hints for question answering.

Strength:
- A pipeline is proposed to enhance VQA capabilities by extracting keywords from questions and generating targeted image descriptions.
- Task-specific guidance is used as a “method” to enhance the VQA and description processes.
- The robustness of the model to hostile cues was evaluated to ensure that the description-based method does not compromise too much on robustness.

Weakness:
- The Figure 1 on Page 2 is lack of the detail of model. It’s hard to find the connection between the three steps. What are their meanings? How they improved from one to another?
- In the main context, authors never mention about Figure 1-4 and Table 1-2. So What are those information related?
- In Experiment on Page 3, author mention they divide MathVision dataset into three sub-datasets. How did they work? What standard author used to split the datasets as those three parts?
- In Results on Page 3-4, What datasets are used for testing ‘Model-wise’? And Which VLM is used for testing ‘Dataset-wise’?
- Authors said their approaches are using a prompt-based pipeline to solve relative problems, which reducing the computational overhead. However, there is no any comparison we can find in this study to show the difference between authors’ approaches and other approaches.

Suggestions:
- In Figure 1 on Page 2, authors should consider to improve the text format for a batter reading.
- In Experiments on Page 3, authors mention the models they used. We suggest author add more details of each model to highlight the difference of model.
- In Result section on Page 4, ‘(Table 4)’ is actually ‘(Table 2)’."
405,"This ambitious paper fine-tuning a large SOTA LLM for structuring clinical notes capable of handling long context windows and rigorously evaluates it to other commercial/open models using a chatbot arena + LLM-as-judge setup.

The background is well set with both a survey of clinical LLM tasks from arXiv and huggingface and an empirical investigation of context windows required for clinical notes in MIMIC-IV.

They chose a TigerBot base model as it is multilingual and demonstrated superior performance to Llama 2-70b chat with accuracy of next token prediction on 8k contexts. Whilst next token prediction isn’t a particularly useful task they justify it as suitable for rapid decision making and early exploration. The poor performance of Llama 2 70b on 8k vs 4k tokens is hypothesised due to under representativeness of clinical vocabulary leading to worsened hallucination - this seems possible, but I can’t believe that the justification that TigerBot was trained on arXiv which has 1.2% biomedical content explains the difference, I’m sure this was part of Llama training too!

The training data is constructed using clinical notes from MIMIC-IV processed using GPT-4. It’s good to see (trained!) expert evaluation of the training data. The ethical provisioning for this needs to be mentioned as PhysioNet does not permit use of OpenAI APIs! This is not mentioned despite the extensive ethics appendix. 

Given the importance of training data it would be useful to have some additional details of how GPT-4 was used to restructure data for the three extraction tasks. There are some additional training datasets such as a ‘previously unseen corpus’ of general-purpose SFT data which needs clarifying. The use of ASclepius for basic clinical tasks like NER and abbreviation seems sensible but despite training for abbreviations they use a medical dictionary at both training and inference?

They structure training from general purpose to basic tasks (ner/abbreviations) to hard tasks (summarisation) but it would be nice to see some experimental results or referenced justification for why they did this. Otherwise this really follows the general LLM -> domain specific fine-tuning paradigm so I’m not sure constitutes a novel strategy in and of itself?

The technical details of training framework are clear and impressive.

Evaluation is rigorous with comparison with range of commercial and open-source (GPT3.5/4, Gemini Pro, llama2, mixtral). The use of ChatBot Arena for blind pairwise evaluation includes control groups with intentionally wrong information and swap-position executions. They use GPT-4 as a judge, supplying the evaluation prompt which is thorough, however despite using 5 domain experts to review training data I can’t see any expert review of the final evaluation data or the LLM as judge strategy? This appears to me to be the biggest weakness in an otherwise strong paper and is perhaps time related?

Overall this is a really ambitious and thorough piece of work which makes both an interesting contribution to the research literature as well as the open source community through release of trained models, datasets and evaluation code."
406,"The premise of the paper is very interesting-- leveraging discretization-free INR representation without losing the benefits of a (somewhat) local representation. 
Method-wise there's a lot going on in this paper-- a latent-space transformer, a diffusion model for next-step prediction and an INR decoder. This is good for a workshop, there's a lot of food for thought and things to discuss. 

There isn't much validation of the papers' ideas so far, experiments are basically one comparison study. So currently we don't learn too much from the method choices-- I would definitely encourage the authors to keep exploring this space, drilling down on the individual components, and running ablations. E.g. what does setting this up as a diffusion model buy? Or maybe you get really good results for a diffusion transformer with a plain grid-based VAE decoder.
Focussing on one key element that works well, and really studying it could make an excellent full conference paper."
407,"**Summary**: The manuscript discusses a novel hybrid method to solve PDEs, which lies at the intersection of PINNs and an operator-based approach. The authors combine iterative methods for PDEs to learn the optimization itself, coupled with supervised learning strategies. Optimistic results are showcased on conventional PDEs compared with existing approaches.


**Strengths**
1. The paper is easy to read and follow. The authors describe their approach succinctly, and the hybrid algorithm is a novel contribution.
2. The problem seems well-motivated concerning the pitfalls of using PINNs or operator-based approaches. 
3. The authors showcase initial promising results and good details about the experiments and setup.

**Areas of improvement**:

1. I have some concerns regarding the performance evaluations. No. of iterations do not provide a clear idea of how great a method is performing computationally. In hindsight, the devised approach would have more inference time than a single forward pass used in PINOs and PINNs. Hence, reporting the computational time of training and testing periods will make the argument more convincing. 
3. The problems chosen for benchmarks are quite simple: very fast FEM-based solvers exist, which have been shown to beat these approaches by orders of magnitude [grossman2023, markdis2021]. However, PINNs and PINOs are useful in high dimensional PDEs and non-local problems where we do not have good classical numerical solvers. Evaluating this set of problems will make the argument for these methods more convincing.

[grossmann2023] Grossmann, Tamara G., et al. ""Can physics-informed neural networks beat the finite element method?."" arXiv preprint arXiv:2302.04107 (2023). 

[markidis2021] Markidis, Stefano. ""The old and the new: Can physics-informed deep-learning replace traditional linear solvers?."" Frontiers in Big Data 4 (2021): 669097."
408,This paper first demonstrated that prompt engineering can outperform fine-tuning in medical QA for open-source models. I really like this work and definitely think it's important as open-source models are more accessible for clinicians and have less privacy issues. Overall I think this is a good paper presenting important conclusions. My only concern is that this paper heavily relies on public benchmark and lacks the evaluation from the clinician.
