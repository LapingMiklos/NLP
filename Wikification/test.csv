ID,abs,kws
0,"The Internet of Things (IoT) now permeates our daily lives, providing important measurement and collection tools to inform our every decision. Millions of sensors and devices are continuously producing data and exchanging important messages via complex networks supporting machine-to-machine communications and monitoring and controlling critical smart-world infrastructures. As a strategy to mitigate the escalation in resource congestion, edge computing has emerged as a new paradigm to solve IoT and localized computing needs. Compared with the well-known cloud computing, edge computing will migrate data computation or storage to the network “edge”, near the end users. Thus, a number of computation nodes distributed across the network can offload the computational stress away from the centralized data center, and can significantly reduce the latency in message exchange. In addition, the distributed structure can balance network traffic and avoid the traffic peaks in IoT networks, reducing the transmission latency between edge/cloudlet servers and end users, as well as reducing response times for real-time IoT applications in comparison with traditional cloud services. Furthermore, by transferring computation and communication overhead from nodes with limited battery supply to nodes with significant power resources, the system can extend the lifetime of the individual nodes. In this paper, we conduct a comprehensive survey, analyzing how edge computing improves the performance of IoT networks. We categorize edge computing into different groups based on architecture, and study their performance by comparing network latency, bandwidth occupation, energy consumption, and overhead. In addition, we consider security issues in edge computing, evaluating the availability, integrity, and the confidentiality of security strategies of each group, and propose a framework for security evaluation of IoT networks with edge computing. Finally, we compare the performance of various IoT applications (smart city, smart grid, smart transportation, and so on) in edge computing and traditional cloud computing architectures.","['cloud computing', 'servers', 'security', 'internet of things']"
1,"The Internet of Things (IoT) is the next era of communication. Using the IoT, physical objects can be empowered to create, receive, and exchange data in a seamless manner. Various IoT applications focus on automating different tasks and are trying to empower the inanimate physical objects to act without any human intervention. The existing and upcoming IoT applications are highly promising to increase the level of comfort, efficiency, and automation for the users. To be able to implement such a world in an ever-growing fashion requires high security, privacy, authentication, and recovery from attacks. In this regard, it is imperative to make the required changes in the architecture of the IoT applications for achieving end-to-end secure IoT environments. In this paper, a detailed review of the security-related challenges and sources of threat in the IoT applications is presented. After discussing the security issues, various emerging and existing technologies focused on achieving a high degree of trust in the IoT applications are discussed. Four different technologies, blockchain, fog computing, edge computing, and machine learning, to increase the level of security in IoT are discussed.","['internet of things', 'security', 'privacy', 'blockchain', 'machine learning']"
2,"Recent years have witnessed a paradigm shift in the storage of Electronic Health Records (EHRs) on mobile cloud environments, where mobile devices are integrated with cloud computing to facilitate medical data exchanges among patients and healthcare providers. This advanced model enables healthcare services with low operational cost, high flexibility, and EHRs availability. However, this new paradigm also raises concerns about data privacy and network security for e-health systems. How to reliably share EHRs among mobile users while guaranteeing high-security levels in the mobile cloud is a challenging issue. In this paper, we propose a novel EHRs sharing framework that combines blockchain and the decentralized interplanetary file system (IPFS) on a mobile cloud platform. Particularly, we design a trustworthy access control mechanism using smart contracts to achieve secure EHRs sharing among different patients and medical providers. We present a prototype implementation using Ethereum blockchain in a real data sharing scenario on a mobile app with Amazon cloud computing. The empirical results show that our proposal provides an effective solution for reliable data exchanges on mobile clouds while preserving sensitive health information against potential threats. The system evaluation and security analysis also demonstrate the performance improvements in lightweight access control design, minimum network latency with high security and data privacy levels, compared to the existing data sharing models.","['blockchain', 'cloud computing', 'privacy', 'security']"
3,"In this survey paper, we systematically summarize existing literature on bearing fault diagnostics with deep learning (DL) algorithms. While conventional machine learning (ML) methods, including artificial neural network, principal component analysis, support vector machines, etc., have been successfully applied to the detection and categorization of bearing faults for decades, recent developments in DL algorithms in the last five years have sparked renewed interest in both industry and academia for intelligent machine health monitoring. In this paper, we first provide a brief review of conventional ML methods, before taking a deep dive into the state-of-the-art DL algorithms for bearing fault applications. Specifically, the superiority of DL based methods are analyzed in terms of fault feature extraction and classification performances; many new functionalities enabled by DL techniques are also summarized. In addition, to obtain a more intuitive insight, a comparative study is conducted on the classification accuracy of different algorithms utilizing the open source Case Western Reserve University (CWRU) bearing dataset. Finally, to facilitate the transition on applying various DL algorithms to bearing fault diagnostics, detailed recommendations and suggestions are provided for specific application conditions. Future research directions to further enhance the performance of DL algorithms on health monitoring are also discussed.","['monitoring', 'deep learning', 'feature extraction', 'machine learning']"
4,"Traditional power grids are being transformed into smart grids (SGs) to address the issues in the existing power system due to uni-directional information flow, energy wastage, growing energy demand, reliability, and security. SGs offer bi-directional energy flow between service providers and consumers, involving power generation, transmission, distribution, and utilization systems. SGs employ various devices for the monitoring, analysis, and control of the grid, deployed at power plants, distribution centers, and in consumers’ premises in a very large number. Hence, an SG requires connectivity, automation, and the tracking of such devices. This is achieved with the help of the Internet of Things (IoT). The IoT helps SG systems to support various network functions throughout the generation, transmission, distribution, and consumption of energy by incorporating the IoT devices (such as sensors, actuators, and smart meters), as well as by providing the connectivity, automation, and tracking for such devices. In this paper, we provide a comprehensive survey on the IoT-aided SG systems, which includes the existing architectures, applications, and prototypes of the IoT-aided SG systems. This survey also highlights the open issues, challenges, and future research directions for the IoT-aided SG systems.","['internet of things', 'security', 'power generation', 'monitoring']"
5,"Industrial systems always prefer to reduce their operational expenses. To support such reductions, they need solutions that are capable of providing stability, fault tolerance, and flexibility. One such solution for industrial systems is cyber physical system (CPS) integration with the Internet of Things (IoT) utilizing cloud computing services. These CPSs can be considered as smart industrial systems, with their most prevalent applications in smart transportation, smart grids, smart medical and eHealthcare systems, and many more. These industrial CPSs mostly utilize supervisory control and data acquisition (SCADA) systems to control and monitor their critical infrastructure (CI). For example, WebSCADA is an application used for smart medical technologies, making improved patient monitoring and more timely decisions possible. The focus of the study presented in this paper is to highlight the security challenges that the industrial SCADA systems face in an IoT-cloud environment. Classical SCADA systems are already lacking in proper security measures; however, with the integration of complex new architectures for the future Internet based on the concepts of IoT, cloud computing, mobile wireless sensor networks, and so on, there are large issues at stakes in the security and deployment of these classical systems. Therefore, the integration of these future Internet concepts needs more research effort. This paper, along with highlighting the security challenges of these CI's, also provides the existing best practices and recommendations for improving and maintaining security. Finally, this paper briefly describes future research directions to secure these critical CPSs and help the research community in identifying the research gaps in this regard.","['cloud computing', 'security', 'wireless sensor networks', 'internet of things']"
6,"Due to the monumental growth of Internet applications in the last decade, the need for security of information network has increased manifolds. As a primary defense of network infrastructure, an intrusion detection system is expected to adapt to dynamically changing threat landscape. Many supervised and unsupervised techniques have been devised by researchers from the discipline of machine learning and data mining to achieve reliable detection of anomalies. Deep learning is an area of machine learning which applies neuron-like structure for learning tasks. Deep learning has profoundly changed the way we approach learning tasks by delivering monumental progress in different disciplines like speech processing, computer vision, and natural language processing to name a few. It is only relevant that this new technology must be investigated for information security applications. The aim of this paper is to investigate the suitability of deep learning approaches for anomaly-based intrusion detection system. For this research, we developed anomaly detection models based on different deep neural network structures, including convolutional neural networks, autoencoders, and recurrent neural networks. These deep models were trained on NSLKDD training data set and evaluated on both test data sets provided by NSLKDD, namely NSLKDDTest+ and NSLKDDTest21. All experiments in this paper are performed by authors on a GPU-based test bed. Conventional machine learning-based intrusion detection models were implemented using well-known classification techniques, including extreme learning machine, nearest neighbor, decision-tree, random-forest, support vector machine, naive-bays, and quadratic discriminant analysis. Both deep and conventional machine learning models were evaluated using well-known classification metrics, including receiver operating characteristics, area under curve, precision-recall curve, mean average precision and accuracy of classification. Experimental results of deep IDS models showed promising results for real-world application in anomaly detection systems.","['machine learning', 'training', 'neural networks', 'deep learning', 'convolutional neural networks']"
7,"With the development of satellite technology, up to date imaging mode of synthetic aperture radar (SAR) satellite can provide higher resolution SAR imageries, which benefits ship detection and instance segmentation. Meanwhile, object detectors based on convolutional neural network (CNN) show high performance on SAR ship detection even without land-ocean segmentation; but with respective shortcomings, such as the relatively small size of SAR images for ship detection, limited SAR training samples, and inappropriate annotations, in existing SAR ship datasets, related research is hampered. To promote the development of CNN based ship detection and instance segmentation, we have constructed a High-Resolution SAR Images Dataset (HRSID). In addition to object detection, instance segmentation can also be implemented on HRSID. As for dataset construction, under the overlapped ratio of 25%, 136 panoramic SAR imageries with ranging resolution from 1m to 5m are cropped to 800 × 800 pixels SAR images. To reduce wrong annotation and missing annotation, optical remote sensing imageries are applied to reduce the interferes from harbor constructions. There are 5604 cropped SAR images and 16951 ships in HRSID, and we have divided HRSID into a training set (65% SAR images) and test set (35% SAR images) with the format of Microsoft Common Objects in Context (MS COCO). 8 state-of-the-art detectors are experimented on HRSID to build the baseline; MS COCO evaluation metrics are applicated for comprehensive evaluation. Experimental results reveal that ship detection and instance segmentation can be well implemented on HRSID.","['imaging', 'synthetic aperture radar', 'detectors', 'convolutional neural network']"
8,"Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future.","['security', 'training', 'testing', 'machine learning']"
9,"Nature computing has evolved with exciting performance to solve complex real-world combinatorial optimization problems. These problems span across engineering, medical sciences, and sciences generally. The Ebola virus has a propagation strategy that allows individuals in a population to move among susceptible, infected, quarantined, hospitalized, recovered, and dead sub-population groups. Motivated by the effectiveness of this strategy of propagation of the disease, a new bio-inspired and population-based optimization algorithm is proposed. This study presents a novel metaheuristic algorithm named Ebola Optimization Search Algorithm (EOSA) based on the propagation mechanism of the Ebola virus disease. First, we designed an improved SIR model of the disease, namely SEIR-HVQD: Susceptible (S), Exposed (E), Infected (I), Recovered (R), Hospitalized (H), Vaccinated (V), Quarantine (Q), and Death or Dead (D). Secondly, we represented the new model using a mathematical model based on a system of first-order differential equations. A combination of the propagation and mathematical models was adapted for developing the new metaheuristic algorithm. To evaluate the performance and capability of the proposed method in comparison with other optimization methods, two sets of benchmark functions consisting of forty-seven (47) classical and thirty (30) constrained IEEE-CEC benchmark functions were investigated. The results indicate that the performance of the proposed algorithm is competitive with other state-of-the-art optimization methods based on scalability, convergence, and sensitivity analyses. Extensive simulation results show that the EOSA outperforms popular metaheuristic algorithms such as the Particle Swarm Optimization Algorithm (PSO), Genetic Algorithm (GA), and Artificial Bee Colony Algorithm (ABC). Also, the algorithm was applied to address the complex problem of selecting the best combination of convolutional neural network (CNN) hyperparameters in the image classification of digital mammography. Results obtained showed the optimized CNN architecture successfully detected breast cancer from digital images at an accuracy of 96.0%. The source code of EOSA is publicly available at https://github.com/NathanielOy/EOSA_Metaheuristic .","['optimization', 'mathematical models', 'heuristic algorithms', 'convolutional neural network']"
10,"This paper presents end-to-end learning from spectrum data-an umbrella term for new sophisticated wireless signal identification approaches in spectrum monitoring applications based on deep neural networks. End-to-end learning allows to: 1) automatically learn features directly from simple wireless signal representations, without requiring design of hand-crafted expert features like higher order cyclic moments and 2) train wireless signal classifiers in one end-to-end step which eliminates the need for complex multi-stage machine learning processing pipelines. The purpose of this paper is to present the conceptual framework of end-to-end learning for spectrum monitoring and systematically introduce a generic methodology to easily design and implement wireless signal classifiers. Furthermore, we investigate the importance of the choice of wireless data representation to various spectrum monitoring tasks. In particular, two case studies are elaborated: 1) modulation recognition and 2) wireless technology interference detection. For each case study three convolutional neural networks are evaluated for the following wireless signal representations: temporal IQ data, the amplitude/phase representation, and the frequency domain representation. From our analysis, we prove that the wireless data representation impacts the accuracy depending on the specifics and similarities of the wireless signals that need to be differentiated, with different data representations resulting in accuracy variations of up to 29%. Experimental results show that using the amplitude/phase representation for recognizing modulation formats can lead to performance improvements up to 2% and 12% for medium to high SNR compared to IQ and frequency domain data, respectively. For the task of detecting interference, frequency domain representation outperformed amplitude/phase and IQ data representation up to 20%.","['monitoring', 'machine learning', 'interference', 'modulation', 'convolutional neural networks']"
11,"Internet of Things (IoT) connects sensing devices to the Internet for the purpose of exchanging information. Location information is one of the most crucial pieces of information required to achieve intelligent and context-aware IoT systems. Recently, positioning and localization functions have been realized in a large amount of IoT systems. However, security and privacy threats related to positioning in IoT have not been sufficiently addressed so far. In this paper, we survey solutions for improving the robustness, security, and privacy of location-based services in IoT systems. First, we provide an in-depth evaluation of the threats and solutions related to both global navigation satellite system (GNSS) and non-GNSS-based solutions. Second, we describe certain cryptographic solutions for security and privacy of positioning and location-based services in IoT. Finally, we discuss the state-of-the-art of policy regulations regarding security of positioning solutions and legal instruments to location data privacy in detail. This survey paper addresses a broad range of security and privacy aspects in IoT-based positioning and localization from both technical and legal points of view and aims to give insight and recommendations for future IoT systems providing more robust, secure, and privacy-preserving location-based services.","['security', 'robustness', 'privacy', 'internet of things']"
12,"Smart cities contain intelligent things which can intelligently automatically and collaboratively enhance life quality, save people's lives, and act a sustainable resource ecosystem. To achieve these advanced collaborative technologies such as drones, robotics, artificial intelligence, and Internet of Things (IoT) are required to increase the smartness of smart cities by improving the connectivity, energy efficiency, and quality of services (QoS). Therefore, collaborative drones and IoT play a vital role in supporting a lot of smart-city applications such as those involved in communication, transportation, agriculture,safety and security, disaster mitigation, environmental protection, service delivery, energy saving, e-waste reduction, weather monitoring, healthcare, etc. This paper presents a survey of the potential techniques and applications of collaborative drones and IoT which have recently been proposed in order to increase the smartness of smart cities. It provides a comprehensive overview highlighting the recent and ongoing research on collaborative drone and IoT in improving the real-time application of smart cities. This survey is different from previous ones in term of breadth, scope, and focus. In particular, we focus on the new concept of collaborative drones and IoT for improving smart-city applications. This survey attempts to show how collaborative drones and IoT improve the smartness of smart cities based on data collection, privacy and security, public safety, disaster management, energy consumption and quality of life in smart cities. It mainly focuses on the measurement of the smartness of smart cities, i.e., environmental aspects, life quality, public safety, and disaster management.","['internet of things', 'monitoring', 'security', 'energy consumption']"
13,"The growing interest and recent breakthroughs in artificial intelligence and machine learning (ML) have actively contributed to an increase in research and development of new methods to estimate the states of electrified vehicle batteries. Data-driven approaches, such as ML, are becoming more popular for estimating the state of charge (SOC) and state of health (SOH) due to greater availability of battery data and improved computing power capabilities. This paper provides a survey of battery state estimation methods based on ML approaches such as feedforward neural networks (FNNs), recurrent neural networks (RNNs), support vector machines (SVM), radial basis functions (RBF), and Hamming networks. Comparisons between methods are shown in terms of data quality, inputs and outputs, test conditions, battery types, and stated accuracy to give readers a bigger picture view of the ML landscape for SOC and SOH estimation. Additionally, to provide insight into how to best approach with the comparison of different neural network structures, an FNN and long short-term memory (LSTM) RNN are trained fifty times each for 3000 epochs. The error is somewhat different for each training repetition due to the random initial values of the trainable parameters, demonstrating that it is important to train networks multiple times to achieve the best result. Furthermore, it is recommended that when performing a comparison among estimation techniques such as those presented in this review paper, the compared networks should have a similar number of learnable parameters and be trained and tested with identical data. Otherwise, it is difficult to make a general conclusion regarding the quality of a given estimation technique.","['batteries', 'machine learning', 'training', 'artificial intelligence']"
14,"Encoder-decoder networks are state-of-the-art approaches to biomedical image segmentation, but have two problems: i.e., the widely used pooling operations may discard spatial information, and therefore low-level semantics are lost. Feature fusion methods can mitigate these problems but feature maps of different scales cannot be easily fused because downand upsampling change the spatial resolution of feature map. To address these issues, we propose INet, which enlarges receptive fields by increasing the kernel sizes of convolutional layers in steps (e.g., from 3 × 3 to 7 × 7 and then 15 × 15) instead of downsampling. Inspired by an Inception module, INet extracts features by kernels of different sizes through concatenating the output feature maps of all preceding convolutional layers. We also find that the large kernel makes the network feasible for biomedical image segmentation. In addition, INet uses two overlapping max-poolings, i.e., max-poolings with stride 1, to extract the sharpest features. Fixed-size and fixed-channel feature maps enable INet to concatenate feature maps and add multiple shortcuts across layers. In this way, INet can recover low-level semantics by concatenating the feature maps of all preceding layers and expedite the training by adding multiple shortcuts. Because INet has additional residual shortcuts, we compare INet with a UNet system that also has residual shortcuts (ResUNet). To confirm INet as a backbone architecture for biomedical image segmentation, we implement dense connections on INet (called DenseINet) and compare it to a DenseUNet system with residual shortcuts (ResDenseUNet). INet and DenseINet require 16.9% and 37.6% fewer parameters than ResUNet and ResDenseUNet, respectively. In comparison with six encoder- decoder approaches using nine public datasets, INet and DenseINet demonstrate efficient improvements in biomedical image segmentation. INet outperforms DeepLabV3, which implementing atrous convolution instead of downsampling to increase receptive fields. INet also outperforms two recent methods (named HRNet and MS-NAS) that maintain high-resolution representations and repeatedly exchange the information across resolutions.","['image segmentation', 'semantics', 'kernel', 'convolution']"
15,"Smart city advancements are driving massive transformations of healthcare, the largest global industry. The drivers include increasing demands for ubiquitous, preventive, and personalized healthcare, to be provided to the public at reduced risks and costs. Mobile cloud computing could potentially meet the future healthcare demands by enabling anytime, anywhere capture and analyses of patients' data. However, network latency, bandwidth, and reliability are among the many challenges hindering the realization of next-generation healthcare. This paper proposes a ubiquitous healthcare framework, UbeHealth, that leverages edge computing, deep learning, big data, high-performance computing (HPC), and the Internet of Things (IoT) to address the aforementioned challenges. The framework enables an enhanced network quality of service using its three main components and four layers. Deep learning, big data, and HPC are used to predict network traffic, which in turn are used by the Cloudlet and network layers to optimize data rates, data caching, and routing decisions. Application protocols of the traffic flows are classified, enabling the network layer to meet applications' communication requirements better and to detect malicious traffic and anomalous data. Clustering is used to identify the different kinds of data originating from the same application protocols. A proof of concept UbeHealth system has been developed based on the framework. A detailed literature review is used to capture the design requirements for the proposed system. The system is described in detail including the algorithmic implementation of the three components and four layers. Three widely used data sets are used to evaluate the UbeHealth system.","['cloud computing', 'quality of service', 'internet of things', 'deep learning']"
16,"The health condition of a wheelset bearing, the key component of a railway bogie, has a considerable impact on the safety of a train. Traditional bearing fault diagnosis techniques generally extract signals manually and then diagnose the bearing health conditions through the classifier. However, high-speed trains (HSTs) are usually faced with variable loads, variable speeds, and strong environmental noise, which pose a huge challenge to the application of the traditional bearing fault diagnosis methods in wheelset bearing fault diagnosis. Therefore, this paper proposes a 1D residual block, and based on the block, a novel deeper 1D convolutional neural network (Der-1DCNN) is proposed. The framework includes the idea of residual learning and can effectively learn high-level and abstract features while effectively alleviating the problem of training difficulty and the performance degradation of a deeper network. Additionally, for the first time, we fully use the wide convolution kernel and dropout technology to improve the model's ability to learn low-frequency signal features related to the fault components and to enhance the network's generalization performance. By constructing a deep residual learning network, Der-1DCNN can adaptively learn the deep fault features of the original vibration signal. This method not only achieves very high diagnostic accuracy for the fault diagnosis task of wheelset bearings in HSTs under strong noise environment, but also its performance is quite superior when the train's working load changes without any domain adaptation algorithm processing. The proposed Der-1DCNN is evaluated on the dataset of the multi-operating conditions of the wheelset bearings of HSTs. Experiments show that this method shows a better diagnostic performance compared with the state-of-the-art deep learning methods of bearing fault diagnosis, which proves the method's effectiveness and superiority.","['convolution', 'training', 'kernel', 'deep learning']"
17,"Applications of Blockchain (BC) technology and Cyber-Physical Systems (CPS) are increasing exponentially. However, framing resilient and correct smart contracts (SCs) for these smart application is a quite challenging task because of the complexity associated with them. SC is modernizing the traditional industrial, technical, and business processes. It is self-executable, self-verifiable, and embedded into the BC that eliminates the need for trusted third-party systems, which ultimately saves administration as well as service costs. It also improves system efficiency and reduces the associated security risks. However, SCs are well encouraging the new technological reforms in Industry 4.0, but still, various security and privacy challenges need to be addressed. In this paper, a survey on SC security vulnerabilities in the software code that can be easily hacked by a malicious user or may compromise the entire BC network is presented. As per the literature, the challenges related to SC security and privacy are not explored much by the authors around the world. From the existing proposals, it has been observed that designing a complex SCs cannot mitigate its privacy and security issues. So, this paper investigates various Artificial Intelligence (AI) techniques and tools for SC privacy protection. Then, open issues and challenges for AI-based SC are analyzed. Finally, a case study of retail marketing is presented, which uses AI and SC to preserve its security and privacy.","['artificial intelligence', 'privacy', 'security', 'blockchain']"
18,"The growing development of IoT (Internet of Things) devices creates a large attack surface for cybercriminals to conduct potentially more destructive cyberattacks; as a result, the security industry has seen an exponential increase in cyber-attacks. Many of these attacks have effectively accomplished their malicious goals because intruders conduct cyber-attacks using novel and innovative techniques. An anomaly-based IDS (Intrusion Detection System) uses machine learning techniques to detect and classify attacks in IoT networks. In the presence of unpredictable network technologies and various intrusion methods, traditional machine learning techniques appear inefficient. In many research areas, deep learning methods have shown their ability to identify anomalies accurately. Convolutional neural networks are an excellent alternative for anomaly detection and classification due to their ability to automatically categorize main characteristics in input data and their effectiveness in performing faster computations. In this paper, we design and develop a novel anomaly-based intrusion detection model for IoT networks. First, a convolutional neural network model is used to create a multiclass classification model. The proposed model is then implemented using convolutional neural networks in 1D, 2D, and 3D. The proposed convolutional neural network model is validated using the BoT-IoT, IoT Network Intrusion, MQTT-IoT-IDS2020, and IoT-23 intrusion detection datasets. Transfer learning is used to implement binary and multiclass classification using a convolutional neural network multiclass pre-trained model. Our proposed binary and multiclass classification models have achieved high accuracy, precision, recall, and F1 score compared to existing deep learning implementations.","['internet of things', 'deep learning', 'security', 'convolutional neural networks', 'neural networks', 'machine learning', 'convolutional neural network']"
19,"Currently, Internet of Things (IoT) and blockchain technologies are experiencing exponential growth in academia and industry. Generally, IoT is a centralized system whose security and performance mainly rely on centralized servers. Therefore, users have to trust the centralized servers; in addition, it is difficult to coordinate external computing resources to improve the performance of IoT. Fortunately, the blockchain may provide this decentralization, high credibility and high security. Consequently, blockchain-based IoT may become a reasonable choice for the design of a decentralized IoT system. In this paper, we propose a novel blockchain-based threshold IoT service system: BeeKeeper. In the BeeKeeper system, servers can process a user's data by performing homomorphic computations on the data without learning anything from them. Furthermore, any node can become a leader's server if the node and the leader desire so. In this way, BeeKeeper's performance can continually increase by attracting external computing resources to join in it. Moreover, malicious nodes can be scrutinized. In addition, BeeKeeper is fault tolerant since a user's BeeKeeper protocol may work smoothly as long as a threshold number of its servers are active and honest. Finally, we deploy BeeKeeper on the Ethereum blockchain and give the corresponding performance evaluation. In our experiments, servers can generate their response with about 107 ms. Moreover, the performance of BeeKeeper mainly depends on the blockchain platform. For instance, the response time is about 22.5 s since the block interval of Ethereum blockchain is about 15 s. In fact, if we use some other blockchain with short block interval, the response time may be obviously short.","['servers', 'performance evaluation', 'internet of things', 'blockchain']"
20,"Recent advancements in the Internet of Health Things (IoHT) have ushered in the wide adoption of IoT devices in our daily health management. For IoHT data to be acceptable by stakeholders, applications that incorporate the IoHT must have a provision for data provenance, in addition to the accuracy, security, integrity, and quality of data. To protect the privacy and security of IoHT data, federated learning (FL) and differential privacy (DP) have been proposed, where private IoHT data can be trained at the owner’s premises. Recent advancements in hardware GPUs even allow the FL process within smartphone or edge devices having the IoHT attached to their edge nodes. Although some of the privacy concerns of IoHT data are addressed by FL, fully decentralized FL is still a challenge due to the lack of training capability at all federated nodes, the scarcity of high-quality training datasets, the provenance of training data, and the authentication required for each FL node. In this paper, we present a lightweight hybrid FL framework in which blockchain smart contracts manage the edge training plan, trust management, and authentication of participating federated nodes, the distribution of global or locally trained models, the reputation of edge nodes and their uploaded datasets or models. The framework also supports the full encryption of a dataset, the model training, and the inferencing process. Each federated edge node performs additive encryption, while the blockchain uses multiplicative encryption to aggregate the updated model parameters. To support the full privacy and anonymization of the IoHT data, the framework supports lightweight DP. This framework was tested with several deep learning applications designed for clinical trials with COVID-19 patients. We present here the detailed design, implementation, and test results, which demonstrate strong potential for wider adoption of IoHT-based health management in a secure way.","['training', 'security', 'blockchain', 'deep learning']"
21,"Owing to the explosive expansion of wireless communication and networking technologies, cost-effective unmanned aerial vehicles (UAVs) have recently emerged and soon they will occupy the major part of our sky. UAVs can be exploited to efficiently accomplish complex missions when cooperatively organized as an ad hoc network, thus creating the well-known flying ad hoc networks (FANETs). The establishment of such networks is not feasible without deploying an efficient networking model allowing a reliable exchange of information between UAVs. FANET inherits common features and characteristics from mobile ad hoc networks (MANETs) and their sub-classes, such as vehicular ad hoc networks (VANETs) and wireless sensor networks (WSNs). Unfortunately, UAVs are often deployed in the sky adopting a mobility model dictated by the nature of missions that they are expected to handle, and therefore, differentiate themselves from any traditional networks. Moreover, several flying constraints and the highly dynamic topology of FANETs make the design of routing protocols a complicated task. In this paper, a comprehensive survey is presented covering the architecture, the constraints, the mobility models, the routing techniques, and the simulation tools dedicated to FANETs. A classification, descriptions, and comparative studies of an important number of existing routing protocols dedicated to FANETs are detailed. Furthermore, the paper depicts future challenge perspectives, helping scientific researchers to discover some themes that have been addressed only ostensibly in the literature and need more investigation. The novelty of this survey is its uniqueness to provide a complete analysis of the major FANET routing protocols and to critically compare them according to different constraints based on crucial parameters, thus better presenting the state of the art of this specific area of research.","['routing', 'wireless sensor networks', 'wireless communication', 'simulation']"
22,"Automated pavement crack image segmentation is challenging because of inherent irregular patterns, lighting conditions, and noise in images. Conventional approaches require a substantial amount of feature engineering to differentiate crack regions from non-affected regions. In this paper, we propose a deep learning technique based on a convolutional neural network to perform segmentation tasks on pavement crack images. Our approach requires minimal feature engineering compared to other machine learning techniques. We propose a U-Net-based network architecture in which we replace the encoder with a pretrained ResNet-34 neural network. We use a “one-cycle” training schedule based on cyclical learning rates to speed up the convergence. Our method achieves an F1 score of 96% on the CFD dataset and 73% on the Crack500 dataset, outperforming other algorithms tested on these datasets. We perform ablation studies on various techniques that helped us get marginal performance boosts, i.e., the addition of spatial and channel squeeze and excitation (SCSE) modules, training with gradually increasing image sizes, and training various neural network layers with different learning rates.","['image segmentation', 'training', 'deep learning', 'convolutional neural network']"
23,"In this paper, we propose a new comprehensive realistic cyber security dataset of IoT and IIoT applications, called Edge-IIoTset, which can be used by machine learning-based intrusion detection systems in two different modes, namely, centralized and federated learning. Specifically, the dataset has been generated using a purpose-built IoT/IIoT testbed with a large representative set of devices, sensors, protocols and cloud/edge configurations. The IoT data are generated from various IoT devices (more than 10 types) such as Low-cost digital sensors for sensing temperature and humidity, Ultrasonic sensor, Water level detection sensor, pH Sensor Meter, Soil Moisture sensor, Heart Rate Sensor, Flame Sensor, etc.). Furthermore, we identify and analyze fourteen attacks related to IoT and IIoT connectivity protocols, which are categorized into five threats, including, DoS/DDoS attacks, Information gathering, Man in the middle attacks, Injection attacks, and Malware attacks. In addition, we extract features obtained from different sources, including alerts, system resources, logs, network traffic, and propose new 61 features with high correlations from 1176 found features. After processing and analyzing the proposed realistic cyber security dataset, we provide a primary exploratory data analysis and evaluate the performance of machine learning approaches (i.e., traditional machine learning as well as deep learning) in both centralized and federated learning modes. The Edge-IIoTset dataset can be publicly accessed from http://ieee-dataport.org/8939 .","['sensors', 'security', 'protocols', 'deep learning']"
24,"IoT is becoming more common and popular due to its wide range of applications in various domains. They collect data from the real environment and transfer it over the networks. There are many challenges while deploying IoT in a real-world, varying from tiny sensors to servers. Security is considered as the number one challenge in IoT deployments, as most of the IoT devices are physically accessible in the real world and many of them are limited in resources (such as energy, memory, processing power and even physical space). In this paper, we are focusing on these resource-constrained IoT devices (such as RFID tags, sensors, smart cards, etc.) as securing them in such circumstances is a challenging task. The communication from such devices can be secured by a mean of lightweight cryptography, a lighter version of cryptography. More than fifty lightweight cryptography (plain encryption) algorithms are available in the market with a focus on a specific application(s), and another 57 algorithms have been submitted by the researchers to the NIST competition recently. To provide a holistic view of the area, in this paper, we have compared the existing algorithms in terms of implementation cost, hardware and software performances and attack resistance properties. Also, we have discussed the demand and a direction for new research in the area of lightweight cryptography to optimize balance amongst cost, performance and security.","['security', 'encryption', 'software', 'sensors']"
25,"Existing work in energy demand side management focuses on the interaction between the utility grid and consumers. However, the previous technique is not focused on energy trading in local community of a renewable energy generation, distributed demand side management and not suitable for real-time environment. This paper presents a distributed demand side management system among multiple homes in community microgrid, with the integration of the internet of things smart meter and in the presence of renewable energy sources. The proposed energy consumption game is formulated for minimizing the cost of electricity in the individual home and the total cost of energy consumption in the whole community. The smart home users are playing game by optimizing their own daily energy consumption of appliances. The multiple participants include the self renewable generation of users, shared community microgrid and optional utility company. Each participant applies its best strategy to minimize energy consumption cost and users can maintain their own privacy of energy consumption. Moreover, the proposed scheme is distributed on blockchain, which provides a trusted communication medium between the participants. It enforces the autonomous monitoring of smart appliances and the billing of electricity consumption via smart contracts. Solidity smart contract is deployed to facilitate the execution of transactions without the involvement of third party in the smart community. Comparison of the results show that the proposed approach minimizes the total cost of energy consumption as well as each user's energy consumption cost.","['energy consumption', 'privacy', 'renewable energy sources', 'internet of things', 'blockchain']"
26,"Deep Learning (DL) algorithms based on artificial neural networks have achieved remarkable success and are being extensively applied in a variety of application domains, ranging from image classification, automatic driving, natural language processing to medical diagnosis, credit risk assessment, intrusion detection. However, the privacy and security issues of DL have been revealed that the DL model can be stolen or reverse engineered, sensitive training data can be inferred, even a recognizable face image of the victim can be recovered. Besides, the recent works have found that the DL model is vulnerable to adversarial examples perturbed by imperceptible noised, which can lead the DL model to predict wrongly with high confidence. In this paper, we first briefly introduces the four types of attacks and privacy-preserving techniques in DL. We then review and summarize the attack and defense methods associated with DL privacy and security in recent years. To demonstrate that security threats really exist in the real world, we also reviewed the adversarial attacks under the physical condition. Finally, we discuss current challenges and open problems regarding privacy and security issues in DL.","['security', 'privacy', 'training', 'deep learning']"
27,"The vision of the Internet of Things (IoT) to interconnect and Internet-connect everyday people, objects, and machines poses new challenges in the design of wireless communication networks. The design of medium access control (MAC) protocols has been traditionally an intense area of research due to their high impact on the overall performance of wireless communications. The majority of research activities in this field deal with different variations of protocols somehow based on ALOHA, either with or without listen before talk, i.e., carrier sensing multiple access. These protocols operate well under low traffic loads and low number of simultaneous devices. However, they suffer from congestion as the traffic load and the number of devices increase. For this reason, unless revisited, the MAC layer can become a bottleneck for the success of the IoT. In this paper, we provide an overview of the existing MAC solutions for the IoT, describing current limitations and envisioned challenges for the near future. Motivated by those, we identify a family of simple algorithms based on distributed queueing (DQ), which can operate for an infinite number of devices generating any traffic load and pattern. A description of the DQ mechanism is provided and most relevant existing studies of DQ applied in different scenarios are described in this paper. In addition, we provide a novel performance evaluation of DQ when applied for the IoT. Finally, a description of the very first demo of DQ for its use in the IoT is also included in this paper.","['internet of things', 'wireless communication', 'performance evaluation', 'protocols']"
28,"In the past decade, sparse and low-rank recovery has drawn much attention in many areas such as signal/image processing, statistics, bioinformatics, and machine learning. To achieve sparsity and/or low-rankness inducing, the ℓ 1 norm and nuclear norm are of the most popular regularization penalties due to their convexity. While the ℓ 1 and nuclear norm are convenient as the related convex optimization problems are usually tractable, it has been shown in many applications that a nonconvex penalty can yield significantly better performance. In recent, nonconvex regularization-based sparse and low-rank recovery is of considerable interest and it in fact is a main driver of the recent progress in nonconvex and nonsmooth optimization. This paper gives an overview of this topic in various fields in signal processing, statistics, and machine learning, including compressive sensing, sparse regression and variable selection, sparse signals separation, sparse principal component analysis (PCA), large covariance and inverse covariance matrices estimation, matrix completion, and robust PCA. We present recent developments of nonconvex regularization based sparse and low-rank recovery in these fields, addressing the issues of penalty selection, applications and the convergence of nonconvex algorithms. Code is available at https://github.com/FWen/ncreg.git.","['estimation', 'optimization', 'convergence', 'machine learning']"
29,"Energy consumption is one of the constraints in wireless sensor networks (WSNs). The routing protocols are the hot areas to address quality-of-service (QoS) related issues, viz., energy consumption, network lifetime, network scalability, and packet overhead. The key issue in WSN is that these networks suffer from the packet overhead, which is the root cause of more energy consumption and degrade the QoS in sensor networks. In WSN, there are several routing protocols, which are used to enhance the performance of the network. Out of those protocols, dynamic source routing (DSR) protocol is more suitable in terms of small energy density, but sometimes when the mode of a node changes from active to sleep, the efficiency decreases as the data packets need to wait at the initial point, where the packet has been sent and this increases the waiting time and end-to-end delay of the packets, which leads to increase in energy consumption. Our problem is to identify the dead nodes and to choose another suitable path so that the data transmission becomes smoother and less energy gets conserved. In order to resolve these issues, we propose directional transmission-based energy aware routing protocol named PDORP. The proposed protocol PDORP has the characteristics of both power efficient gathering sensor information system and DSR routing protocols. In addition, hybridization of genetic algorithm and bacterial foraging optimization is applied to proposed routing protocol to identify energy efficient optimal paths. The performance analysis, comparison through a hybridization approach of the proposed routing protocol, gives better result comprising less bit error rate, less delay, less energy consumption, and better throughput, which leads to better QoS and prolong the lifetime of the network. Moreover, the computation model is adopted to evaluate and compare the performance of the both routing protocols using soft computing techniques.","['wireless sensor networks', 'routing', 'optimization', 'energy consumption']"
30,"Landslide inventories are in high demand for risk assessment of this natural hazard, particularly in tropical mountainous regions. This research designed residual networks for landslide detection using spectral (RGB bands) and topographic information (altitude, slope, aspect, curvature). Recent studies indicate that deep learning methods such as convolutional neural networks (CNN) improve landslide mapping results compared to traditional machine learning. But the effects of network architecture designs and data fusion remain largely underexplored in landslide detection. We compared a one-layer CNN with two of its deeper counterparts and residual networks with two fusion strategies (layer stacking and feature-level fusion) to detect landslides in Cameron Highlands, Malaysia. Sixteen different maps were created using proposed methods and evaluated in separate training and testing sub-areas based on overall accuracy, F1-score, and mean intersection over union (mIOU) metrics. When layer stacking is used as a fusion approach, none of the network designs improved landslide detection results. However, our findings showed that when using feature-level fusion, results could be enhanced with the same network designs. Residual networks performed best improving F1-score and mIOU by 0.13 and 12.96%, respectively, using feature-level fusion rather than layer stacking. CNN models also enhanced the detection outcome with the same fusion approach. On single modality datasets, models' performance varies according to input data, highlighting the effects of input data on network architecture selection. In general, residual networks found to converge faster and generalize better to test areas than other models tested in this research.","['training', 'deep learning', 'testing', 'convolutional neural network']"
31,"Internet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges.","['machine learning', 'internet of things', 'software', 'security']"
32,"Internet of Things (IoT) is an integration of the Sensor, Embedded, Computing, and Communication technologies. The purpose of the IoT is to provide seamless services to anything, anytime at any place. IoT technologies play a crucial role everywhere, which brings the fourth revolution of disruptive technologies after the internet and Information and Communication Technology (ICT). The Research & Development community has predicted that the impact of IoT will be more than the internet and ICT on society, which improves the well-being of society and industries. Addressing the predominant system-level design aspects like energy efficiency, robustness, scalability, interoperability, and security issues result in the use of a potential IoT system. This paper presents the current state of art of the functional pillars of IoT and its emerging applications to motivate academicians and researches to develop real-time, energy-efficient, scalable, reliable, and secure IoT applications. This paper summarizes the architecture of IoT, with the contemporary status of IoT architectures. Highlights of the IoT system-level issues to develop more advanced real-time IoT applications have been discussed. Millions of devices exchange information using different communication standards, and interoperability between them is a significant issue. This paper provides the current status of the communication standards and application layer protocols used in IoT with the detailed analysis. The computing paradigms like Cloud, Cloudlet, Fog, and Edge computing facilitate IoT with various services like data offloading, resource and device management, etc. In this paper, an exhaustive analysis of Edge Computing in IoT with different edge computing architectures and existing status are deliberated. The widespread adoption of IoT in society has resulted in privacy and security issues. This paper emphasizes on analyzing the security challenges, privacy and security threats, conventional mitigation techniques, and further scope for IoT security. The features like fewer memory footprints, scheduling, real-time task execution, fewer interrupt, and thread switching latency of Real-Time Operating Systems (RTOS) enables the development of time critical IoT applications. Also, this review offers the analysis of the RTOS's suitable for IoT with the current status and networking stack. Finally, open research issues in IoT system development are discussed.","['internet of things', 'security', 'protocols', 'privacy']"
33,"Internet of Things (IoT) technology is prospering and entering every part of our lives, be it education, home, vehicles, or healthcare. With the increase in the number of connected devices, several challenges are also coming up with IoT technology: heterogeneity, scalability, quality of service, security requirements, and many more. Security management takes a back seat in IoT because of cost, size, and power. It poses a significant risk as lack of security makes users skeptical towards using IoT devices. This, in turn, makes IoT vulnerable to security attacks, ultimately causing enormous financial and reputational losses. It makes up for an urgent need to assess present security risks and discuss the upcoming challenges to be ready to face the same. The undertaken study is a multi-fold survey of different security issues present in IoT layers: perception layer, network layer, support layer, application layer, with further focus on Distributed Denial of Service (DDoS) attacks. DDoS attacks are significant threats for the cyber world because of their potential to bring down the victims. Different types of DDoS attacks, DDoS attacks in IoT devices, impacts of DDoS attacks, and solutions for mitigation are discussed in detail. The presented review work compares Intrusion Detection and Prevention models for mitigating DDoS attacks and focuses on Intrusion Detection models. Furthermore, the classification of Intrusion Detection Systems, different anomaly detection techniques, different Intrusion Detection System models based on datasets, various machine learning and deep learning techniques for data pre-processing and malware detection has been discussed. In the end, a broader perspective has been envisioned while discussing research challenges, its proposed solutions, and future visions.","['internet of things', 'security', 'deep learning', 'machine learning']"
34,"The infrastructure of wireless sensor networks (WSN) is structured in an ad-hoc manner and organized nodes reporting the events to the Base Station (BS). A WSN is integrated with smart technologies to develop fast Internet of Things (IoT) communications among different applications. Recently, many researchers proposed their solutions to optimize IoT data transmissions in an energy efficient manner with cost effective support. However, most of the solutions have focused on the design and development of static topologies and overlooked the dynamic structure of mobile sensor nodes. Furthermore, due to limited constraints of sensor nodes with open accessibility of wireless communications medium, data protection against malicious activities need to be redesign with the least network overheads. Therefore, the contribution of this article is to propose an intrusion prevention framework for mobile IoT devices with its integration to WSN so that to provide data security with improved network delivery ratio. The proposed framework is composed of two sub-components. Firstly, non-overlapping and autonomously organized clusters are generated and maintained the clusters' stability based on the uncertainty principle. Secondly, end-to-end secure and multi-hop routing paths are developed based on the blockchain architecture. The simulation results demonstrate a significant improvement when compared to existing solutions in terms of different network metrics.","['routing', 'wireless sensor networks', 'blockchain', 'security']"
35,"The rapid advancements in computing, storage, communications, and networking technologies have enabled the creation of Digital Twins (DTs). A DT is a digital representation of a real-world physical component, product, or equipment. A DT can be used for 3-D design, testing, simulation, and prototyping prior to the manufacturing of the physical component. Once a physical component is in operation, a DT can be used for configuration, monitoring, diagnostics, and prognostics. It is expected that DTs will gain significant attention in the foreseeable future, and will play a key role in Industry 4.0. However, today's approaches, systems, and technologies leveraged for the creation of DTs are mostly centralized and fall short of providing trusted data provenance, audit, and traceability. Also, data related to transactions, logs, and history are not secure or tamper-proof. In this paper, we propose a blockchain-based creation process of DTs to guarantee secure and trusted traceability, accessibility, and immutability of transactions, logs, and data provenance. Our proposed approach uses smart contracts to govern and track transactions initiated by participants involved in the creation of DTs. Our approach also employs decentralized storage of interplanetary file systems to store and share DTs data. Moreover, we present details on our system design and architecture, implementation, and algorithms. Furthermore, we provide security and cost analysis, and show how our approach fulfills the requirements of DTs process creation. We make the smart contract code for creating DTs publicly available on Github.","['testing', 'monitoring', 'blockchain', 'security']"
36,"Fog computing is an emerging technology to address computing and networking bottlenecks in large scale deployment of IoT applications. It is a promising complementary computing paradigm to cloud computing where computational, networking, storage and acceleration elements are deployed at the edge and network layers in a multi-tier, distributed and possibly cooperative manner. These elements may be virtualized computing functions placed at edge devices or network elements on demand, realizing the “computing everywhere” concept. To put the current research in perspective, this paper provides an inclusive taxonomy for architectural, algorithmic and technologic aspects of fog computing. The computing paradigms and their architectural distinctions, including cloud, edge, mobile edge and fog computing are subsequently reviewed. Practical deployment of fog computing includes a number of different aspects such as system design, application design, software implementation, security, computing resource management and networking. A comprehensive survey of all these aspects from the architectural point of view is covered. Current reference architectures and major application-specific architectures describing their salient features and distinctions in the context of fog computing are explored. Base architectures for application, software, security, computing resource management and networking are presented and are evaluated using a proposed maturity model.","['cloud computing', 'resource management', 'security', 'software']"
37,"An automatic modulation classification has a very broad application in wireless communications. Recently, deep learning has been used to solve this problem and achieved superior performance. In most cases, the input size is fixed in convolutional neural network (CNN)-based modulation classification. However, the duration of the actual radio signal burst is variable. When the signal length is greater than the CNN input length, how to make full use of the complete signal burst to improve the classification accuracy is a problem needs to be considered. In this paper, three fusion methods are proposed to solve this problem, such as voting-based fusion, confidence-based fusion, and feature-based fusion. The simulation experiments are done to analyze the performance of these methods. The results show that the three fusion methods perform better than the non-fusion method. The performance of the two fusion methods based on confidence and feature is very close, which is better than that of the voting-based fusion.","['modulation', 'convolution', 'wireless communication', 'deep learning', 'convolutional neural network']"
38,"The Internet of Things (IoT) connects billions of devices to afford inventive opportunities between things and people. The rapid development of products related to the IoT is a new challenge to keep security issues, lack of confidence, and understanding of the IoT. Analytical hierarchy process (AHP) is a classic multi-criteria decision making (MCDM) method used to analyze and scale complex problems and to obtain weights for the selected criteria. The vague and inconsistent information in real situations can lead to the decision maker's confusion. The decision makers cannot determine accurate judgments for all situations due to the conditions of uncertainty factors in real life; in addition to the limited knowledge and experience of decision makers. In this research, we present a neutrosophic AHP of the IoT in enterprises to help decision makers to estimate the influential factors. The estimation of influential factors can affect the success of the IoT-related enterprise. This study combines AHP methods with neutrosophic techniques to effectively present the criteria related to influential factors. The recommended alternatives are presented based on neutrosophic techniques satisfying the estimated influential factors for a successful enterprise. A case study is applied in Smart Village, Cairo, Egypt, to show the applicability of the proposed model. The smart village' consistency rate is measured after applying neutrosophic methodologies to reach to nearest optimum results. Additional case studies on the smart city in the U.K. and China have been presented to justify that our proposal can be used and replicated in different environments.","['internet of things', 'estimation', 'uncertainty', 'security']"
39,"Deep learning techniques have gained significant importance among artificial intelligence techniques for any computing applications. Among them, deep convolutional neural networks (DCNNs) is one of the widely used deep learning networks for any practical applications. The accuracy is generally high and the manual feature extraction process is not necessary in these networks. However, the high accuracy is achieved at the cost of huge computational complexity. The complexity in DCNN is mainly due to: 1) increased number of layers between input and output layers and 2) two set of parameters (one set of filter coefficients and another set of weights) in the fully connected network need to be adjusted. In this paper, the second aspect is targeted to reduce the computational complexity of conventional DCNN. Suitable modifications are performed in the training algorithm to reduce the number of parameter adjustments. The weight adjustment process in the fully connected layer is completely eliminated in the proposed modified approach. Instead, a simple assignment process is used to find the weights of this fully connected layer. Thus, the computational complexity is significantly reduced in the proposed approach. The application of modified DCNN is explored in the context of magnetic resonance brain tumor image classification. Abnormal brain tumor images from four different classes are used in this paper. The experimental results show promising results for the proposed approach.","['convolutional neural networks', 'convolution', 'deep learning', 'convolutional neural network']"
40,"Wireless sensor networks (WSNs) are a prominent fundamental technology of the Internet of Things (IoTs). Rather than device-to-device communications, group communications in the form of broadcasting and multicasting incur efficient message deliveries among resource-constrained sensor nodes in the IoT-enabled WSNs. Secure and efficient key management is in many cases used to protect the authenticity, integrity, and confidentiality of multicast messages. This paper develops two group key establishment protocols for secure multicast communications among the resource-constrained devices in IoT. Major deployment conditions and requirements of each protocol are described in terms of the specific IoT application scenarios. Furthermore, the applicability of the two protocols is analyzed and justified by a comprehensive analysis of the performance, scalability, and security of the protocols proposed.","['protocols', 'wireless sensor networks', 'internet of things', 'security']"
41,"With the wide applications of wireless sensor networks (WSNs) in various fields, such as environment monitoring, battlefield surveillance, healthcare, and intrusion detection, trust establishment among sensor nodes becomes a vital requirement to improve security, reliability, and successful cooperation. The existing trust management approaches for large-scale WSN are failed due to their low dependability (i.e., cooperation), higher communication, and memory overheads (i.e., resource inefficient). In this paper, we propose a novel and comprehensive trust estimation approach (LTS) for large-scale WSN that employs clustering to improve cooperation, trustworthiness, and security by detecting malicious (faulty or selfish) sensor nodes with reduced resource (memory and power) consumption. The proposed scheme (LTS) operates on two levels, namely, intra-cluster and inter-cluster along with distributed approach and centralized approach, respectively, to make accurate trust decision of sensor nodes with minimum overheads. LTS consists of unique features, such as robust trust estimation function, attack resistant, and efficient trust aggregation at the cluster, head to obtain the global feedback trust value. Data trust along with communication trust plays a significant role to cope with malicious nodes. In LTS, punishment and trust severity can be tuned according to the application requirement, which makes it an innovative LTS. Moreover, dishonest recommendations (outliers) are eliminated before aggregation at the base station by observing the statistical dispersion. The theoretical and mathematical validations along with simulation results exhibit the great performance of our proposed approach in terms of trust evaluation cost, prevention, and detection of malicious nodes as well as communication overhead.","['wireless sensor networks', 'security', 'reliability', 'estimation']"
42,"Energy efficiency is one of the main challenges in developing Wireless Sensor Networks (WSNs). Since communication has the largest share in energy consumption, efficient routing is an effective solution to this problem. Hierarchical clustering algorithms are a common approach to routing. This technique splits nodes into groups in order to avoid long-range communication which is delegated to the cluster head (CH). In this paper, we present a new clustering algorithm that selects CHs using the grey wolf optimizer (GWO). GWO is a recent swarm intelligence algorithm based on the behavior of grey wolves that shows impressive characteristics and competitive results. To select CHs, the solutions are rated based on the predicted energy consumption and current residual energy of each node. In order to improve energy efficiency, the proposed protocol uses the same clustering in multiple consecutive rounds. This allows the protocol to save the energy that would be required to reform the clustering. We also present a new dual-hop routing algorithm for CHs that are far from the base station and prove that the presented method ensures minimum and most balanced energy consumption while remaining nodes use single-hop communication. The performance of the protocol is evaluated in several different scenarios and it is shown that the proposed protocol improves network lifetime in comparison to a number of recent similar protocols.","['protocols', 'clustering algorithms', 'energy consumption', 'wireless sensor networks', 'routing']"
43,"The current cellular technology and vehicular networks cannot satisfy the mighty strides of vehicular network demands. Resource management has become a complex and challenging objective to gain expected outcomes in a vehicular environment. The 5G cellular network promises to provide ultra-high-speed, reduced delay, and reliable communications. The development of new technologies such as the network function virtualization (NFV) and software defined networking (SDN) are critical enabling technologies leveraging 5G. The SDN-based 5G network can provide an excellent platform for autonomous vehicles because SDN offers open programmability and flexibility for new services incorporation. This separation of control and data planes enables centralized and efficient management of resources in a very optimized and secure manner by having a global overview of the whole network. The SDN also provides flexibility in communication administration and resource management, which are of critical importance when considering the ad-hoc nature of vehicular network infrastructures, in terms of safety, privacy, and security, in vehicular network environments. In addition, it promises the overall improved performance. In this paper, we propose a flow-based policy framework on the basis of two tiers virtualization for vehicular networks using SDNs. The vehicle to vehicle (V2V) communication is quite possible with wireless virtualization where different radio resources are allocated to V2V communications based on the flow classification, i.e., safety-related flow or non-safety flows, and the controller is responsible for managing the overall vehicular environment and V2X communications. The motivation behind this study is to implement a machine learning-enabled architecture to cater the sophisticated demands of modern vehicular Internet infrastructures. The inclination towards robust communications in 5G-enabled networks has made it somewhat tricky to manage network slicing efficiently. This paper also presents a proof of concept for leveraging machine learning-enabled resource classification and management through experimental evaluation of special-purpose testbed established in custom mininet setup. Furthermore, the results have been evaluated using Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN). While concluding the paper, it is shown that the LSTM has outperformed the rest of classification techniques with promising results.","['resource management', 'machine learning', 'privacy', 'security']"
44,"The Internet of Things (IoT) consists of resource-constrained smart devices capable to sense and process data. It connects a huge number of smart sensing devices, i.e., things, and heterogeneous networks. The IoT is incorporated into different applications, such as smart health, smart home, smart grid, etc. The concept of smart healthcare has emerged in different countries, where pilot projects of healthcare facilities are analyzed. In IoT-enabled healthcare systems, the security of IoT devices and associated data is very important, whereas Edge computing is a promising architecture that solves their computational and processing problems. Edge computing is economical and has the potential to provide low latency data services by improving the communication and computation speed of IoT devices in a healthcare system. In Edge-based IoT-enabled healthcare systems, load balancing, network optimization, and efficient resource utilization are accurately performed using artificial intelligence (AI), i.e., intelligent software-defined network (SDN) controller. SDN-based Edge computing is helpful in the efficient utilization of limited resources of IoT devices. However, these low powered devices and associated data (private sensitive data of patients) are prone to various security threats. Therefore, in this paper, we design a secure framework for SDN-based Edge computing in IoT-enabled healthcare system. In the proposed framework, the IoT devices are authenticated by the Edge servers using a lightweight authentication scheme. After authentication, these devices collect data from the patients and send them to the Edge servers for storage, processing, and analyses. The Edge servers are connected with an SDN controller, which performs load balancing, network optimization, and efficient resource utilization in the healthcare system. The proposed framework is evaluated using computer-based simulations. The results demonstrate that the proposed framework provides better solutions for IoT-enabled healthcare systems.","['servers', 'artificial intelligence', 'internet of things', 'security']"
45,"A rise in the population has immensely increased the pressure on the agriculture sector. With the advent of technology, this decade is witnessing a shift from conventional approaches to the most advanced ones. The Internet of Things (IoT) has transformed both the quality and quantity of the agriculture sector. Hybridization of species along with the real-time monitoring of the farms paved a way for resource optimization. Scientists, research institutions, academicians, and most nations across the globe are moving towards the practice and execution of collaborative projects to explore the horizon of this field for serving mankind. The tech industry is racing to provide more optimal solutions. Inclusion of IoT, along with cloud computing, big data analytics, and wireless sensor networks can provide sufficient scope to predict, process, and analyze the situations and improve the activities in the real-time scenario. The concept of heterogeneity and interoperability of the devices by providing flexible, scalable, and durable methods, models are also opening new domains in this field. Therefore, this paper contributes towards the recent IoT technologies in the agriculture sector, along with the development of hardware and software systems. The public and private sector projects and startup's started all over the globe to provide smart and sustainable solutions in precision agriculture are also discussed. The current scenario, applications, research potential, limitations, and future aspects are briefly discussed. Based on the concepts of IoT a precision farming framework is also proposed in this article.","['internet of things', 'wireless sensor networks', 'monitoring', 'cloud computing']"
46,"Despite the numerous and noticeable inherited gains of Mobile Cloud Computing (MCC) in healthcare, its growth is being hindered by privacy and security challenges. Such issues require the utmost urgent attention to realize its full scale and efficient usage. There is a need to secure Health Information worldwide, regionally, and locally. To fully avail of the health services, it is crucial to put in place the demanded security practices for the prevention of security breaches and vulnerabilities. Hence, this research is deliberated on to provide requirement-oriented health information security using the Modular Encryption Standard (MES) based on the layered modeling of the security measures. The performance analysis shows that the proposed work excels, compared to other commonly used algorithms against the health information security at the MCC environment in terms of better performance and auxiliary qualitative security ensuring measures.","['security', 'cloud computing', 'privacy', 'encryption']"
47,"The development of mobile cloud computing technology has made location-based service (LBS) increasingly more popular. Given the continuous requests to cloud LBS servers, the amounts of location and trajectory information collected by LBS servers are continuously increasing. Privacy awareness for LBS has been extensively studied in recent years. Among the privacy concerns about LBS, trajectory privacy preservation is particularly important. Based on privacy preservation models, previous work have mainly focused on peer-to-peer and centralized architectures. However, the burden on users is heavy in peer-to-peer architectures, because user devices need to communicate with LBS servers directly. In centralized architectures, a trusted third party (TTP) is introduced, and acts as a bridge between users and the LBS server. Anonymity technologies, such as k-anonymity, mix-zone, and dummy technologies, are usually implemented by the TTP to ensure safety. There are certain drawbacks in TTP architectures: Users have no physical control of the TTP. Moreover, the TTP is more attractive to adversaries, because substantially more sensitive information is stored by the TTP. To solve the above-mentioned problems, in this paper, we propose a fog structure to store partial important data with the dummy anonymity technology to ensure physical control, which can be considered as absolutely trust. Compared with cloud computing, fog computing is a promising technique that extends the cloud computing to the edge of a network. Moreover, fog computing provides local computation and storage abilities, wide geo-distribution, and support for mobility. Therefore, mobile users' partial important information can be stored on a fog server to ensure better management. We take the principles of similarity, intersection, practicability, and correlation into consideration and design a dummy rotation algorithm with several properties. The effectiveness of the proposed method is validated through extensive simulations, which show that the proposed method can provide enhanced privacy preservation.","['servers', 'trajectory', 'privacy', 'cloud computing']"
48,"A blockchain as a trustworthy and secure decentralized and distributed network has been emerged for many applications such as in banking, finance, insurance, healthcare and business. Recently, many communities in blockchain networks want to deploy machine learning models to get meaningful knowledge from geographically distributed large-scale data owned by each participant. To run a learning model without data centralization, distributed machine learning (DML) for blockchain networks has been studied. While several works have been proposed, privacy and security have not been sufficiently addressed, and as we show later, there are vulnerabilities in the architecture and limitations in terms of efficiency. In this paper, we propose a privacy-preserving DML model for a permissioned blockchain to resolve the privacy, security, and performance issues in a systematic way. We develop a differentially private stochastic gradient descent method and an error-based aggregation rule as core primitives. Our model can treat any type of differentially private learning algorithm where non-deterministic functions should be defined. The proposed error-based aggregation rule is effective to prevent attacks by an adversarial node that tries to deteriorate the accuracy of DML models. Our experiment results show that our proposed model provides stronger resilience against adversarial attacks than other aggregation rules under a differentially private scenario. Finally, we show that our proposed model has high usability because it has low computational complexity and low transaction latency.","['blockchain', 'security', 'privacy', 'machine learning']"
49,"In this paper, we propose a fully ear-worn long-term blood pressure (BP) and heart rate (HR) monitor to achieve a higher wearability. Moreover, to enable practical application scenarios, we present a machine learning framework to deal with severe motion artifacts induced by head movements. We suggest situating all electrocardiogram (ECG) and photoplethysmography (PPG) sensors behind two ears to achieve a super wearability, and successfully acquire weak ear-ECG/PPG signals using a semi-customized platform. After introducing head motions toward real-world application scenarios, we apply a support vector machine classifier to learn and identify raw heartbeats from motion artifacts-impacted signals. Furthermore, we propose an unsupervised learning algorithm to automatically filter out residual distorted/faking heartbeats, for ECG-to-PPG pulse transit time (PTT) and HR estimation. Specifically, we introduce a dynamic time warping-based learning approach to quantify distortion conditions of raw heartbeats referring to a high-quality heartbeat pattern, which are then compared with a threshold to perform purification. The heartbeat pattern and the distortion threshold are learned by a K-medoids clustering approach and a histogram triangle method, respectively. Afterward, we perform a comparative analysis on ten PTT or PTT&HR-based BP learning models. Based on an acquired data set, the BP and HR estimation using the proposed algorithm has an error of -1.4±5.2 mmHg and 0.8±2.7 beats/min, respectively, both much lower than the state-of-the-art approaches. These results demonstrate the capability of the proposed machine learning-empowered system in ear-ECG/PPG acquisition and motion-tolerant BP/HR estimation. This proof-of-concept system is expected to illustrate the feasibility of ear-ECG/PPG-based motion-tolerant BP/HR monitoring.","['sensors', 'estimation', 'monitoring', 'machine learning']"
50,"Automatic extraction of buildings from remote sensing imagery plays a significant role in many applications, such as urban planning and monitoring changes to land cover. Various building segmentation methods have been proposed for visible remote sensing images, especially state-of-the-art methods based on convolutional neural networks (CNNs). However, high-accuracy building segmentation from high-resolution remote sensing imagery is still a challenging task due to the potentially complex texture of buildings in general and image background. Repeated pooling and striding operations used in CNNs reduce feature resolution causing a loss of detailed information. To address this issue, we propose a light-weight deep learning model integrating spatial pyramid pooling with an encoder-decoder structure. The proposed model takes advantage of a spatial pyramid pooling module to capture and aggregate multi-scale contextual information and of the ability of encoder-decoder networks to restore losses of information. The proposed model is evaluated on two publicly available datasets; the Massachusetts roads and buildings dataset and the INRIA Aerial Image Labeling Dataset. The experimental results on these datasets show qualitative and quantitative improvement against established image segmentation models, including SegNet, FCN, U-Net, Tiramisu, and FRRN. For instance, compared to the standard U-Net, the overall accuracy gain is 1.0% (0.913 vs. 0.904) and 3.6% (0.909 vs. 0.877) with a maximal increase of 3.6% in model-training time on these two datasets. These results demonstrate that the proposed model has the potential to deliver automatic building segmentation from high-resolution remote sensing images at an accuracy that makes it a useful tool for practical application scenarios.","['buildings', 'remote sensing', 'image segmentation', 'deep learning']"
51,"The Internet of Things (IoT) is playing a vital role in the rapid automation of the healthcare sector. The branch of IoT dedicated towards medical science is at times termed as Healthcare Internet of Things (H-IoT). The key elements of all H-IoT applications are data gathering and processing. Due to the large amount of data involved in healthcare, and the enormous value that accurate predictions hold, the integration of machine learning (ML) algorithms into H-IoT is imperative. This paper aims to serve both as a compilation as well as a review of the various state of the art applications of ML algorithms currently being integrated with H-IoT. Some of the most widely used ML algorithms have been briefly introduced and their use in various H-IoT applications has been analyzed in terms of their advantages, scope, and possible improvements. Applications have been divided into the domains of diagnosis, prognosis and spread control, assistive systems, monitoring, and logistics. In healthcare, practical use of a model requires it to be highly accurate and to have ample measures against security attacks. The applications of ML algorithms in H-IoT discussed in this paper have shown experimental evidence of accuracy and practical usability. The constraints and drawbacks of each of these applications have also been described.","['monitoring', 'internet of things', 'security', 'machine learning']"
52,"The advancement of the Internet of Things (IoT) has allowed for unprecedented data collection, automation, and remote sensing and actuation, transforming autonomous systems and bringing smart command and control into numerous cyber physical systems (CPS) that our daily lives depend on. Simultaneously, dramatic improvements in machine learning and deep neural network architectures have enabled unprecedented analytical capabilities, which we see in increasingly common applications and production technologies, such as self-driving vehicles and intelligent mobile applications. Predictably, these technologies have seen rapid adoption, which has left many implementations vulnerable to threats unforeseen or undefended against. Moreover, such technologies can be used by malicious actors, and the potential for cyber threats, attacks, intrusions, and obfuscation that are only just being considered, applied, and countered. In this paper, we consider the good, the bad, and the ugly use of machine learning for cybersecurity and CPS/IoT. In detail, we consider the numerous benefits (good use) that machine learning has brought, both in general, and specifically for security and CPS/IoT, such as the improvement of intrusion detection mechanisms and decision accuracy in CPS/IoT. More pressing, we consider the vulnerabilities of machine learning (bad use) from the perspectives of security and CPS/IoT, including the ways in which machine learning systems can be compromised, misled, and subverted at all stages of the machine learning life-cycle (data collection, pre-processing, training, validation, implementation, etc.). Finally, the most concerning, a growing trend has been the utilization of machine learning in the execution of cyberattacks and intrusions (ugly use). Thus, we consider existing mechanisms with the potential to improve target acquisition and existing threat patterns, as well as those that can enable novel attacks yet to be seen.","['machine learning', 'internet of things', 'training', 'security']"
53,"Edge computing has recently emerged as an extension to cloud computing for quality of service (QoS) provisioning particularly delay guarantee for delay-sensitive applications. By offloading the computationally intensive workloads to edge servers, the quality of computation experience, e.g., network transmission delay and transmission energy consumption, could be improved greatly. However, the computation resource of an edge server is so scarce that it cannot respond quickly to the bursting computation requirements. Accordingly, queuing delay is un-negligible in a computationally intensive environment, e.g., a computing environment consists of the Internet of Things (IoT) applications. In addition, the computation energy consumption in edge servers may be higher than that in clouds when the workload is heavy. To provide QoS for end users while achieving green computing for computing systems, the cooperation between edge servers and the cloud is significantly important. In this paper, the energy-efficient and delay-guaranteed workload allocation problem in an IoT-edge-cloud computing system are investigated. We formulate a delay-based workload allocation problem which suggests the optimal workload allocations among local edge server, neighbor edge servers, and cloud toward the minimal energy consumption as well as the delay guarantee. The problem is then tackled using a delay-base workload allocation (DBWA) algorithm based on Lyapunov drift-plus-penalty theory. The theoretical analysis and simulation results have been conducted to demonstrate the efficiency of the proposal for energy efficiency and delay guarantee in an IoT-edge-cloud system.","['cloud computing', 'servers', 'energy consumption', 'internet of things']"
54,"As an algorithm with excellent performance, convolutional neural network has been widely used in the field of image processing and achieved good results by relying on its own local receptive fields, weight sharing, pooling, and sparse connections. In order to improve the convergence speed and recognition accuracy of the convolutional neural network algorithm, this paper proposes a new convolutional neural network algorithm. First, a recurrent neural network is introduced into the convolutional neural network, and the deep features of the image are learned in parallel using the convolutional neural network and the recurrent neural network. Secondly, according to the idea of ResNet's skip convolution layer, a new residual module ShortCut3-ResNet is constructed. Then, a dual optimization model is established to realize the integrated optimization of the convolution and full connection process. Finally, the effects of various parameters of the convolutional neural network on the network performance are analyzed through simulation experiments, and the optimal network parameters of the convolutional neural network are finally set. Experimental results show that the convolutional neural network algorithm proposed in this paper can learn the diverse features of the image, and improve the accuracy of feature extraction and image recognition ability of the convolutional neural network.","['feature extraction', 'convolution', 'optimization', 'convolutional neural network']"
55,"Iris recognition is one of the most representative identification technologies in biometric recognition, which is widely used in various fields. Recently, many deep learning methods have been used in biometric recognition, owing to their advantages such as automatic learning, high accuracy, and strong generalization ability. The deep convolutional neural network (CNN) is the mainstream method of image processing widely used in many domains, but it has poor anti-noise capacity in image classification and is easily affected by slight disturbances. CNN also needs a large number of samples for training. The recent capsule network not only has high recognition accuracy in classification tasks but can also learn part-whole relationships, increasing the robustness of the model. Furthermore, it can be trained using a small number of samples. In this paper, we propose a deep learning method based on the capsule network architecture in iris recognition. The structure detail of the network is adjusted, and we provide a modified routing algorithm based on the dynamic routing between two capsule layers to make this technique adapt to iris recognition. Migration learning makes the deep learning method available even when the number of samples is limited. Therefore, three state-of-the-art pretrained models, VGG16, InceptionV3, and ResNet50, are introduced. We divide the three networks into a series of subnetwork structures according to the number of their major constituent blocks. They are used as the convolutional part to extract primary features, instead of a single convolutional layer in the capsule network. Our experiments are conducted on three iris datasets, JluIrisV3.1, JluIrisV4, and CASIA-V4 Lamp, to analyze the performance of different network structures. We also test the proposed networks in simulated strong and weak light environments, showing that the networks with capsule architecture are more stable than those without.","['deep learning', 'convolution', 'routing', 'training']"
56,"The encrypted image retrieval in cloud computing is a key technology to realize the massive images of storage and management and images safety. In this paper, a novel feature extraction method for encrypted image retrieval is proposed. First, the improved Harris algorithm is used to extract the image features. Next, the Speeded-Up Robust Features algorithm and the Bag of Words model are applied to generate the feature vectors of each image. Then, Local Sensitive Hash algorithm is applied to construct the searchable index for the feature vectors. The chaotic encryption scheme is utilized to protect images and indexes security. Finally, secure similarity search is executed on the cloud server. The experimental results show that compared with the existing encryption retrieval schemes, the proposed retrieval scheme not only reduces the time consumption but also improves the image retrieval accuracy.","['feature extraction', 'indexes', 'encryption', 'cloud computing']"
57,"The recent advancements in Internet of Things (IoT), cloud computing, and Artificial Intelligence (AI) transformed the conventional healthcare system into smart healthcare. By incorporating key technologies such as IoT and AI, medical services can be improved. The convergence of IoT and AI offers different opportunities in healthcare sector. In this view, the current research article presents a new AI and IoT convergence-based disease diagnosis model for smart healthcare system. The major goal of this article is to design a disease diagnosis model for heart disease and diabetes using AI and IoT convergence techniques. The presented model encompasses different stages namely, data acquisition, preprocessing, classification, and parameter tuning. IoT devices such as wearables and sensors permit seamless data collection while AI techniques utilize the data in disease diagnosis. The proposed method uses Crow Search Optimization algorithm-based Cascaded Long Short Term Memory (CSO-CLSTM) model for disease diagnosis. In order to achieve better classification of the medical data, CSO is applied to tune both `weights' and `bias' parameters of CLSTM model. Besides, isolation Forest (iForest) technique is employed in this research work to remove the outliers. The application of CSO helps in considerable improvement in the diagnostic outcomes of CLSTM model. The performance of CSO-LSTM model was validated using healthcare data. During the experimentation, the presented CSO-LSTM model accomplished the maximum accuracies of 96.16% and 97.26% in diagnosing heart disease and diabetes respectively. Therefore, the proposed CSO-LSTM model can be employed as an appropriate disease diagnosis tool for smart healthcare systems.","['artificial intelligence', 'sensors', 'internet of things', 'convergence', 'cloud computing']"
58,"Flying ad hoc networks (FANETs) have dynamic topology because of the mobile unmanned aerial vehicles (UAVs). The limited battery resource and mobility of UAVs cause unstable routing in the FANET. In this paper, we try to minimize this issue with the help of an efficient clustering scheme. We propose a bio-inspired clustering scheme for FANETs (BICSF), which uses the hybrid mechanism of glowworm swarm optimization (GSO) and krill herd (KH). The proposed scheme uses energy aware cluster formation and cluster head election on the basis of the GSO algorithm. Furthermore, we propose an efficient cluster management algorithm using the behavioral study of KH. We also use genetic operators such as mutation and crossover for the optimal position of the UAV. For route selection, we propose a path detection function based on the weighted residual energy, number of neighbors, and distance between the UAVs for efficient communication. The performance of BICSF is evaluated in terms of cluster building time, energy consumption, cluster lifetime, and the probability of delivery success with grey wolf optimization and ant colony optimization-based clustering algorithms.","['clustering algorithms', 'topology', 'routing', 'energy consumption']"
59,"Energy management system (EMS) is responsible for the optimal operation of microgrids. EMS adjusts its operational schedule for near future by using the available information. Market price signals are generally used for the operation of microgrids, which are obtained by using estimation/ forecasting methods. However, it is difficult to precisely predict the market prices due to the involvement of various complex factors like weather, policy, demand, errors in forecasting methods, and fuel cost. Therefore, in this paper, the uncertainties associated with the real-time market price signals (buying and selling) are realized via a robust optimization method. In addition to market price signals, uncertainties associated with renewable power sources and forecasted load values are also considered. Initially, a deterministic model is formulated for an ac/dc hybrid microgrid. Then a min-max robust counterpart is formulated by considering the worstcase uncertainties. Finally, an equivalent mixed integer problem is formulated by using linear duality and other optimality conditions. The developed model can provide feasible solutions for all the scenarios if the uncertainties fluctuate within the specified bounds. The effect of market price uncertainties on internal power transfer and external power trading, operation cost, the state-of-charge of energy storage elements, and unit commitment of dispatchable generators is analyzed. Taguchi's orthogonal array (OA) method is used to find the worst-case scenario within the specified uncertainty bounds. Then, Monte Carlo method is used to generate various scenarios within the uncertainty bounds to evaluate the robustness of the selected scenario via Taguchi's OA method. Finally, a violation index is formulated to evaluate the robustness of the proposed approach against the deterministic model. Simulations results have validated the robustness of the proposed optimization strategy.","['uncertainty', 'robustness', 'optimization', 'generators']"
60,"Through three development routes of authentication, communication, and computing, the Internet of Things (IoT) has become a variety of innovative integrated solutions for specific applications. However, due to the openness, extensiveness and resource constraints of IoT, each layer of the three-tier IoT architecture suffers from a variety of security threats. In this work, we systematically review the particularity and complexity of IoT security protection, and then find that Artificial Intelligence (AI) methods such as Machine Learning (ML) and Deep Learning (DL) can provide new powerful capabilities to meet the security requirements of IoT. We analyze the technical feasibility of AI in solving IoT security problems and summarize a general process of AI solutions for IoT security. For four serious IoT security threats: device authentication, Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks defense, intrusion detection and malware detection, we summarize representative AI solutions and compare the different algorithms and technologies used by various solutions. It should be noted that although AI provides many new capabilities for the security protection of IoT, it also brings new potential challenges and possible negative effects to IoT in terms of data, algorithm and architecture. In the future, how to solve these challenges can serve as potential research directions.","['internet of things', 'artificial intelligence', 'deep learning', 'machine learning', 'security']"
61,"The open nature of radio propagation makes wireless transmissions exposed to unauthorized users and become vulnerable to both the jamming and eavesdropping attacks. In industrial environments, wireless transmissions are also adversely affected by the presence of large-bodied obstructing machinery, metallic friction-induced impairments, and equipment noise. This may result in the failure of wireless transmissions and thus wastes the precious energy resources for industrial wireless sensor networks (IWSNs). This paper is motivated to present a review on the challenges and solutions of improving the physical-layer security and reliability for IWSNs. We first discuss some wireless reliability enhancement techniques for mitigating the background interference, path loss, multipath fading, and link failure. Then, we provide an overview of wireless jamming and eavesdropping attacks along with their countermeasures, where a jammer attempts to emit an interfering radio signal for disrupting the desired communications between a wireless sensor and its sink, while an eavesdropper intends to tap the confidential sensor-sink transmissions. Additionally, we evaluate the tradeoff between the security and reliability, called security-reliability tradeoff, in the context of wireless sensor-sink transmissions. Finally, we discuss a range of open challenges and future trends for IWSNs, including the energy-efficient security and reliability designs, joint jamming-and-eavesdropping defense mechanisms, as well as the energy harvesting for IWSNs.","['reliability', 'wireless sensor networks', 'security', 'interference']"
62,"The Internet of Things (IoT) refers to the network of devices which contain electronics, sensors or software that enables them to connect at anytime and anywhere through a cyber-physical system. Before the establishment of such a system, it should be considered to what extent the users are ready to adopt and use it in their daily routines. Therefore, this paper explores users' attitudes towards using IoT technologies to receive healthcare services. This is in contrast to most previous research, which has studied the technical requirements or devices of the IoT that are required in healthcare services, or ways in which connectivity and performance can be improved using the IoT. Based on known models of technology acceptance, an integrated framework was developed to investigate the impact of security and privacy concerns, and familiarity with the technology, on users' trust in the IoT, and then to measure the effect of that trust on Omani users' attitudes regarding use of IoT technologies to receive healthcare services. This framework enabled the measurement of risk perception as a mediator between user trust and their attitudes towards using the IoT. Data were collected from 387 respondents and were analysed using SPSS 25 and AMOS 25 statistics software. Exploratory and confirmatory analysis and structural equation modelling were applied. The findings showed that levels of security, privacy and familiarity affected trust in the IoT. Furthermore, these levels of trust in the IoT were found to affect both users' perceptions of risk in, and their attitude towards, using the IoT. The users' risk perception partially mediated the relations between users' trust and their attitude regarding use of the IoT. The framework was supported and interpreted by 40 per cent of the variance in the attitude towards using the IoT in healthcare, while the mediator showed 47 per cent of the variance in the attitude towards using the IoT in healthcare.","['internet of things', 'security', 'privacy', 'sensors']"
63,"With the rapid growth of surveillance cameras to monitor the human activity demands such system which recognize the violence and suspicious events automatically. Abnormal and violence action detection has become an active research area of computer vision and image processing to attract new researchers. The relevant literature presents different techniques for detection of such activities from the video proposed in the recent years. This paper reviews various state-of-the-art techniques of violence detection. In this paper, the methods of detection are divided into three categories that is based on classification techniques used: violence detection using traditional machine learning, using support vector machine (SVM), and using deep learning. The feature extraction techniques and object detection techniques of the each single method are also presented. Moreover, datasets and video features that used in the techniques, which play a vital role in recognition process are also discussed. For better understanding, the steps of the research approaches have been presented in an architecture diagram. The overall research findings have been discussed which may be helpful for finding the potential future work in this research domain.","['feature extraction', 'cameras', 'deep learning', 'machine learning']"
64,"Internet of things IoT is playing a remarkable role in the advancement of many fields such as healthcare, smart grids, supply chain management, etc. It also eases people's daily lives and enhances their interaction with each other as well as with their surroundings and the environment in a broader scope. IoT performs this role utilizing devices and sensors of different shapes and sizes ranging from small embedded sensors and wearable devices all the way to automated systems. However, IoT networks are growing in size, complexity, and number of connected devices. As a result, many challenges and problems arise such as security, authenticity, reliability, and scalability. Based on that and taking into account the anticipated evolution of the IoT, it is extremely vital not only to maintain but to increase confidence in and reliance on IoT systems by tackling the aforementioned issues. The emergence of blockchain opened the door to solve some challenges related to IoT networks. Blockchain characteristics such as security, transparency, reliability, and traceability make it the perfect candidate to improve IoT systems, solve their problems, and support their future expansion. This paper demonstrates the major challenges facing IoT systems and blockchain's proposed role in solving them. It also evaluates the position of current researches in the field of merging blockchain with IoT networks and the latest implementation stages. Additionally, it discusses the issues related to the IoT-blockchain integration itself. Finally, this research proposes an architectural design to integrate IoT with blockchain in two layers using dew and cloudlet computing. Our aim is to benefit from blockchain features and services to guarantee a decentralized data storage and processing and address security and anonymity challenges and achieve transparency and efficient authentication service.","['internet of things', 'blockchain', 'security', 'sensors', 'reliability']"
65,"A heterogeneous ring domain communication topology with equal area in each ring is presented in this paper in an effort to solve the energy balance problem in original IPv6 routing protocol for low power and lossy networks (RPL). A new clustering algorithm and event-driven cluster head rotation mechanism are also proposed based on this topology. The clustering information announcement message and clustering acknowledgment message were designed according to RFC and original RPL message structure. An energy-efficient heterogeneous ring clustering (E2HRC) routing protocol for wireless sensor networks is then proposed and the corresponding routing algorithms and maintenance methods are established. Related messages are analyzed in detail. Experimental results show that in comparison against the original RPL, the E2HRC routing protocol more effectively balances wireless sensor network energy consumption, thus decreasing both node energy consumption and the number of control messages.","['routing', 'wireless sensor networks', 'energy consumption', 'topology']"
66,"The Internet of Things (IoT) is rapidly becoming an integral component of the industrial market in areas such as automation and analytics, giving rise to what is termed as the Industrial IoT (IIoT). The IIoT promises innovative business models in various industrial domains by providing ubiquitous connectivity, efficient data analytics tools, and better decision support systems for a better market competitiveness. However, IIoT deployments are vulnerable to a variety of security threats at various levels of the connectivity and communications infrastructure. The complex nature of the IIoT infrastructure means that availability, confidentiality and integrity are difficult to guarantee, leading to a potential distrust in the network operations and concerns of loss of critical infrastructure, compromised safety of network end-users and privacy breaches on sensitive information. This work attempts to look at the requirements currently specified for a secure IIoT ecosystem in industry standards, such as Industrial Internet Consortium (IIC) and OpenFog Consortium, and to what extent current IIoT connectivity protocols and platforms hold up to the standards with regard to security and privacy. The paper also discusses possible future research directions to enhance the security, privacy and safety of the IIoT.","['protocols', 'standards', 'privacy', 'internet of things']"
67,"With an enormous range of applications, the Internet of Things (IoT) has magnetized industries and academicians from everywhere. IoT facilitates operations through ubiquitous connectivity by providing Internet access to all the devices with computing capabilities. With the evolution of wireless infrastructure, the focus from simple IoT has been shifted to smart, connected and mobile IoT (M-IoT) devices and platforms, which can enable low-complexity, low-cost and efficient computing through sensors, machines, and even crowdsourcing. All these devices can be grouped under a common term of M-IoT. Even though the positive impact on applications has been tremendous, security, privacy and trust are still the major concerns for such networks and insufficient enforcement of these requirements introduces non-negligible threats to M-IoT devices and platforms. Thus, it is important to understand the range of solutions which are available for providing a secure, privacy-compliant, and trustworthy mechanism for M-IoT. There is no direct survey available, which focuses on security, privacy, trust, secure protocols, physical layer security and handover protections in M-IoT. This paper covers such requisites and presents comparisons of state-the-art solutions for IoT which are applicable to security, privacy, and trust in smart and connected M-IoT networks. Apart from these, various challenges, applications, advantages, technologies, standards, open issues, and roadmap for security, privacy and trust are also discussed in this paper.","['privacy', 'protocols', 'internet of things', 'security']"
68,"Network traffic classification serves as a building block for important tasks such as security and quality of service management. The field has been studied for a long time, with many techniques such as classical machine learning and deep learning methods currently available. However, the emergence of stronger encryption protocols has led to the rise of new challenges. One of the challenges is capturing and labeling a large amount of encrypted traffic data especially for training deep learning classifiers, as current techniques rely on deep packet inspection tools (DPI) which perform poorly on encrypted traffic. In this paper, we propose a semi-supervised learning approach using Deep Convolutional Generative Adversarial Network (DCGAN). The basic idea is to utilize the samples generated by DCGAN generators as well as unlabeled data to improve the performance of a classifier trained on a few labeled samples. Thus, alleviating the difficulties associated with large dataset collecting and labeling. To demonstrate the efficacy of our approach, we evaluated our model using a self-collected dataset of the recently established QUIC protocol as well as publicly available ISCX VPN-NonVPN dataset. Our approach is able to achieve 89% and 78% accuracy with a very small number of labeled samples (just 10% of the dataset) on both QUIC and ISCX VPN-NonVPN datasets respectively.","['deep learning', 'generators', 'protocols', 'encryption']"
69,"Recently, the Internet of Things (IoT) has an important role in the growth and development of digitalized electric power stations while offering ambitious opportunities, specifically real-time monitoring and cybersecurity. In this regard, this paper introduces a novel IoT architecture for the online monitoring of the gas-insulated switchgear (GIS) status instead of the traditional observation methods. The proposed IoT architecture is derived from the concept of the cyber-physic system (CPS) in Industry 4.0. However, the cyber-attacks and the classification of the GIS insulation defects represent the main challenges against the implementation of IoT topology for the online monitoring and tracking of the GIS status. For this purpose, advanced machine learning techniques are utilized to detect cyber-attacks to conduct the paradigm and verification. Different test scenarios on various defects in GIS are performed to demonstrate the effectiveness of the proposed IoT architecture. Partial discharge pulse sequence features are extracted for each defect to represent the inputs for IoT architecture. The results confirm that the proposed IoT architecture based on the machine learning technique, that is the extreme gradient boosting (XGBoost), can visualize all defects in the GIS with different alarms, besides showing the cyber-attacks on the networks effectively. Furthermore, the defects of GIS and the fake data due to the cyber-attacks are recognized and presented on the dashboard of the proposed IoT platform with high accuracy and more clarified visualization to enhance the decision-making about the GIS status.","['monitoring', 'machine learning', 'topology', 'internet of things']"
70,"Convolutional Neural Networks (CNNs) have become the de facto technique for image feature extraction in recent years. However, their design and construction remains a complicated task. As more developments are made in progressing the internal components of CNNs, the task of assembling them effectively from core components becomes even more arduous. To overcome these barriers, we propose the Swarm Optimized Block Architecture, combined with an enhanced adaptive particle swarm optimization (PSO) algorithm for deep CNN model evolution. The enhanced PSO model employs adaptive acceleration coefficients generated using several cosine annealing mechanisms to overcome stagnation. Specifically, we propose a combined training and structure optimization process for deep CNN model generation, where the proposed PSO model is utilized to explore a bespoke search space defined by a simplified block-based structure. The proposed PSO model not only devises deep networks specifically for image classification, but also builds and pre-trains models for transfer learning tasks. To significantly reduce the hardware and computational cost of the search, the devised CNN model is optimized and trained simultaneously, using a weight sharing mechanism and a final fine-tuning process. Our system compares favorably with related research for optimized deep network generation. It achieves an error rate of 4.78% on the CIFAR-10 image classification task, with 34 hours of combined optimization and training, and an error rate of 25.42% on the CIFAR-100 image data set in 36 hours. All experiments were performed on a single NVIDIA GTX 1080Ti consumer GPU.","['optimization', 'training', 'feature extraction', 'convolutional neural networks']"
71,"The cognitive radio (CR) network consists of primary users (PUs) and secondary users (SUs). The SUs in the CR network senses the spectrum band to opportunistically access the white space. Exploiting the white spaces helps to improve the spectrum efficiency. Owing to the excellent learning ability of machine learning/deep learning framework, many works in the recent past have applied shallow/deep multi-layer perceptron approach for spectrum sensing. However, the multi-layer perceptron networks are not well suited for time-series data due to the absence of memory elements. On the other hand, long short-term memory (LSTM) network, an improved version of Recurrent neural network is well suited for time-series data. In this paper, we propose an LSTM based spectrum sensing (LSTM-SS), which learns the implicit features from the spectrum data, for instance, the temporal correlation (i.e., the correlation between the present and past timestamp).Moreover, the CR systems also exploits the PU activity statistics to improve the CR performance. In this context, we compute the PU activity statistics like on and off period duration, duty cycle and propose the PU activity statistics based spectrum sensing (PAS-SS) to enhance the sensing performance. The proposed sensing schemes are validated on the spectrum data of various radio technologies acquired using an experimental test-bed setup. The proposed LSTM-SS scheme is compared with the state of the art spectrum sensing techniques. Experimental results indicate that the proposed schemes has improved detection performance and classification accuracy at low signal to noise ratio regimes. We notice that the improvement achieved is at the cost of longer training time and a nominal increase in execution time.","['signal to noise ratio', 'correlation', 'machine learning', 'deep learning']"
72,"Wireless sensor networks suffer from some limitations such as energy constraints and the cooperative demands essential to perform multi-hop geographic routing for Internet of things (IoT) applications. Quality of Service (QoS) depends to a great extent on offering participating nodes an incentive for collaborating. This paper presents a mathematical model for a new-generation of forwarding QoS routing determination that enables allocation of optimal path to satisfy QoS parameters to support a wide range of communication-intensive IoT's applications. The model is used to investigate the effects of multi-hop communication on a traffic system model designed with a Markov discrete-time M/M/1 queuing model, applicable to green deployment of duty-cycle sensor nodes. We present analytical formulation for the biterror-rate, and a critical path-loss model is defined to the specified level of trust among the most frequently used nodes. Additionally, we address the degree of irregularity parameter for promoting adaptation to geographic switching with respect to two categories of transmission in distributed systems: hop-by-hop and end-to-end retransmission schemes. The simulations identified results for the average packet delay transmission, the energy consumption for transmission, and the throughput. The simulations offer insights into the impact of radio irregularity on the neighbor-discovery routing technique of both schemes. Based on the simulation results, the messages en-coded with non-return-to-zero have more green efficiency over multihop IoT (without loss of connectivity between nodes) than those encoded with the Manchester operation. The findings presented in this paper are of great help to designers of IoT.","['quality of service', 'mathematical model', 'wireless sensor networks', 'routing', 'energy consumption', 'internet of things']"
73,"As one of the key components in mechanical systems, rotatory machine plays a significant role in safe and stable operation. Accurate prediction of the Remaining Useful Life (RUL) of rotatory machine contributes to realization of intelligent operation and maintenance for mechanical manufacturing. In order to overcome the limitations of traditional machine learning algorithms in dealing with complex nonlinear signals, a novel prediction framework for RUL of rotatory machine based on deep learning is proposed in this paper. One-dimensional convolutional neural network is utilized to extract local features from the original signal sequence. In addition, the proposed framework analyzes sensor signals and predicts RUL by combining Long Short-Term Memory (LSTM) network with attention mechanism. Multi-layer LSTM is set up to extract useful temporal features layer by layer and improve the robustness of the model, while attention mechanism is able to effectively solve the problem of information loss in the long-distance signal transmission of LSTM. Through the feature extraction of multi-layer LSTM and the strong supervision ability of attention mechanism, the RUL of rotatory machine can be accurately predicted. The experimental results show that the proposed method for RUL estimation is efficient and has higher prediction accuracy than the traditional machine learning algorithms.","['feature extraction', 'convolution', 'machine learning', 'deep learning']"
74,"Signal modulation identification (SMI) plays a very important role in orthogonal frequency-division multiplexing (OFDM) systems. Currently, SMI methods are often implemented via feature extraction based on machine learning. However, the traditional methods encounter a bottleneck where the probability of correct classification (PCC) is very limited and hence it is hard to implement in practical OFDM systems due to the fact that traditional methods are difficult to extract feature of the OFDM signals. In order solve these problems, we propose a deep learning (DL) based SMI method for identifying OFDM signals. Specifically, convolutional neural network (CNN) is adopted to train in-phase and quadrature (IQ) samples for OFDM signals. Then we choose dropout layer to prevent overfitting and improve its identification accuracy. In addition, datasets with different modulation modes are adopted to verify our trained CNN. Experiments are conducted to show that our proposed method achieves higher accuracy and better consistency than traditional methods. Moreover, extensive results confirm that the proposed method performs robustly in different datasets.","['ofdm', 'modulation', 'convolution', 'feature extraction']"
75,"Multiple-input multiple-output (MIMO) radars and synthetic aperture radar (SAR) techniques are well researched and have been effectively combined for many imaging applications ranging from remote sensing to security. Despite numerous studies that apply MIMO concepts to SAR imaging, the design process of a MIMO-SAR system is non-trivial, especially for millimeter-wave (mmWave) imaging systems. Many issues have to be carefully addressed. Besides, compared with conventional monostatic sampling schemes or MIMO-only solutions, efficient image reconstruction methods for MIMO-SAR topologies are more complicated in short-range applications. To address these issues, we present highly-integrated and reconfigurable MIMO-SAR testbeds, along with examples of three-dimensional (3-D) image reconstruction algorithms optimized for MIMO-SAR configurations. The presented testbeds utilize commercially available wideband mmWave sensors and motorized rail platforms. Several aspects of the MIMO-SAR testbed design process, including MIMO array calibration, electrical/mechanical synchronization, system-level verification, and performance evaluation, are described. We present three versions of MIMO-SAR testbeds with different implementation costs and accuracies to provide alternatives for other researchers who want to implement their testbed framework. Several representative examples in various real-world imaging applications are presented to demonstrate the capabilities of the proposed testbeds and algorithms.","['imaging', 'sensors', 'synthetic aperture radar', 'image reconstruction']"
76,"Hyperspectral imaging has become a mature technology which brings exciting possibilities in various domains, including satellite image analysis. However, the high dimensionality and volume of such imagery is a serious problem which needs to be faced in Earth Observation applications, where efficient acquisition, transfer and storage of hyperspectral images are key factors. To reduce the time (and ultimately cost) of transferring hyperspectral data from a satellite back to Earth, various band selection algorithms have been proposed. They are built upon the observation that for a vast number of applications only a subset of all bands convey the important information about the underlying material, hence we can safely decrease the data dimensionality without deteriorating the performance of hyperspectral classification and segmentation techniques. In this paper, we introduce a novel algorithm for hyperspectral band selection that couples new attention-based convolutional neural networks used to weight the bands according to their importance with an anomaly detection technique which is exploited for selecting the most important bands. The proposed attention-based approach is data-driven, re-uses convolutional activations at different depths of a deep architecture, identifying the most informative regions of the spectrum. Also, it is modular, easy to implement, seamlessly applicable to any convolutional network, and can be trained end-to-end using gradient descent. Our rigorous experiments, performed over benchmark sets and backed up with statistical tests, showed that the deep models equipped with the attention mechanism are competitive with the state-of-the-art band selection techniques and can work orders or magnitude faster, they deliver high-quality classification, and consistently identify significant bands in the training data, permitting the creation of refined and extremely compact sets that retain the most meaningful features. Also, the attention modules do not deteriorate the classification abilities, and slow down neither training nor inference of the deep models.","['hyperspectral imaging', 'training', 'convolutional neural networks', 'convolutional neural network']"
77,"To collect data of distributed sensors located at different areas in challenging scenarios through artificial way is obviously inefficient, due to the numerous labor and time. Unmanned Aerial Vehicle (UAV) emerges as a promising solution, which enables multi-UAV collect data automatically with the preassigned path. However, without a well-planned path, the required number and consumed energy of UAVs will increase dramatically. Thus, minimizing the required number and optimizing the path of UAVs, referred as multi-UAV path planning, are essential to achieve the efficient data collection. Therefore, some heuristic algorithms such as Genetic Algorithm (GA) and Ant Colony Algorithm (ACA) which works well for multi-UAV path planning have been proposed. Nevertheless, in challenging scenarios with high requirement for timeliness, the performance of convergence speed of above algorithms is imperfect, which will lead to an inefficient optimization process and delay the data collection. Deep learning (DL), once trained by enough datasets, has high solving speed without worries about convergence problems. Thus, in this paper, we propose an algorithm called Deep Learning Trained by Genetic Algorithm (DL-GA), which combines the advantages of DL and GA. GA will collect states and paths from various scenarios and then use them to train the deep neural network so that while facing the familiar scenarios, it can rapidly give the optimized path, which can satisfy high timeliness requirements. Numerous experiments demonstrate that the solving speed of DL-GA is much faster than GA almost without loss of optimization capacity and even can outperform GA under some specific conditions.","['sensors', 'optimization', 'convergence', 'deep learning']"
78,"Arbitrary-oriented object detection is an important task in the field of remote sensing object detection. Existing studies have shown that the polar coordinate system has obvious advantages in dealing with the problem of rotating object modeling, that is, using fewer parameters to achieve more accurate rotating object detection. However, present state-of-the-art detectors based on deep learning are all modeled in Cartesian coordinates. In this article, we introduce the polar coordinate system to the deep learning detector for the first time, and propose an anchor free Polar Remote Sensing Object Detector (P-RSDet), which can achieve competitive detection accuracy via using simpler object representation model and less regression parameters. In P-RSDet method, arbitrary-oriented object detection can be achieved by predicting the center point and regressing one polar radius and two polar angles. Besides, in order to express the geometric constraint relationship between the polar radius and the polar angle, a Polar Ring Area Loss function is proposed to improve the prediction accuracy of the corner position. Experiments on DOTA, UCAS-AOD and NWPU VHR-10 datasets show that our P-RSDet achieves state-of-the-art performances with simpler model and less regression parameters.","['detectors', 'remote sensing', 'object detection', 'deep learning']"
79,"Blockchain technology is a distributed ledger with records of data containing all details of the transactions carried out and distributed among the nodes present in the network. All the transactions carried out in the system are confirmed by consensus mechanisms, and the data once stored cannot be altered. Blockchain technology is the necessary technology behind Bitcoin, which is a popular digital Cryptocurrency. “Cloud computing is a practice of using a network of remote servers hosted on the internet to store, manage, and process data, rather than a local server or a personal computer.” It is still facing many challenges like data security, data management, compliance, reliability. In this article, we have mentioned some of the significant challenges faced by the cloud and proposed solutions by integrating it with blockchain technology. We tend to investigate a brief survey on earlier studies focused on blockchain integrating with the cloud to depict their supremacy. In this survey, we have also developed architecture integrating blockchain with cloud revealing the communication between blockchain and cloud.","['cloud computing', 'blockchain', 'security', 'servers']"
80,"Medical image classification plays an important role in disease diagnosis since it can provide important reference information for doctors. The supervised convolutional neural networks (CNNs) such as DenseNet provide the versatile and effective method for medical image classification tasks, but they require large amounts of data with labels and involve complex and time-consuming training process. The unsupervised CNNs such as principal component analysis network (PCANet) need no labels for training but cannot provide desirable classification accuracy. To realize the accurate medical image classification in the case of a small training dataset, we have proposed a light-weighted hybrid neural network which consists of a modified PCANet cascaded with a simplified DenseNet. The modified PCANet has two stages, in which the network produces the effective feature maps at each stage by convoluting inputs with various learned kernels. The following simplified DenseNet with a small number of weights will take all feature maps produced by the PCANet as inputs and employ the dense shortcut connections to realize accurate medical image classification. To appreciate the performance of the proposed method, some experiments have been done on mammography and osteosarcoma histology images. Experimental results show that the proposed hybrid neural network is easy to train and it outperforms such popular CNN models as PCANet, ResNet and DenseNet in terms of classification accuracy, sensitivity and specificity.","['kernel', 'neural networks', 'training', 'convolution']"
81,"The application of the Internet of Things in agricultural development usually occurs via a monitoring network that consists of a large number of sensor nodes, thus gradually transforming agriculture from a human-oriented and single-machine-centric production model to an information- and software-centric production model. Due to the large area coverage of agriculture and the variety of production objects, if all farmland perception information is gathered into the cloud server, the server will exert greater pressure on the network, which reduces the speed of response to event processing. This problem may be perfectly solved by the recent emergence of Edge computing, which can share the load of the cloud server and reduce the delay. Edge computing has prospects in agricultural applications, such as pest identification, safety traceability of agricultural products, unmanned agricultural machinery, agricultural technology promotion, and intelligent management. The application of the Agricultural Internet of Things integrates artificial intelligence, the Internet of Things, and blockchain and Virtual/Augmented Reality technologies. This paper primarily reviews the application of Edge computing in the Agricultural Internet of Things and investigates the combination of Edge computing and Artificial Intelligence, blockchain and Virtual/Augmented reality technology. The challenges of Edge computing task allocation, data processing, privacy protection and security, and service stability in agriculture are reviewed. The future development direction of Edge computing in the Agricultural Internet of Things is predicted.","['internet of things', 'monitoring', 'artificial intelligence', 'blockchain']"
82,"Wireless sensor networks (WSNs) are currently being used for monitoring and control in smart grids. To ensure the quality of service (QoS) requirements of smart grid applications, WSNs need to provide specific reliability guarantees. Real-time link quality estimation (LQE) is essential for improving the reliability of WSN protocols. However, many state-of-the-art LQE methods produce numerical estimates that are suitable neither for describing the dynamic random features of radio links nor for determining whether the reliability satisfies the requirements of smart grid communication standards. This paper proposes a wavelet-neural-network-based LQE (WNN-LQE) algorithm that closes the gap between the QoS requirements of smart grids and the features of radio links by estimating the probability-guaranteed limits on the packet reception ratio (PRR). In our algorithm, the signal-to-noise ratio (SNR) is used as the link quality metric. The SNR is approximately decomposed into two components: a time-varying nonlinear part and a non-stationary random part. Each component is separately processed before it is input into the WNN model. The probability-guaranteed limits on the SNR are obtained from the WNN-LQE algorithm and are then transformed into estimated limits on the PRR via the mapping function between the SNR and PRR. Comparative experimental results are presented to demonstrate the validity and effectiveness of the proposed LQE algorithm.","['wireless sensor networks', 'reliability', 'estimation', 'quality of service']"
83,"In this paper, a real-life application of bi-level evolutionary optimization is proposed to optimize the electricity industry infrastructure. It offers a coordinated generation and transmission expansion planning (CGTEP) from the perspective of an independent system operator (ISO). The main objective of the proposed study is to show the effect of optimizing the generators concerning capacity and location both to reduce the transmission investment and increasing the reliability of the network. The proposed framework of bi-level optimization contributes to utilize global evolutionary optimization method GA in its hybrid form in level-I to select the location of lines and energy generators. The respective capacities of the corresponding selected lines and generators are optimized in the level-II by RW. In conflicting objectives of minimizing the investment for capacity addition in the network and maximizing the reliability, a Pareto-optimal solution is achieved by using the theory of marginal value (TMV). To satisfy TMV, the total cost is minimized, which comprises the cost of investment in building new transmission and generation capacities, cost of not-served expected energy, cost of unutilized expected generation, and cost of unserved energy due to the constrained network. Proposed methodology on IEEE 24-bus power system is presented encountering the combination of N-1 and probable N-2 contingency security criteria. The comparison results show that bi-level GA-RW optimization minimizes the investment with increasing power system reliability.","['generators', 'planning', 'optimization', 'reliability']"
84,"Cloud manufacturing (CMfg) is a new service-oriented production paradigm from the wide application of cloud computing for the manufacturing industry. The aim of this manufacturing mode is to provide resource-sharing and on-demand manufacturing mode to improve operation efficiency. Resource allocation is considered as a crucial technology to implement CMfg. Traditional resource allocation approaches in CMfg mainly focus on the optimal resource selection process, but the energy consumption for manufacturing resources is rarely considered. In response, this paper proposes a hybrid energy-aware resource allocation approach to help requestors acquire energy-efficient and satisfied manufacturing services. The problem description on energy-aware resource allocation in CMfg is first summarized. Then a local selection strategy based on fuzzy similarity degree is put forth to obtain appropriate candidate services. A multi-objective mathematical model for energy-aware service composition is established and the nondominated sorting genetic algorithm (NSGA-II) is adopted to conduct the combinatorial optimization process. Furthermore, a technique for order preference by similarity to an ideal solution is used to determine the optimal composite services. Finally, a case study is illustrated to validate the effectiveness of the proposed approach.","['cloud computing', 'energy consumption', 'mathematical model', 'optimization']"
85,"To prolong the function of wireless sensor networks (WSNs), the lifetime of the system has to be increased. WSNs lifetime can be calculated by using a few generic parameters, such as the time until the death of the first node and other parameters according to the application. Literature indicates that choosing the most appropriate cluster head by clustering is one of the most successful ways to improve the lifespan of the WSN. The drawback of clustering protocols is based on the probabilistic model. Sometimes they select two cluster heads for two different clusters which are very close to each other and results in head situated at the edge of the cluster in some cases. This type of cluster head selection leads to a reduction in energy efficiency. Therefore, we have proposed the LEACH-Fuzzy Clustering (LEACH-FC) protocol and implemented a fuzzy logic-based cluster head selection and cluster formation to maximize the lifetime of the network. For selections of cluster head and formation of the cluster, we have used a centralized approach instead of distributed ones. We have also employed fuzzy logic in the selection of vice cluster head, which is also a centralized approach. The proposed algorithm has been found to be effective in balancing the energy load at each node thereby enhancing the reliability of WSN. It outperforms other proposed algorithms for improving network lifetime and energy consumption.","['wireless sensor networks', 'reliability', 'protocols', 'energy consumption']"
86,"The classification performance of aerial scenes relies heavily on the discriminative power of feature representation from high-spatial resolution remotely sensed imagery. The convolutional neural networks (CNNs) have recently been applied to adaptively learn image features at different levels of abstraction rather than requiring handcrafted features and achieved state-of-the-art performance. However, most of these networks focus on multi-stage global feature learning yet neglect the local information, which plays an important role in scene recognition. To address this issue, a novel end-to-end global-local attention network (GLANet) is proposed to capture both global and local information for aerial scene classification. FC layers in the VGGNet are replaced by the global attention (GA) branch and local attention (LA) branch, one of which learns the global information while the other learns the local semantic information via attention mechanisms. During each training, the labels of input images can be predicted by the local, global, and their concatenated features using softmax. According to different predicted labels, two auxiliary loss functions are further computed and imposed on the proposed network to enhance the supervision for network learning. The experimental results on three challenging large-scale scene datasets demonstrate the effectiveness of the proposed global-local attention network.","['spatial resolution', 'convolutional neural networks', 'training', 'convolution']"
87,"The increasing number of applications based on the Internet of Things, as well as advances in wireless communication, information and communication technology, and mobile cloud computing, has allowed mobile users to access a wider range of resources when mobile. As the use of vehicular cloud computing has become more popular due to its ability to improve driver and vehicle safety, researchers and industry have a growing interest in the design and development of vehicular networks for emerging applications. Vehicle drivers can now access a variety of on demand resources en route via vehicular network service providers. The adaptation of vehicular cloud services faces many challenges, including cost, privacy, and latency. The contributions of this paper are as follows. First, we propose a game theory-based framework to manage on-demand service provision in a vehicular cloud. We present three different game approaches, each of which helps drivers minimize their service costs and latency, and maximize their privacy. Second, we propose a quality-of-experience framework for service provision in a vehicular cloud for various types of users, a simple but effective model to determine driver preferences. Third, we propose using the trusted third party concept to represent drivers and service providers, and ensure fair game treatment. We develop and evaluate simulations of the proposed approaches under different network scenarios with respect to privacy, service cost, and latency, by varying the vehicle density and driver preferences. The results show that the proposed approach outperforms conventional models, since the game theory system introduces a bounded latency of ≤3%, achieves service cost savings up to 65%, and preserves driver privacy by reducing revealed information by up to 47%.","['internet of things', 'cloud computing', 'privacy', 'wireless communication']"
88,"Deep learning has been applied in physical-layer communications systems in recent years and has demonstrated fascinating results that were comparable or even better than human expert systems. In this paper, a novel convolutional neural networks (CNNs)-based autoencoder communication system is proposed, which can work intelligently with arbitrary block length, can support different throughput and can operate under AWGN and Rayleigh fading channels as well as deviations from AWGN environments. The proposed generalized communication system is comprised of carefully designed convolutional neural layers and, hence, inherits CNN's breakthrough characteristics, such as generalization, feature learning, classification, and fast training convergence. On the other hand, the end-to-end architecture jointly performs the tasks of encoding/decoding and modulation/demodulation. Finally, we provide the numerous simulation results of the learned system in order to illustrate its generalization capability under various system conditions.","['training', 'encoding', 'neural networks', 'modulation', 'convolutional neural network']"
89,"Clustering has been intensively studied in machine learning and data mining communities. Although demonstrating promising performance in various applications, most of the existing clustering algorithms cannot efficiently handle clustering tasks with incomplete features which is common in practical applications. To address this issue, we propose a novel K-means based clustering algorithm which unifies the clustering and imputation into one single objective function. It makes these two processes be negotiable with each other to achieve optimality. Furthermore, we design an alternate optimization algorithm to solve the resultant optimization problem and theoretically prove its convergence. The comprehensive experimental study has been conducted on nine UCI benchmark datasets and real-world applications to evaluate the performance of the proposed algorithm, and the experimental results have clearly demonstrated the effectiveness of our algorithm which outperforms several commonly-used methods for incomplete data clustering.","['clustering algorithms', 'optimization', 'data mining', 'convergence']"
90,"Intra-pulse modulation recognition of radar signals is an important part of modern electronic intelligence reconnaissance and electronic support systems. With the increasing density of radar signals, the analysis and processing of multi-component radar signals have become an urgent problem in the current radar reconnaissance system. In this paper, an intra-pulse modulation recognition approach for single-component and dual-component radar signals is proposed. First, in order to adapt to the time-frequency energy distribution characteristics of various radar signals, we propose to extract the time-frequency images (TFIs) of received signals by Cohen class time-frequency distribution (CTFD) with multiple kernel functions. Besides, the image processing methods are used to suppress noise and adjust the size and amplitude of the TFIs. Second, we design and pre-train a TFI feature extraction network for radar signals based on a convolutional neural network (CNN). Finally, to improve the probability of successful recognition (PSR) of the recognition system in the pulse overlapping environment, a multi-label classification network based on a deep Q-learning network (DQN) is explored. Besides, two sub-networks take TFIs based on special kernel functions as input and re-judge the recognition results of some specific signals to further enhance the recognition effect of the recognition system. The proposed approach can identify 8 kinds of random overlapping radar signals. The simulation results show that the overall PSR of dual-component radar signals and single-component radar signals can reach 94.83% and 94.43%, respectively, when the signal-to-noise ratio (SNR) is -6 dB.","['modulation', 'feature extraction', 'kernel', 'convolutional neural network']"
91,"The prognostics and health management (PHM) plays the main role to handle the risk of failure before its occurrence. Next, it has a broad spectrum of applications including utility networks, energy storage systems (ESS), etc. However, an accurate capacity estimation of batteries in ESS is mandatory for their safe operations and decision making policy. ESS comprises of different storage mechanisms such as batteries, capacitors, etc. Consequently, the measurement of different charging profiles (CPs) has a strong relation to battery capacity. These profiles include temperature (T), voltage (V), and current (I) where the CPs patterns vary as the battery ages with cycles. Consequently, estimating a battery capacity, the conventional methods practice single channel charging profile (SCCP) and hop multiple channel CPs (MCCPs) that cause incorrect battery health estimation. To tackle these issues, this article proposes MCCPs based battery management system (BMS) to estimate batteries health/capacity through the deep learning (DL) concept where the patterns in these CPs are changed as the battery ages with time and cycles. Thus, we deeply investigate both machine learning (ML) and DL based methods to provide a concrete comparative analysis of our method. The adaptive boosting (AB) and support vector regression (SVR) are widely compared with long short-term memory (LSTM), multi-layer perceptron (MLP), bi-directional LSTM (BiLSTM), and convolutional neural network (CNN) to attain the appropriate approach for battery capacity and state of health (SOH) estimation. These approaches have a high learning capability of inter-relation between the battery capacity and variation in CPs patterns. To validate and verify the proposed technique, we use NASA battery dataset and experimentally prove that BiLSTM outperforms all the approaches and obtains the smallest error values for MAE, MSE, RMSE, and MAPE using MCCPs compared to SCCP.","['batteries', 'estimation', 'deep learning', 'machine learning']"
92,"In wireless sensor networks (WSNs), collecting data with mobile sinks is an effective way to solve the “energy hole problem”. However, most of existing algorithms of mobile sinks ignore the load balance of rendezvous nodes, which will significantly shorten the network lifetime. Moreover, most mobile sinks are usually required to visit locations of sensor nodes without taking advantage of their communication ranges. Therefore, this paper proposes an energy-efficient trajectory planning algorithm (EETP) based on multi-objective particle swarm optimization (MOPSO) to shorten the trajectory length of the mobile sink and balance the load of rendezvous nodes. EETP aims to reduce the delay in data delivery and prolong the network lifetime. To shorten the trajectory length of the mobile sink, we design a mechanism to select potential visiting points within communication overlapping ranges of sensor nodes, rather than locations of sensor nodes. Additionally, according to trajectory characteristics of the mobile sink, we design an effective trajectory encoding method that can generate a trajectory containing an unfixed number of visiting points. The simulation results show that the proposed EETP is superior to existing WRP, CB and the MOPSO-based algorithm, in terms of delay in data delivery, network lifetime and energy consumption.","['trajectory', 'planning', 'wireless sensor networks', 'energy consumption']"
93,"The paper discusses a new framework combining the possibilities of Big Data processing and machine leaning developed for security monitoring of mobile Internet of Things. The mathematical foundations and the problem statement are considered. The description of the used data set and the architecture of proposed security monitoring framework are provided. The framework specifies several machine learning mechanisms intended for solving classification tasks. The classifier operation results are exposed to plurality voting, weighted voting, and soft voting. The framework performance and accuracy is assessed experimentally.","['machine learning', 'security', 'monitoring', 'internet of things']"
94,"Security and safety is a big concern for today’s modern world. For a country to be economically strong, it must ensure a safe and secure environment for investors and tourists. Having said that, Closed Circuit Television (CCTV) cameras are being used for surveillance and to monitor activities i.e. robberies but these cameras still require human supervision and intervention. We need a system that can automatically detect these illegal activities. Despite state-of-the-art deep learning algorithms, fast processing hardware, and advanced CCTV cameras, weapon detection in real-time is still a serious challenge. Observing angle differences, occlusions by the carrier of the firearm and persons around it further enhances the difficulty of the challenge. This work focuses on providing a secure place using CCTV footage as a source to detect harmful weapons by applying the state of the art open-source deep learning algorithms. We have implemented binary classification assuming pistol class as the reference class and relevant confusion objects inclusion concept is introduced to reduce false positives and false negatives. No standard dataset was available for real-time scenario so we made our own dataset by making weapon photos from our own camera, manually collected images from internet, extracted data from YouTube CCTV videos, through GitHub repositories, data by university of Granada and Internet Movies Firearms Database (IMFDB) imfdb.org. Two approaches are used i.e. sliding window/classification and region proposal/object detection. Some of the algorithms used are VGG16, Inception-V3, Inception-ResnetV2, SSDMobileNetV1, Faster-RCNN Inception-ResnetV2 (FRIRv2), YOLOv3, and YOLOv4. Precision and recall count the most rather than accuracy when object detection is performed so these entire algorithms were tested in terms of them. Yolov4 stands out best amongst all other algorithms and gave a F1-score of 91% along with a mean average precision of 91.73% higher than previously achieved.","['cameras', 'deep learning', 'security', 'object detection']"
95,"It has been recognized that deeper and wider neural networks are continuously advancing the state-of-the-art performance of various computer vision and machine learning tasks. However, they often require large sets of labeled data for effective training and suffer from extremely high computational complexity, preventing them from being deployed in real-time systems, for example vehicle object detection from vehicle cameras for assisted driving. In this paper, we aim to develop a fast deep neural network for real-time video object detection by exploring the ideas of knowledge-guided training and predicted regions of interest. Specifically, we will develop a new framework for training deep neural networks on datasets with limited labeled samples using cross-network knowledge projection which is able to improve the network performance while reducing the overall computational complexity significantly. A large pre-trained teacher network is used to observe samples from the training data. A projection matrix is learned to project this teacher-level knowledge and its visual representations from an intermediate layer of the teacher network to an intermediate layer of a thinner and faster student network to guide and regulate the training process. To further speed up the network, we propose to train a low-complexity object detection using traditional machine learning methods, such as support vector machine. Using this low-complexity object detector, we identify the regions of interest that contain the target objects with high confidence. We obtain a mathematical formula to estimate the regions of interest to save the computation for each convolution layer. Our experimental results on vehicle detection from videos demonstrated that the proposed method is able to speed up the network by up to 16 times while maintaining the object detection performance.","['training', 'neural networks', 'object detection', 'real-time systems']"
96,"The integration of radar and communications systems can provide great advantages, such as enhanced efficiency, structure simplification, less occupied hardware resources, and interference mitigation, compared with traditional individual radar and communications applications. Extensive studies have presented achieving improved system performance, whereas the problem of low probability of intercept (LPI)-based waveform design for the integrated system is seldom discussed in the literature. In this paper, an LPI-based optimal orthogonal frequency multiplexing modulation (OFDM) waveform design strategy is developed for an integrated radar and communications system (IRCS). The dedicated transmitter in this system transmits integrated OFDM waveform for simultaneously target parameter estimation and downlink communications. The basis of the underlying strategy is to employ the optimization technique to design the integrated OFDM waveform of IRCS in order to minimize the total radiated power, while satisfying the specified requirements of target parameter estimation and data information rate. We analytically show that the resulting optimization problem is convex and can be solved by formulating the Karush-Kuhn-Tuckers optimality conditions. Numerical simulation results demonstrate that our proposed strategy can solve the waveform design problem in the IRCS with low complexity, and the LPI performance of the IRCS can efficiently be enhanced by utilizing the proposed integrated OFDM waveform design strategy.","['radar', 'ofdm', 'interference', 'optimization']"
97,"Microorganisms play a great role in ecosystem, wastewater treatment, monitoring of environmental changes, and decomposition of waste materials. However, some of them are harmful to humans and animals such as tuberculosis bacteria and plasmodium. In such course, it is important to identify, track, analyze, consider the beneficial side and get rid of the negative effects of microorganisms using fast, accurate, and reliable methods. In recent decades, image analysis techniques have been used to address the drawbacks of manual traditional approaches in the identification and analysis of microorganisms. As image segmentation being an important step (technique) in the detection, tracking, monitoring, feature extraction, modeling, and analysis of microorganisms, different methods have been deployed, from classical approaches to current deep neural networks upon different challenges on microorganism images. This survey comprehensively analyses the various studies focused on developing microorganism image segmentation methods in the last 30 years (since 1989). In this survey, segmentation methods are categorized into classical and machine learning methods. Furthermore, these methods are subcategorized into threshold-based, region-based, and edge-based which belong to classical methods, supervised and unsupervised machine leaning-based methods which belong to machine learning category. A growth trend of different methods and most frequently used methods in each category are meticulously analyzed. A clear explanation of the suitability of these methods for different segmentation challenges encountered on microscopic microorganism images is also enlightened.","['image segmentation', 'machine learning', 'feature extraction', 'monitoring']"
98,"Automatic modulation recognition (AMR) plays an important role in various communications systems. It has the ability of adaptive modulation and can adapt to various complex environments. Automatic modulation recognition is also widely used in orthogonal frequency division multiplexing (OFDM) systems. However, because the recognition accuracy of traditional methods to extract the features of OFDM signals is very limited. In order to solve these problems, many deep learning based AMR methods have been proposed to improve the recognition performance. However, most of these AMR methods neglect the harmful effect by carrier phase offset (PO) which often appears in realistic communications systems. Hence it is required to consider the PO effect for designing the OFDM system. Unlike conventional methods, we propose a convolutional neural network (CNN) based AMR method for considering PO in the OFDM system. The proposed method is used to eliminate the PO to achieve the high classification accuracy. Experiment results are provided to confirm the proposed method when comparing to conventional methods.","['modulation', 'ofdm', 'deep learning', 'convolutional neural network']"
99,"The rapid development of information and wireless communication technologies together with the large increase in the number of end-users have made the radio spectrum more crowded than ever. Besides, providing a stable and reliable service is challenging, as electromagnetic environments are evolving and becoming more sophisticated. Accordingly, there is an urgent need for more reliable and intelligent communication systems that can improve the spectrum efficiency and the quality of service to provide agile management of network resources, so as to better meet the needs of future wireless users. Specifically, Automatic Modulation Recognition (AMR) plays an essential role in most intelligent communication systems especially with the emergence of Software Defined Radio (SDR). AMR is an indispensable task while performing spectrum sensing in Cognitive Radio (CR). Thanks to the significant advancements in Deep Learning (DL) applications, new and powerful tools have been provided which can tackle problems in this space. Thus, today, integrating DL models into AMR has gained the attention of many researchers. This work aims to provide a comprehensive state-of-the-art review of the most recent Machine Learning (ML) based AMR methods for Single-Input Single-Output (SISO) and Multiple-Input Multiple-Output (MIMO) systems. Furthermore, the architecture of each model will be identified along with a detailed comparison in terms of specifications and performance. Finally, an outline of the open problems, challenges, and potential research directions is provided along with discussion and conclusion.","['modulation', 'wireless communication', 'deep learning', 'machine learning']"
100,"This paper presents an optimization framework to maximize the lifetime of wireless sensor networks for structural health monitoring with and without energy harvesting. We develop a mathematical model and formulate the problem as a large-scale mixed integer non-linear programming problem. We also propose a solution based on the Branch-and-Bound algorithm augmented with reducing the search space. The proposed strategy builds up the optimal route from each source to the sink node by providing the best set of hops in each route and the optimal power allocation of each sensor node. To reduce the computational complexity, we propose heuristic routing algorithms. In this heuristic algorithm, the power levels are selected from the optimal predefined values, the problem is formulated by an integer non-linear programming, and the Branch-and-Bound reduced space algorithm is used to solve the problem. Moreover, we propose two sub-optimal algorithms to reduce the computation complexity. In the first algorithm, after selecting the optimal transmission power levels from a predefined value, a genetic algorithm is used to solve the integer non-linear problem. In the second sub-optimal algorithm, we solve the problem by decoupling the optimal power allocation scheme from the optimal route selection. Therefore, the problem is formulated by an integer non-linear programming, which is solved using the Branch-and-Bound space-reduced method with reduced binary variables (i.e., reduced complexity), and after the optimum route selection, the optimal power is allocated for each node. The numerical results reveal that the presented algorithm can prolong the network lifetime significantly compared with the existing schemes. Moreover, we mathematically formulate the adaptive energy harvesting period to increase the network lifetime with the possibility to approach infinity. Finally, the minimum harvesting period to have infinite lifetime is obtained.","['wireless sensor networks', 'monitoring', 'routing', 'mathematical model']"
101,"The recent research by deep learning has shown many breakthroughs with high performance that were not achieved with traditional machine learning algorithms. Particularly in the field of object detection, commercial products with high accuracy in the real environment are applied through the deep learning methods. However, the object detection method using the convolutional neural network (CNN) has a disadvantage that a large number of feature maps should be generated in order to be robust against scale change and occlusion of the object. Also, simply raising the number of feature maps does not improve performance. In this paper, we propose to integrate additional prediction layers into conventional Yolo-v3 using spatial pyramid pooling to complement the detection accuracy of the vehicle for large scale changes or being occluded by other objects. Our proposed detector achieves 85.29% mAP, which outperformed than those of the DPM, ACF, R-CNN, CompACT, NANO, EB, GP-FRCNN, SA-FRCNN, Faster-R CNN2, HAVD, and SSD-VDIG on the UA-DETRAC benchmark data-set consisting of challenging real-world-traffic videos.","['object detection', 'convolution', 'deep learning', 'machine learning']"
102,"Cloud computing enables customers with limited computational resources to outsource their huge computation workloads to the cloud with massive computational power. However, in order to utilize this computing paradigm, it presents various challenges that need to be addressed, especially security. As eigen-decomposition (ED) and singular value decomposition (SVD) of a matrix are widely applied in engineering tasks, we are motivated to design secure, correct, and efficient protocols for outsourcing the ED and SVD of a matrix to a malicious cloud in this paper. In order to achieve security, we employ efficient privacy-preserving transformations to protect both the input and output privacy. In order to check the correctness of the result returned from the cloud, an efficient verification algorithm is employed. A computational complexity analysis shows that our protocols are highly efficient. We also introduce an outsourcing principle component analysis as an application of our two proposed protocols.","['cloud computing', 'security', 'protocols', 'privacy']"
103,"Homomorphic encryption (HE) is one of promising cryptographic candidates resolving privacy issues in machine learning on sensitive data such as biomedical data and financial data. However, HE-based solutions commonly suffer from relatively high computational costs due to a large number of iterations in the optimization algorithms such as gradient descent (GD) for the learning phase. In this paper, we propose a new method called ensemble GD for logistic regression, a commonly used machine learning technique for binary classification. Our ensemble method reduces the number of iterations of GD, which results in substantial improvement on the performance of logistic regression based on HE in terms of speed and memory. The convergence of ensemble GD based on HE is guaranteed by our theoretical analysis on the erroneous variant of ensemble GD. We implemented ensemble GD for the logistic regression based on an approximate HE scheme HEAAN on MNIST data set and Credit data set from UCI machine learning repository. Compared to the standard GD for logistic regression, our ensemble method requires only about 60% number of iterations, which results in 60-70% reduction on the running time of total learning procedure in encrypted state, and 30-40% reduction on the storage of encrypted data set.","['machine learning', 'encryption', 'privacy', 'convergence']"
104,"Research in the recognition of human activities of daily living has significantly improved using deep learning techniques. Traditional human activity recognition techniques often use handcrafted features from heuristic processes from single sensing modality. The development of deep learning techniques has addressed most of these problems by the automatic feature extraction from multimodal sensing devices to recognise activities accurately. In this paper, we propose a deep learning multi-channel architecture using a combination of convolutional neural network (CNN) and Bidirectional long short-term memory (BLSTM). The advantage of this model is that the CNN layers perform direct mapping and abstract representation of raw sensor inputs for feature extraction at different resolutions. The BLSTM layer takes full advantage of the forward and backward sequences to improve the extracted features for activity recognition significantly. We evaluate the proposed model on two publicly available datasets. The experimental results show that the proposed model performed considerably better than our baseline models and other models using the same datasets. It also demonstrates the suitability of the proposed model on multimodal sensing devices for enhanced human activity recognition.","['feature extraction', 'convolution', 'deep learning', 'convolutional neural network']"
105,"Deep Neural Networks (DNNs) have been gaining state-of-the-art achievement compared with many traditional Machine Learning (ML) models in diverse fields. However, adversarial examples challenge the further deployment and application of DNNs. Analysis has been carried out for studying the reasons of DNNs' vulnerability to adversarial perturbation and focused on model architecture. No research has been done on investigating the impact of optimization algorithms (namely, optimizers in DNNs) employed in training DNN models on models' sensitivity to adversarial examples. This paper aims to study this impact from an experimental perspective. We analyze the sensitivity of a model not only from the aspect of white-box and black-box attack setups, but also from the aspect of different types of datasets. Four common optimizers, SGD, RMSprop, Adadelta, and Adam, are investigated on structured and unstructured datasets. Extensive experiment results indicate that an optimization algorithm does pose effects on the DNN model sensitivity to adversarial examples. That is, when training models and generating adversarial examples, Adam optimizer can generate better quality adversarial examples for structured datasets, and Adadelta optimizer can generate better quality adversarial examples for unstructured datasets. In addition, the choice of optimizers does not affect the transferability of adversarial examples.","['optimization', 'sensitivity', 'neural networks', 'training', 'machine learning']"
106,"Due to the limited resources and scalability, the security protocols for the Internet of Things (IoT) need to be light-weighted. The cryptographic solutions are not feasible to apply on small and low-energy devices of IoT because of their energy and space limitations. In this paper, a light-weight protocol to secure the data and achieving data provenance is presented for the multi-hop IoT network. The Received Signal Strength Indicator (RSSI) of communicating IoT nodes are used to generate the link fingerprints. The link fingerprints are matched at the server to compute the correlation coefficient. Higher the value of correlation coefficient, higher the percentage of the secured data transfers. Lower value gives the detection of adversarial node in between a specific link. Data provenance has also been achieved by comparison of packet header with all the available link fingerprints at the server. The time complexity is computed at the node and server level, which is O(1). The energy dissipation is calculated for the IoT nodes and overall network. The results show that the energy consumption of the system presented in this paper is 52-53 mJ for each IoT node and 313.626 mJ for the entire network. The RSSI values are taken in real time from MICAz motes and simulations are performed on MATLAB for adversarial node detection, data provenance, and time-complexity. Experimental results show that up to 97% correlation is achieved when no adversarial node is present in the IoT network.","['internet of things', 'correlation', 'protocols', 'security']"
107,"Wireless sensor networks (WSNs) are one of the chief enabling technologies for the Internet of Things. These networks are severely resource-constrained which calls for designing energy-efficient and effective routing techniques. The hierarchical- or clustering-based routing approaches have shown to improve both energy-efficiency and scalability in WSNs. However, when clustering is implemented in mobile WSNs (MWSNs), the mobility of sensor nodes results in high data loss due to possible dis-association of nodes with their cluster heads which negatively affects the data rates and energy consumption. In order to mitigate the impact of node mobility on clustering, we propose two mobility-aware hierarchical clustering algorithms for MWSN based on three-layer clustering hierarchy: mobility-aware centralized clustering algorithm (MCCA) and mobility-aware hybrid clustering algorithm (MHCA). The MCCA algorithm employs centralized gridding at both layers of clustering hierarchy, and the MHCA algorithm employs centralized gridding at the upper layer and distributed clustering at the lower layer. The simulation results show that our proposed algorithms improve network lifetime, reduce energy consumption, stabilize cluster formation, and enhance data rates in mobile sensor networks. We also observe that the centralized clustering approach is superior to the hybrid clustering approach.","['clustering algorithms', 'wireless sensor networks', 'routing', 'internet of things']"
108,"Cyberattacks targeting Internet of Things (IoT), have increased significantly, over the past decade, with the spread of internet-connected smart devices and applications. Routing Protocol for Low-Power and Lossy Network (RPL) enables messages to be routed between nodes for the Wireless Sensor Network in the network layer. RPL protocol, which is sensitive and difficult to protect, is exposed to various attacks. These attacks negatively affect data transmission and cause great destruction to the topology by consuming the resources. Hello Flooding (HF) attacks against RPL cause consumption of constrained resources (memory, processing and energy) in nodes. Therefore, in this study, a Gated Recurrent Unit network model based deep learning has been proposed to predict and prevent HF attacks on RPL protocol in IoT networks. The proposed model has been compared with Support Vector Machine and Logistic Regression methods, and different power states and total energy consumptions of the nodes have been taken into consideration and experimented with. The results confirm the promised and expected performance from the model in terms of source efficiency and IoT security. In addition, attack detection has been carried out with a much lower error rate than literature studies for HF attacks from RPL flood attacks.","['topology', 'energy consumption', 'deep learning', 'internet of things']"
109,"Personalised training of motor and cognitive abilities is fundamental to help older people maintain a good quality of life, especially in case of frailty conditions. However, the training activity can increase the stress level, especially in persons affected by a chronic stress condition. Wearable technologies and m-health solutions can support the person, the medical specialist, and long-term care facilities to efficiently implement personalised therapy solutions by monitoring the stress level of each subject during the motor and cognitive training. In this paper we present a comprehensive work on this topic, starting from a pilot study involving a group of frail older adults suffering from Mild Cognitive Impairment (MCI) who actively participated in cognitive and motor rehabilitation sessions equipped with wearable physiological sensors and a mobile application for physiological monitoring. We analyse the collected data to investigate the stress response of frail older subjects during the therapy, and how the cognitive training is positively affected by physical exercise. Then, we evaluated a stress detection system based on several machine learning algorithms in order to highlight their performances on the real dataset we collected. However, stress detection algorithms generally provide only the identification of a stressful/non stressful event, which is not sufficient to personalise the therapy. Therefore, we propose a mobile system architecture for online stress monitoring able to infer the stress level during a session. The obtained result is then used as input for a Decision Support System (DSS) in order to support the medical user in the definition of a personalised therapy for frail older adults.","['stress', 'training', 'monitoring', 'machine learning']"
110,"Intelligent signal processing for wireless communications is a vital task in modern wireless systems, but it faces new challenges because of network heterogeneity, diverse service requirements, a massive number of connections, and various radio characteristics. Owing to recent advancements in big data and computing technologies, artificial intelligence (AI) has become a useful tool for radio signal processing and has enabled the realization of intelligent radio signal processing. This survey covers four intelligent signal processing topics for the wireless physical layer, including modulation classification, signal detection, beamforming, and channel estimation. In particular, each theme is presented in a dedicated section, starting with the most fundamental principles, followed by a review of up-to-date studies and a summary. To provide the necessary background, we first present a brief overview of AI techniques such as machine learning, deep learning, and federated learning. Finally, we highlight a number of research challenges and future directions in the area of intelligent radio signal processing. We expect this survey to be a good source of information for anyone interested in intelligent radio signal processing, and the perspectives we provide therein will stimulate many more novel ideas and contributions in the future.","['artificial intelligence', 'wireless communication', 'modulation', 'deep learning', 'machine learning']"
111,"In the task of using deep learning semantic segmentation model to extract water from high-resolution remote sensing images, multiscale feature sensing and extraction have become critical factors that affect the accuracy of image classification tasks. A single-scale training mode will cause one-sided extraction results, which can lead to “reverse” errors and imprecise detail expression. Therefore, fusing multiscale features for pixel-level classification is the key to achieving accurate image segmentation. Based on this concept, this paper proposes a deep learning scheme to achieve fine extraction of image water bodies. The process includes multiscale feature perception splitting of images, a restructured deep learning network model, multiscale joint prediction, and postprocessing optimization performed by a fully connected conditional random field (CRF). According to the scale space concept of remote sensing, we apply hierarchical multiscale splitting processing to images. Then, we improve the structure of the image semantic segmentation model DeepLabV3+, an advanced image semantic segmentation model, and adjust the feature output layer of the model to multiscale features after weighted fusion. At the back end of the deep learning model, the water boundary details are optimized with the fully connected CRF. The proposed multiscale training method is well adapted to feature extraction for the different scale images in the model. In the multiscale output fusion, assigning different weights to the output features of each scale controls the influence of the various scale features on the water extraction results. We carried out a large number of water extraction experiments on GF1 remote sensing images. The results show that the method significantly improves the accuracy of water extraction and demonstrates the effectiveness of the method.","['feature extraction', 'image segmentation', 'remote sensing', 'deep learning']"
112,"The smart campus can monitor students in real time by analyzing students' images, but a large number of images bring an unbearable burden to the smart campus. The convenience of cloud computing has attracted smart campus to outsource their huge amount of data to cloud servers. Although the outsourcing of data can reduce the computational and storage burden on smart campus, the privacy preserving becomes the biggest concern. This issue has attracted many researchers to study the protection of outsourced multimedia data. In this paper, we propose an effective and practical privacy-preserving computation outsourcing protocol for the local binary pattern (LBP) feature over huge encrypted images. The image owner uploads the encrypted version of images to the cloud. The cloud server takes the responsibility of extracting the LBP features from encrypted images for various applications. In the encryption process, an image is divided into non-overlapping blocks at first, and the blocks are shuffled to protect the image content. Next, all the non-center pixels in each block are shuffled. Finally, the pixels are encrypted by splitting the original image data randomly. When such an encrypted image is received, the cloud servers can calculate the LBP features by secure multiparty computation. The extracted features can be applied to many applications, such as texture classification, image retrieval, face recognition, and so on.","['encryption', 'servers', 'cloud computing', 'privacy']"
113,"The Internet of Things (IoT) has emerged as a technology capable of connecting heterogeneous nodes/objects, such as people, devices, infrastructure, and makes our daily lives simpler, safer, and fruitful. Being part of a large network of heterogeneous devices, these nodes are typically resource-constrained and became the weakest link to the cyber attacker. Classical encryption techniques have been employed to ensure the data security of the IoT network. However, high-level encryption techniques cannot be employed in IoT devices due to the limitation of resources. In addition, node security is still a challenge for network engineers. Thus, we need to explore a complete solution for IoT networks that can ensure nodes and data security. The rule-based approaches and shallow and deep machine learning algorithms- branches of Artificial Intelligence (AI)- can be employed as countermeasures along with the existing network security protocols. This paper presented a comprehensive layer-wise survey on IoT security threats, and the AI-based security models to impede security threats. Finally, open challenges and future research directions are addressed for the safeguard of the IoT network.","['security', 'internet of things', 'encryption', 'artificial intelligence', 'protocols']"
114,"Demanded by high-performance wireless (WirelessHP) networks for industrial control applications, channel coding should be used and optimized. However, the adopted coding schemes in modern wireless communication standards are not sufficient for WirelessHP applications, in terms of both low latency and high reliability. Starting from the essential characteristics of WirelessHP regarding channel coding, this paper gives a detailed analysis of currently used short packet coding schemes in industrial wireless sensor networks, including seven coding schemes and their possible variants. The metrics employed for evaluation are bit-error rate, packet error rate, and throughput. To find suitable coding schemes from a large number of options, we propose four principles to filter the most promising coding schemes. Based on overall comparison from the perspective of practical implementation, challenges of the available coding schemes are analyzed, and directions are recommended for future research. Some reflections on how to construct specially designed coding schemes for short packets to meet the high reliability and low-latency constraints of WirelessHP are also provided.","['wireless communication', 'wireless sensor networks', 'standards', 'reliability']"
115,"The Internet of Things (IoT) aims to change many aspects of people's daily lives by extending the scope of computing to the physical world, and thus shift the environment of computing more to a distributed and decentralized form. The amount of IoT devices and their collaborative behavior causes new challenges to the scalability of traditional software testing, and the heterogeneity of IoT devices increases costs and the complexity of coordination of testing due to the number of variables. In this paper, we introduce IoT Testing as a Service-IoT-TaaS, a novel service-based approach for an automated IoT testing framework aims to resolve constraints regarding coordination, costs, and scalability issues of traditional software testing in the context of standards-based development of IoT devices, and explore its design and implementation. IoT-TaaS is composed of remote distributed interoperability testing, scalable automated conformance testing, and semantics validation testing components adequate for testing IoT devices. To provide a conceptual overview, we analyze its technical and systemic advancement and compare it to traditional testing with concrete examples.","['semantics', 'standards', 'software', 'internet of things']"
116,"Large-scale deployment of renewable energy sources in power systems is basically motivated by two universally recognized challenges: the need to reduce as far as possible the environmental impact of the massive increase of energy request and the dependency on fossil-fuel. Renewable energy sources are interfaced to the network by means of interfacing power converters which inherently exhibit zero inertia differently from the conventional synchronous generators. This matter jointly to the high level of time variability of the renewable resources involve dramatically frequency changes, recurrent frequency oscillations and high variability of frequency profile in general. The need of a fast estimation of time variability of the power system inertia arises at the aim of predicting critical conditions. Based on the analysis of some actual data of the Italian Transmission Network, in this paper the authors propose an autoregressive model which is able to describe the dynamic evolution of the power system inertia. More specifically, the inertia is modeled as the sum of a periodic component and a noise stochastic process distributed according a non-Gaussian model. The numerical results reported in the last part of the paper, demonstrating the efficiency and precision of estimation of inertia, allow justifying the assumptions of the above modeling.","['estimation', 'generators', 'power systems', 'renewable energy sources']"
117,"Convolutional neural networks (CNNs) have recently attracted considerable attention due to their outstanding accuracy in applications, such as image recognition and natural language processing. While one advantage of the CNNs over other types of neural networks is their reduced computational cost, faster execution is still desired for both training and inference. Since convolution operations pose most of the execution time, multiple algorithms were and are being developed with the aim of accelerating this type of operations. However, due to the wide range of convolution parameter configurations used in the CNNs and the possible data type representations, it is not straightforward to assess in advance which of the available algorithms will be the best performing in each particular case. In this paper, we present a performance evaluation of the convolution algorithms provided by the cuDNN, the library used by most deep learning frameworks for their GPU operations. In our analysis, we leverage the convolution parameter configurations from widely used the CNNs and discuss which algorithms are better suited depending on the convolution parameters for both 32 and 16-bit floating-point (FP) data representations. Our results show that the filter size and the number of inputs are the most significant parameters when selecting a GPU convolution algorithm for 32-bit FP data. For 16-bit FP, leveraging specialized arithmetic units (NVIDIA Tensor Cores) is key to obtain the best performance.","['convolution', 'neural networks', 'training', 'performance evaluation', 'deep learning']"
118,"Wireless transceivers for mass-market applications must be cost effective. We may achieve this goal by deploying non-ideal low-cost radio frequency (RF) analog components. However, their imperfections may result in RF impairments, including phase noise (PN), carrier frequency offset (CFO), and in-phase (I) and quadrature-phase (Q) imbalance. These impairments introduce in-band and out-of-band interference terms and degrade the performance of wireless systems. In this survey, we present RF-impairment signal models and discuss their impacts. Moreover, we review RF-impairment estimation and compensation in single-carrier (SC) and multicarrier systems, especially orthogonal frequency division multiplexing (OFDM). Furthermore, we discuss the effects of the RF impairments in already-established wireless technologies, e.g., multiple-input multiple-output (MIMO), massive MIMO, full-duplex, and millimeter-wave communications and review existing estimation and compensation algorithms. Finally, future research directions investigate the RF impairments in emerging technologies, including cell-free massive MIMO communications, non-orthogonal multicarrier systems, non-orthogonal multiple access (NOMA), ambient backscatter communications, and intelligent reflecting surface (IRS)-assisted communications. Furthermore, we discuss artificial intelligence (AI) approaches for developing estimation and compensation algorithms for RF impairments.","['radio frequency', 'ofdm', 'estimation', 'interference']"
119,"Wireless Sensor Networks (WSNs) have been widely used as the communication system in the Internet of Things (IoT). In addition to the services provided by WSNs, many IoT-based applications require reliable data delivery over unstable wireless links. To guarantee reliable data delivery, existing works exploit geographic opportunistic routing with multiple candidate forwarders in WSNs. However, these approaches suffer from serious Denial of Service (DoS) attacks, where a large number of invalid data are deliberately delivered to receivers to disrupt the normal operations of WSNs. In this paper, we propose a selective authentication-based geographic opportunistic routing (SelGOR) to defend against the DoS attacks, meeting the requirements of authenticity and reliability in WSNs. By analyzing statistic state information (SSI) of wireless links, SelGOR leverages an SSI-based trust model to improve the efficiency of data delivery. Unlike previous opportunistic routing protocols, SelGOR ensures data integrity by developing an entropy-based selective authentication algorithm, and is able to isolate DoS attackers and reduce the computational cost. Specifically, we design a distributed cooperative verification scheme to accelerate the isolation of attackers. This scheme also makes SelGOR avoid duplicate data transmission and redundant signature verification resulting from opportunistic routing. The extensive simulations show that SelGOR provides reliable and authentic data delivery, while it only consumes 50% of the computational cost compared to other related solutions.","['routing', 'wireless sensor networks', 'reliability', 'internet of things']"
120,"Wireless sensors are an important component to develop the Internet of Things (IoT) Sensing infrastructure. There are enormous numbers of sensors connected with each other to form a network (well known as wireless sensor networks) to complete the IoT Infrastructure. These deployed wireless sensors are with limited energy and processing capabilities. The IoT infrastructure becomes a key factor to building cyber-physical-social networking infrastructure, where all these sensing devices transmit data toward the cloud data center. Data routing toward cloud data center using such low power sensor is still a challenging task. In order to prolong the lifetime of the IoT sensing infrastructure and building scalable cyber infrastructure, there is the requirement of sensing optimization and energy efficient data routing. Toward addressing these issues of IoT sensing, this paper proposes a novel rendezvous data routing protocol for low-power sensors. The proposed method divides the sensing area into a number of clusters to lessen the energy consumption with data accumulation and aggregation. As a result, there will be less amount of data stream to the network. Another major reason to select cluster-based data routing is to reduce the control overhead. Finally, the simulation of the proposed method is done in the Castalia simulator to observe the performance. It has been concluded that the proposed method is energy efficient and it prolongs the networks lifetime for scalable IoT infrastructure.","['sensors', 'wireless sensor networks', 'routing', 'energy consumption', 'internet of things']"
121,"Internet of Things (IoT) widely use analysis of data with artificial intelligence (AI) techniques in order to learn from user actions, support decisions, track relevant aspects of the user, and notify certain events when appropriate. However, most AI techniques are based on mathematical models that are difficult to understand by the general public, so most people use AI-based technology as a black box that they eventually start to trust based on their personal experience. This article proposes to go a step forward in the use of AI in IoT, and proposes a novel approach within the Human-centric AI field for generating explanations about the knowledge learned by a neural network (in particular a multilayer perceptron) from IoT environments. More concretely, this work proposes two techniques based on the analysis of artificial neuron weights, and another technique aimed at explaining each estimation based on the analysis of training cases. This approach has been illustrated in the context of a smart IoT kitchen that detects the user depression based on the food used for each meal, using a simulator for this purpose. The results revealed that most auto-generated explanations made sense in this context (i.e. 97.0%), and the execution times were low (i.e. 1.5 ms or lower) even considering the common configurations varying independently the number of neurons per hidden layer (up to 20), the number of hidden layers (up to 20) and the number of training cases (up to 4,000).","['artificial intelligence', 'training', 'estimation', 'internet of things']"
122,"The great advancements in communication, cloud computing, and the internet of things (IoT) have opened critical challenges in security. With these developments, cyberattacks are also rapidly growing since the current security mechanisms do not provide efficient solutions. Recently, various artificial intelligence (AI) based solutions have been proposed for different security applications, including intrusion detection. In this paper, we propose an efficient AI-based mechanism for intrusion detection systems (IDS) in IoT systems. We leverage the advancements of deep learnings and metaheuristics (MH) algorithms that approved their efficiency in solving complex engineering problems. We propose a feature extraction method using the convolutional neural networks (CNNs) to extract relevant features. Also, we develop a new feature selection method using a new variant of the transient search optimization (TSO) algorithm, called TSODE, using the operators of differential evolution (DE) algorithm. The proposed TSODE uses the DE to improve the process of balancing between exploitation and exploration phases. Furthermore, we use three public datasets, KDDCup-99, NSL-KDD, BoT-IoT, and CICIDS-2017 to assess the performance of the developed method, which achieved higher accuracy compared to several existing approaches.","['feature extraction', 'deep learning', 'convolutional neural networks', 'cloud computing', 'internet of things', 'security']"
123,"The Internet of Things is made of diverse networked things (i.e., smart, intelligent devices) that are consistently interconnected, producing meaningful data across the network without human interaction. Nowadays, the Healthcare system is widely interconnected with IoT environments to facilitate the best possible patient monitoring, efficient diagnosis, and timely operate with existing diseases towards the patients. Concerning the security and privacy of the patient's information. This paper is focused on Secure surveillance mechanism for a medical healthcare system with enabled internet of Things (sensors) by intelligently recorded video summary into the server and keyframes image encryption. This paper is twofold. Firstly, a well-organized keyframe extraction mechanism is called to extract meaningful image frames (detected normal/abnormal activities keyframe) by the visual sensor with an alert sent to the concerned authority in the healthcare system. Secondly, the final decision about the happened activity with extracted keyframes to keep highly secure from any adversary, and we propose an efficient probabilistic and lightweight encryption algorithm. Our proposed mechanism verifies effectiveness through producing results, robustness in nature, minimum execution time, and comparatively secure than other images (keyframes) encryption algorithms. Additionally, this mechanism can reduce storage, bandwidth, required transmission cost, and timely analysis of happened activity from any adversary with protecting the privacy of the patient's information in the IoT enabled healthcare system.","['encryption', 'internet of things', 'privacy', 'security']"
124,"One of the most challenging predictive data analysis efforts is an accurate prediction of depth of anesthesia (DOA) indicators which has attracted growing attention since it provides patients a safe surgical environment in case of secondary damage caused by intraoperative awareness or brain injury. However, many researchers put heavily handcraft feature extraction or carefully tailored feature engineering to each patient to achieve very high sensitivity and low false prediction rate for a particular dataset. This limits the benefit of the proposed approaches if a different dataset is used. Recently, representations learned using the deep convolutional neural network (CNN) for object recognition are becoming a widely used model of the processing hierarchy in the human visual system. The correspondence between models and brain signals that holds the acquired activity at high temporal resolution has been explored less exhaustively. In this paper, deep learning CNN with a range of different architectures is designed for identifying related activities from raw electroencephalography (EEG). Specifically, an improved short-time Fourier transform is used to stand for the time-frequency information after extracting the spectral images of the original EEG as input to CNN. Then CNN models are designed and trained to predict the DOA levels from EEG spectrum without handcrafted features, which presents an intuitive mapping process with high efficiency and reliability. As a result, the best trained CNN model achieved an accuracy of 93.50%, interpreted as CNN's deep learning to approximate the DOA by senior anesthesiologists, which highlights the potential of deep CNN combined with advanced visualization techniques for EEG-based brain mapping.","['electroencephalography', 'feature extraction', 'deep learning', 'convolutional neural network']"
125,"Deep learning has achieved exciting results in face recognition; however, the accuracy is still unsatisfying for occluded faces. To improve the robustness for occluded faces, this paper proposes a novel deep dictionary representation-based classification scheme, where a convolutional neural network is employed as the feature extractor and followed by a dictionary to linearly code the extracted deep features. The dictionary is composed by a gallery part consisting of the deep features of the training samples and an auxiliary part consisting of the mapping vectors acquired from the subjects either inside or outside the training set and associated with the occlusion patterns of the testing face samples. A squared Euclidean norm is used to regularize the coding coefficients. The proposed scheme is computationally efficient and is robust to large contiguous occlusion. In addition, the proposed scheme is generic for both the occluded and non-occluded face images and works with a single training sample per subject. The extensive experimental evaluations demonstrate the superior performance of the proposed approach over other state-of-the-art algorithms.","['training', 'testing', 'convolutional neural network', 'deep learning']"
126,"Today’s electrical power system became more complex interconnected network that is expanding every day. The transmission lines of the power system are more severely loaded than ever before. Hence, the power system is facing many problems such as power losses increasing, voltage instability, line overloads, etc. The optimization of real and reactive powers due to the installation of energy resources at appropriate buses can minimize the losses and improve the voltage profile especially, for congested networks. As a result, the optimal power flow problem (OPF) is considered more important tool for the processes of planning and operation of power systems. OPF is a very significant tool for power system operators to meet the electricity demand of the consumers efficiently, and for the reliable operation of the power system. However, the incorporation of renewable energy sources (RESs) into the electrical grid is a very challenging problem due to their intermittent nature. In this paper, the proposed power flow model contains three different types of energy sources: thermal power generators representing the conventional energy sources, wind power generators (WPGs), and solar photovoltaic generators (SPGs) representing RESs. Uncertain output powers from WPGs and SPGs are forecasted with the aid of Weibull and lognormal probability distribution functions (PDF), respectively. The under and overestimation output powers of RESs are taken into consideration while formulating the objective function through adding a penalty and reserve cost, respectively. Moreover, carbon tax is imposed to the main objective function to help in reducing carbon emissions. A jellyfish search optimizer (JS) is employed to reach optimization in the modified IEEE 30-bus test system to validate its feasibility. To examine the effectiveness of the proposed JS algorithm, its simulation results are compared with the results of four other nature-inspired global optimization algorithms. The developed OPF algorithm considers several practical cases such as generation uncertainty of renewable energy sources, time-varying load and the ramp rate limits of thermal generators. The simulation results show the effectiveness of the JS algorithm in solving the OPF problem in terms of minimization of total generation cost and solution convergence.","['generators', 'optimization', 'renewable energy sources', 'uncertainty']"
127,"Prolonging the network lifetime is one of the vital challenges in wireless sensor networks (WSNs). Typically, the lifespan of WSNs can be increased by a technique called clustering, which plays a significant role in simplifying intra-domain routing. The clustering method accounts only a small number of nodes, which are randomly selected as cluster heads (CHs). The main responsibility of CHs is to receive collected data or information from its member nodes and to aggregate the received data and convey the received data to the sink (Sk) or base station (BSn). In this paper, we have proposed a method namely “reliability-based enhanced technique for the ordering of preference by similarity ideal solution (RE-TOPSIS)” combining with fuzzy logic which uses multi-criteria decision making (MCDM) approach aiding in the effective and reliable selection of CHs. It also uses the conventional LEACH protocol to enable one-time CH selection or scheduling in each cluster based on RE-TOPSIS rank index value. This process completely eliminates the need of CH selection process in each round of LEACH's setup state cycle. We have accounted for various criteria such as 1) residual energy; 2) distances between adjacent nodes; 3) energy utilization rates; 4) availability of neighboring nodes; 5) distances between the sink and CHs as well as distances between CHs to member nodes; and 6) the reliability index for completely devising the new scheme. The simulations are accomplished to assess or suggest the performances of the proposed RE-TOPSIS and to compare its performances with the performances of the existing protocols. The results show that the proposed scheme enhances the network lifespan, conserves energy, and introduces a considerable reduction in the frequency of CH selection per cycle by about 20%-25% as compared with the contemporary fuzzy-TOPSIS and LEACH protocols and finally the metrics of the proposed RE-TOPSIS are highlighted.","['wireless sensor networks', 'protocols', 'reliability', 'routing']"
128,"Cybersecurity is important today because of the increasing growth of the Internet of Things (IoT), which has resulted in a variety of attacks on computer systems and networks. Cyber security has become an increasingly difficult issue to manage as various IoT devices and services grow. Malicious traffic identification using deep learning techniques has emerged as a key component of network intrusion detection systems (IDS). Deep learning methods have been a research focus in network intrusion detection. A Recurrent Neural Network (RNN) is useful in a wide range of applications. First, this paper proposes a novel deep learning model for anomaly detection in IoT networks using a recurrent neural network. Long Short Term Memory (LSTM), BiLSTM, and Gated Recurrent Unit (GRU) techniques are used to implement the proposed model for anomaly detection in IoT networks. A Convolutional Neural Network (CNN) can analyze input features without losing important information, making them particularly well suited for feature learning. Next, a hybrid deep learning model was proposed using convolutional and recurrent neural networks. Finally, a lightweight deep learning model for binary classification was proposed using LSTM, BiLSTM, and GRU based approaches. The proposed deep learning models are validated using NSLKDD, BoT-IoT, IoT-NI, IoT-23, MQTT, MQTTset, and IoT-DS2 datasets. Compared to current deep learning implementations, the proposed multiclass and binary classification model achieved high accuracy, precision, recall, and F1 score.","['internet of things', 'security', 'deep learning', 'convolutional neural network']"
129,"The traditional security risk monitoring technology cannot adapt to cyber-physical power systems (CPPS) concerning evaluation criteria, real-time monitoring, and technical reliability. The aim of this paper is to propose and implement a log analysis architecture for CPPS to detect the log anomalies, which introduces the distributed streaming processing mechanism. The processing mechanism can train the network protocol feature database precisely over the big data platform, which improves the efficiency of the network in terms of log anomaly detection. Moreover, we propose an ensemble prediction algorithm based on time series (EPABT) considering the characteristics of the statistical log analysis to predict abnormal features during the network traffic analysis. We then present a new asymmetric error cost (AEC) evaluation criterion to meet the characteristics of CPPS. The experimental results demonstrate that the EPABT provides an efficient tool for detecting the accuracy and reliability of abnormal situation prediction as compared with the several state-of-the-art algorithms. Meanwhile, the AEC can effectively evaluate the differences in the cost between the high and low prediction results. To the best of our knowledge, these two algorithms provide strong support for the practical application of power industrial network security risk monitoring.","['monitoring', 'security', 'power systems', 'reliability']"
130,"Traditional machine learning approaches to drug sensitivity prediction assume that training data and test data must be in the same feature space and have the same underlying distribution. However, in real-world applications, this assumption does not hold. For example, we sometimes have limited training data for the task of drug sensitivity prediction in multiple myeloma patients (target task), but we have sufficient auxiliary data for the task of drug sensitivity prediction in patients with another cancer type (related task), where the auxiliary data for the related task are in a different feature space or have a different distribution. In such cases, transfer learning, if applied correctly, would improve the performance of prediction algorithms on the test data of the target task via leveraging the auxiliary data from the related task. In this paper, we present two transfer learning approaches that combine the auxiliary data from the related task with the training data of the target task to improve the prediction performance on the test data of the target task. We evaluate the performance of our transfer learning approaches exploiting three auxiliary data sets and compare them against baseline approaches using the area under the receiver operating characteristic curve on the test data of the target task. Experimental results demonstrate the good performance of our approaches and their superiority over the baseline approaches when auxiliary data are incorporated.","['prediction algorithms', 'sensitivity', 'training', 'machine learning']"
131,"Electroencephalography (EEG) is a common and significant tool for aiding in the diagnosis of epilepsy and studying the human brain electrical activity. Previously, the traditional machine learning (ML)-based classifier are used to identify the seizure by extracting features from the EEG signals manually. Although the effectiveness of these contributions have already been proved, they cannot achieve multiple class classification with automatic feature extraction. Meanwhile, the identifiable EEG segment is too long to limit the capability of real-time epileptic seizure detection. In this paper, a novel deep convolutional long short-term memory (C-LSTM) model is proposed for detecting seizure and tumor in human brain and identifying two eyes statuses (open and close). It achieves to predict a result in every 0.006 seconds with a short detection duration (one second). By comparing with other two types deep learning approaches (DCNN and LSTM), the presented deep C-LSTM obtains the best performance for classifying these five classes. All of the obtained total accuracy are over 98.80%.","['electroencephalography', 'machine learning', 'feature extraction', 'deep learning']"
132,"The Internet of Things (IoT) aims to transform everyday physical objects into an interconnected ecosystem with digital data accessible anywhere and anytime. “Things” in IoT are embedded with sensing, processing, and actuating capabilities and cooperate in providing smart and innovative services autonomously. The rapid spread of IoT services arises different security vulnerabilities that need to be carefully addressed. Several emerging and promising technologies and techniques are introduced to improve the security of IoT. This paper aims to provide an up-to-date vision of the current research topics related to IoT security. Initially, we introduce common elements and protocols of IoT to demystify the origins of threats in IoT. Then, we propose a taxonomy of IoT attacks and analyze the security vulnerabilities of IoT at different layers. Subsequently, we provide a comparison of recent security schemes based on emerging solutions including fog computing, edge computing, software-defined networking (SDN), blockchain, lightweight cryptography, homomorphic and searchable encryption, and machine learning. Finally, security challenges are discussed and future directions are highlighted for future interested researchers.","['security', 'internet of things', 'protocols', 'machine learning', 'blockchain']"
133,"Multi-source remote sensing imagery has become widely accessible owing to the development of data acquisition systems. In this paper, we address the challenging task of the semantic segmentation of buildings via multi-source remote sensing imagery with different spatial resolutions. Unlike previous works that mainly focused on optimizing the segmentation model, which did not enable the severe problems caused by the unaligned resolution between the training and testing data to be fundamentally solved, we propose to integrate SR techniques with the existing framework to enhance the segmentation performance. The feasibility of the proposed method was evaluated by utilizing representative multi-source study materials: high-resolution (HR) aerial and low-resolution (LR) panchromatic satellite imagery as the training and testing data, respectively. Instead of directly conducting building segmentation from the LR imagery by using the model trained using the HR imagery, the deep learning-based super-resolution (SR) model was first adopted to super-resolved LR imagery into SR space, which could mitigate the influence of the difference in resolution between the training and testing data. The experimental results obtained from the test area in Tokyo, Japan, demonstrate that the proposed SR-integrated method significantly outperforms that without SR, improving the Jaccard index and kappa by approximately 19.01% and 19.10%, respectively. The results confirmed that the proposed method is a viable tool for building semantic segmentation, especially when the resolution is unaligned.","['buildings', 'remote sensing', 'spatial resolution', 'training', 'deep learning']"
134,"Recently, IoT (Internet of Things) has been an attractive area of research to develop smart home, smart city environment. IoT sensors generate data stream continuously and majority of the IoT based applications are highly delay sensitive. The initially used cloud based IoT services suffers from higher delay and lack of efficient resources utilization. Fog computing is introduced to improve these problems by bringing cloud services near to edge in small scale and distributed nature. This work considers an integrated fog-cloud environment to minimize resource cost and reduce delay to support real-time applications at a lower operational cost. We first present a cooperative three-layer fog-cloud computing environment, and propose a novel optimization model in this environment. This model has a composite objective function to minimize the bandwidth cost and provide load balancing. We consider balancing load in both links' bandwidth and servers' CPU processing capacity level. Simulation results show that our framework can minimize the bandwidth cost and balance the load by utilizing the cooperative environment effectively. We assign weight factors to each objective of the composite objective function to set the level of priority. When minimizing bandwidth cost gets higher priority, at first, the demand generated from the traffic generator sensors continues to be satisfied by the regional capacity of layer-1 fog. If the demand of a region goes beyond the capacity of that region, remaining demand is served by other regions layer-1 fog, then by layer-2 fog, and finally by the cloud. However, when load balancing is the priority, the demand is distributed among these resources to reduce delay. Link level load balancing can reduce the queueing delay of links while server level load balancing can decrease processing delay of servers in an overloaded situation. We further analyzed how the unit bandwidth cost, the average and maximum link utilization, the servers' resources utilization, and the average number of servers used vary with different levels of priority on different objectives. As a result, our optimization formulation allows tradeoff analysis in the cooperative three-layer fog-cloud computing environment.","['servers', 'bandwidth', 'internet of things', 'cloud computing']"
135,"Fog computing (FC) is the extension of Cloud Computing (CC), from the core of the internet architecture to the edge of the network, with the aim to perform processes closer to end-users. This extension is proven to enhance security, and to reduce latency and energy consumption. Blockchain (BC), on the other hand, is the base technology behind crypto-currencies, yet is implemented in wide range of different applications. The security and reliability, along with the distributed trust management criteria proposed in BC, excited the research community to integrate it with FC, in a step towards reaching a distributed and trusted, Data, Payment, Reputation, and Identity management systems. In this survey we present the up-to-date state-of-the-art of FC-BC integration with a detailed literature review and classification. We discuss and categorize the related papers according to the year of publication, domain, used algorithms, BC roles, and the placement of the BC in the FC architecture. Our research presents detailed observations, analysis, and open challenges for the BC-FC integration. We believe such conclusions may clarify the vision of the BC-FC integration, and calibrate the compass towards open issues and future research directions.","['cloud computing', 'security', 'blockchain', 'reliability']"
136,"Birdwatching is a common hobby but to identify their species requires the assistance of bird books. To provide birdwatchers a handy tool to admire the beauty of birds, we developed a deep learning platform to assist users in recognizing 27 species of birds endemic to Taiwan using a mobile app named the Internet of Birds (IoB). Bird images were learned by a convolutional neural network (CNN) to localize prominent features in the images. First, we established and generated a bounded region of interest to refine the shapes and colors of the object granularities and subsequently balanced the distribution of bird species. Then, a skip connection method was used to linearly combine the outputs of the previous and current layers to improve feature extraction. Finally, we applied the softmax function to obtain a probability distribution of bird features. The learned parameters of bird features were used to identify pictures uploaded by mobile users. The proposed CNN model with skip connections achieved higher accuracy of 99.00 % compared with the 93.98% from a CNN and 89.00% from the SVM for the training images. As for the test dataset, the average sensitivity, specificity, and accuracy were 93.79%, 96.11%, and 95.37%, respectively.","['feature extraction', 'deep learning', 'training', 'convolutional neural network']"
137,"Automatic modulation recognition (AMR) plays an important role in cognitive radio (CR), which relies on AMR responding to changes in the surrounding environment and then adjust strategies simultaneously. Deep learning based reliable AMR method have been developed in recent years. However, all of their AMR training models are considered in a specialized channel rather than generalized channel. Hence, these AMR methods are hard to be applied in general scenarios. In this paper, we propose a blind channel identification (BCI) aided generalized AMR (GenAMR) method based on deep learning which is conducted by two independent convolutional neural networks (CNNs). The first CNN is trained on in-phase and quadrature (IQ) sampling signals, which is utilized to distinguish channel categories like BCI behaviors. The second CNN is trained by line of sight (LOS) model and non-line of sight (NLOS) model, respectively. Simulation results confirm that our proposed generalized AMR method is significantly better than conventional method.","['modulation', 'convolution', 'deep learning', 'training']"
138,"Eye tracking is becoming a very important tool across many domains, including human-computer-interaction, psychology, computer vision, and medical diagnosis. Different methods have been used to tackle eye tracking, however, some of them are inaccurate under real-world conditions, while some require explicit user calibration which can be burdensome. Some of these methods suffer from poor image quality and variable light conditions. The recent success and prevalence of deep learning have greatly improved the performance of eye-tracking. The availability of large-scale datasets has further improved the performance of deep learning-based methods. This article presents a survey of the current state-of-the-art on deep learning-based gaze estimation techniques, with a focus on Convolutional Neural Networks (CNN). This article also provides a survey on other machine learning-based gaze estimation techniques. This study aims to empower the research community with valuable and useful insights that can enhance the design and development of improved and efficient deep learning-based eye-tracking models. This study also provides information on various pre-trained models, network architectures, and open-source datasets that are useful for training deep learning models.","['estimation', 'machine learning', 'convolutional neural network', 'deep learning']"
139,"Drones have become prevalent for the delivery of goods by many retail companies such as Amazon and Dominos. Amazon has an issued patent that describes how drones scan and collect data on their fly-overs while dropping off packages. In this context, we propose a path optimization algorithm for a drone multi-hop communications network that can carry and forward data in addition to its primary function of parcel deliveries. We argue that traditional Delay Tolerant Networking (DTN) based protocols may not be efficient for this purpose. Therefore, this paper proposes a new DTN-based algorithm that optimizes drone flight paths in conjunction with optimized routing to deliver both parcels and data in a power efficient way and within the shortest possible time. We propose a heuristic algorithm called Weighted Flight Path Planning (WFPP) that priorities the data packets in an exchange pool in order to create an optimized path for the drones. Our approach is to determine a weight for each packet based on the packet's remaining time to live, priority, size, and location of the packet's destination. When two drones meet each other, they exchange the high weighted packets. Simulation studies show that WFPP delivers up to 25% more packets compared with EBR, EPIDEMIC, and a similar path planning method. Also, WFPP reduces the data delivery delays by up to 66% while the overhead ratio is low.","['planning', 'protocols', 'routing', 'delays']"
140,"Quickly and efficiently transmitting data to sink via intelligent routing is an important issue in wireless sensor networks. In previous scenarios, there has existed the phenomenon of “energy hole,”which results in difficulties in synchronous optimization of energy and delay. Thus, a smart High-Speed Backbone Path (HSBP) construction approach is proposed in this paper. In the HSBP approach, several High-speed Backbone Paths (HBPs) are established at different locations of the network, and the duty cycles of nodes on the HBPs are increased to 1; therefore, the data are forwarded by HBPs without the existence of sleeping delay, which greatly reduces transmission latency. Furthermore, the HBPs are built in regions with adequate residual energy, and they are switched periodically; thus, more nodes can be utilized to equalize the energy consumption. A comprehensive performance analysis demonstrates that the HSBP approach has obvious advantages in improving network performance compared with previous studies; it reduces transmission delay by 48.10% and improves energy utilization by 38.21% while guaranteeing the same network lifetime.","['routing', 'wireless sensor networks', 'energy consumption', 'optimization']"
141,"Clustering analysis has the very broad applications on data analysis, such as data mining, machine learning, and information retrieval. In practice, most of clustering algorithms suffer from the effects of noises, different densities and shapes, cluster overlaps, etc. To solve the problems, in this paper, we propose a simple but effective density-based clustering framework (DCF) and implement a clustering algorithm based on DCF. In DCF, a raw data set is partitioned into core points and non-core points by a neighborhood density estimation model, and then the core points are clustered first, because they usually represent the center or dense region of the cluster structure. Finally, DCF classifies the non-core points into initial clusters in sequence. In experiments, we compare our algorithm with Dp and DBSCAN algorithms on synthetic and real-world data sets. The experimental results show that the performance of the proposed clustering algorithm is comparable with DBSCAN and Dp algorithms.","['clustering algorithms', 'shape', 'estimation', 'data mining']"
142,"Given a bug report, bug localization technique can help developers automatically locate potential buggy files. Information retrieval and deep learning approaches have been applied in bug localization by extracting lexical features in bug reports and syntactic features in source code files, though they fail to utilize the structural and semantic information of source code files. In this paper, we present a bug localization system CAST, which exploits deep learning and customized abstract syntax trees of programs to locate potential buggy source files automatically and effectively. Specifically, CAST extracts both lexical semantics in bug reports (e.g., words) and source files (e.g., method names) and program semantics in source files (e.g., abstract syntax tree, AST). Moreover, CAST enhances the tree-based convolutional neural network (TBCNN) model with customized ASTs, which distinguish between user-defined methods and system-provided ones to reflect their contributions leading to defects. Furthermore, customized ASTs group the syntactic entities with similar semantics and prune the ones with little or redundant semantics in order to facilitate the learning performance. Experimental results on four widely-used software projects show that CAST significantly outperforms the state-of-the-art methods in locating the buggy source files.","['deep learning', 'semantics', 'software', 'convolutional neural network']"
143,"Safety critical systems in Advanced Driver Assistance Systems (ADAS) depend on multiple sensors to perceive the environment in which they operate. Radar sensors provide many advantages and complementary capabilities to other available sensors but are not without their own shortcomings. Performance of radar perception algorithms still pose many challenges, one of which is in object detection and classification. In order to increase redundancy in ADAS, the ability for a radar system to detect and classify objects independent of other sensors is desirable. In this paper, an investigation of a machine learning based radar perception algorithm for object detection is implemented, along with a novel, automated workflow for generating large-scale virtual datasets used for training and testing. Physics-based electromagnetic simulation of a complex scattering environment is used to create the virtual dataset. Objects are classified and localized within Doppler-Range results using a single channel 77 GHz FMCW radar system. Utilizing a fully convolutional network, the radar perception model is trained and tested. The training is performed using a wide range of environments and traffic scenarios. Model inference is tested on completely new environments and traffic scenarios. These simulated radar returns are highly scalable and offer an efficient method for dataset generation. These virtual datasets facilitate a simple method of introducing variability in training data, corner case evaluation and root cause analysis, amongst other advantages.","['radar', 'sensors', 'training', 'machine learning', 'object detection']"
144,"Money laundering has been a global issue for decades, which is one of the major threat for economy and society. Government, regulatory and financial institutions are combating it together in their respective capacity, however still billions of dollars in fines by authorities make the headlines in the news. High-speed internet services have enabled financial institutions to deliver better customer experience through multi-channel engagements, which has led to exponential growth in transactions and new avenues for laundering the money for fraudsters. Literature shows the usage of statistical methods, data mining and Machine Learning (ML) techniques for money laundering detection, but limited research on Deep Learning (DL) techniques, primarily due to lack of model interpretability and explainability of the decisions made. Several studies are conducted on application of ML for Anti-Money Laundering (AML), and Explainable Artificial Intelligence (XAI) techniques in general, but lacks the study on usage of DL techniques together with XAI. This paper aims to review the current state-of-the-art literature on DL together with XAI for identifying suspicious money laundering transactions and identify future research areas. Key findings of the review are, researchers have preferred variants of Convolutional Neural Networks, and AutoEncoder; graph deep learning together with natural language processing is emerging as an important technology for AML; XAI use is not seen in AML domain; 51% ML methods used in AML are non-interpretable, 58% studies used sample of old real data; key challenges for researchers are access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced. Future research directions are, application of XAI techniques to bring-out explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between research community and industry to benefit from domain knowledge and controlled access to data.","['deep learning', 'machine learning', 'artificial intelligence', 'data mining']"
145,"Crowd counting is a challenging task due to the influence of various factors, such as scene transformation, complex crowd distribution, uneven illumination, and occlusion. To overcome such problems, scale-adaptive convolutional neural network (SaCNN) used a convolutional neural network to obtain high-quality crowd density map estimation and integrate the density map to get the estimated headcount. To obtain better performance on crowd counting, an improved crowd counting method based on SaCNN was proposed in this paper. The spread parameter, i.e., the standard variance, of geometry-adaptive Gaussian kernel used in SaCNN was optimized to generate a higher quality ground truth density map for training. The absolute count loss with weight 4e-5 was used to jointly optimize with the density map loss to improve the network generalization ability for crowd scenes with few pedestrians. Also, a random cropping method was applied to improve the diversity of training samples to enhance network generalization ability. The experimental results upon ShanghaiTech public dataset showed that the proposed method can obtain more accurate and more robust results on crowd counting than those of SaCNN.","['kernel', 'training', 'estimation', 'convolutional neural network']"
146,"The work in this paper proposes the application of the pinball quantile loss function to guide a deep neural network for Non-Intrusive Load Monitoring. The proposed architecture leverages concepts such as Convolution Neural Networks and Recurrent Neural Networks. For evaluation purposes, this paper also presents a set of complementary performance metrics for energy estimation. Finally, this paper also reports on the results of a comprehensive benchmark between the proposed network and three alternative deep neural networks, when guided by the pinball and Mean Squared Error loss functions. The obtained results confirm the disaggregation superiority of the proposed system, while also showing that the performances obtained using the pinball loss function are consistently superior to the ones obtained using the Mean Squared Error loss.","['neural networks', 'estimation', 'convolution', 'monitoring']"
147,"Trust and reputation are important terms whether the communication is Humans-to-Human (H2H), Human-Machine-Interaction (HMI) or Machine-to-Machine (M2M). As Cloud computing and the internet of things (IoT) bring new innovations, they also cause various security and privacy issues. As numerous devices are continuously integrating as a core part of IoT, it is necessarily important to consider various security issues such as the trustworthiness of a user or detection of a malicious user. Moreover, fog computing also known as edge computing is revolutionizing the Cloud-based IoT by providing the Cloud services at the edge of the network, which can provide aid in overcoming security, privacy and trust issues. In this work, we propose a context-aware trust evaluation model to evaluate the trustworthiness of a user in a Fog based IoT (FIoT). The proposed approach uses a context-aware multi-source trust and reputation based evaluation system which helps in evaluating the trustworthiness of a user effectively. Further, we use context-aware feedback and feedback crawler system which helps in making trust evaluation unbiased, effective and reliable. Furthermore, we introduce monitor mode for malicious/untrustworthy users, which helps in monitoring the behavior and trustworthiness of a user. The proposed approach uses several tunable factors, which can be tuned based on the system's requirements. The simulations and results indicate that our approach is effective and reliable to evaluate the trustworthiness of a user.","['cloud computing', 'privacy', 'security', 'internet of things']"
148,"This paper proposes a novel solution, called a decentralized, efficient, privacy-preserving, and selective aggregation (DEP2SA) scheme, designed to support secure and user privacy-preserving data collection in the advanced metering infrastructure. DEP2SA is more efficient and applicable in real-life deployment, as compared with the state of the art, by adopting and adapting a number of key technologies: 1) it uses a multi-recipient system model, making it more applicable to a liberalized electricity market; 2) it uses the homomorphic Paillier encryption and selective aggregation methods to protect users' consumption data against both external and internal attacks, thus making it more secure; 3) it aggregates data at the gateways that are closest to the data originator, thus saving bandwidth and reducing the risk of creating a performance bottleneck in the system; and 4) it uses short signature and batch signature verification methods to further reduce computational and communication overheads imposed on aggregating nodes. The scheme has been analyzed in terms of security, computational, and communication overheads, and the results show that it is more secure, efficient, and scalable than related schemes.","['privacy', 'encryption', 'encryption', 'security']"
149,"In this paper, we consider the problem of resource allocation in a dense small-cell network. Each small-cell base station is powered by a renewable energy source and operates in the full-duplex mode. We account for the rate-dependent energy term for data decoding into the total energy consumption at the small-cell base station. Owing to this new energy term, the transmitter and receiver operations now draw the energy from a common source. For a new energy consumption model and high interference scenario, which arises due to full-duplex communications, we formulate an energy and load aware resource management optimization problem under the energy causality and total transmit power constraints of the small-cell base station and uplink user equipments. In particular, the problem minimizes the data queue length of each network user equipment by jointly designing the beamformers, power, and sub-carrier allocation and their scheduling. Owing to the non-convexity of the problem, a global solution is inefficient; thus, we opt for the successive parametric convex approximation method to obtain a sub-optimal solution. This method solves for the convex approximate of the non-convex problem in each iteration and leads to faster convergence. For practical implementation, we further develop a distributed algorithm by using the dual decomposition framework, which relies on limited exchange of information between the involved base stations. Numerical simulations compare the network scenario which accounts for uplink channel rate-dependent energy consumption with that which ignores it. Results advocate the need for redesigning of the resource allocation scheme. In addition, numerical simulations also validate the usefulness of full-duplex communications over the half-duplex communications in terms of minimizing the sum data queue length of the users.","['interference', 'resource management', 'decoding', 'optimization']"
150,"The ever-increasing number of diverse and computation-intensive Internet of things (IoT) applications is bringing phenomenal growth in global Internet traffic. Mobile devices with limited resource capacity (i.e., computation and storage resources) and battery lifetime are experiencing technical challenges to satisfy the task requirements. Mobile edge computing (MEC) integrated with IoT applications offloads computation-intensive tasks to the MEC servers at the network edge. This technique shows remarkable potential in reducing energy consumption and delay. Furthermore, caching popular task input data at the edge servers reduces duplicate content transmission, which eventually saves associated energy and time. However, the offloaded tasks are exposed to multiple users and vulnerable to malicious attacks and eavesdropping. Therefore, the assignment of security services to the offloaded tasks is a major requirement to ensure confidentiality and privacy. In this article, we propose a green and secure MEC technique combining caching, cooperative task offloading, and security service assignment for IoT networks. The study not only investigates the synergy between energy and security issues, but also offloads IoT tasks to the edge servers without violating delay requirements. A resource-constrained optimization model is formulated, which minimizes the overall cost combining energy consumption and probable security-breach cost. We also develop a two-stage heuristic algorithm and find an acceptable solution in polynomial time. Simulation results prove that the proposed technique achieves notable improvement over other existing strategies.","['internet of things', 'servers', 'security', 'energy consumption']"
151,"Low-power wireless network for the emerging Internet of Things (IoT) should be reliable enough to satisfy the application requirements, and also energy-efficient for embedded devices to remain battery powered. Synchronized communication methods such as Time Slotted Channel Hopping (TSCH) have shown promising results for these purposes, achieving end-to-end reliability over 99% with low duty-cycles. However, they lack one thing: flexibility to support a wide variety of applications and services with unpredictable traffic load and routing topology due to “fixed” slotframe sizes. To this end, we propose TESLA, a traffic-aware elastic slotframe adjustment scheme for TSCH networks which enables each node to dynamically self-adjust its slotframe size at run time. TESLA aims to minimize its energy consumption without sacrificing reliable packet delivery by utilizing incoming traffic load to estimate channel contention level experienced by each neighbor. We extensively evaluate the effectiveness of TESLA on large-scale 110-node and 79-node testbeds, demonstrating that it achieves up to 70.2% energy saving compared to Orchestra (the de facto TSCH scheduling mechanism) while maintaining 99% reliability.","['reliability', 'routing', 'topology', 'internet of things']"
152,"This paper presents a novel framework for the demystification of convolutional deep learning models for time-series analysis. This is a step toward making informed/explainable decisions in the domain of time series, powered by deep learning. There have been numerous efforts to increase the interpretability of image-centric deep neural network models, where the learned features are more intuitive to visualize. Visualization in the domain of time series is significantly challenging, as there is no direct interpretation of the filters and inputs compared with the imaging modality. In addition, a little or no concentration has been devoted to the development of such tools in the domain of time series in the past. TSViz provides possibilities to explore and analyze the network from different dimensions at different levels of abstraction, which includes the identification of the parts of the input that were responsible for a particular prediction (including per filter saliency), importance of the different filters present in the network, notion of diversity present in the network through filter clustering, understanding of the main sources of variation learned by the network through inverse optimization, and analysis of the network's robustness against adversarial noise. As a sanity check for the computed influence values, we demonstrate our results on pruning of neural networks based on the computed influence information. These representations allow the user to better understand the network so that the acceptability of these deep models for time-series analysis can be enhanced. This is extremely important in domains, such as finance, industry 4.0, self-driving cars, health care, and counter-terrorism, where reasons for reaching a particular prediction are equally important as the prediction itself. We assess the proposed framework for interpretability with a set of desirable properties essential for any method in this direction.","['deep learning', 'visualization', 'optimization', 'neural networks']"
153,"Wireless sensor networks (WSN) have been investigated as a powerful distributed sensing application to enhance the efficiency of embedded systems and wireless networking capabilities. Although WSN has offered unique opportunities to set the foundation for using ubiquitous and pervasive computing, it suffered from several issues and challenges such as frequently changing network topology and congestion issue which affect not only network bandwidth usage but also performance. The main objective of this study is to introduce a congestion-aware clustering and routing (CCR) protocol to alleviate the congestion issue over the network. The CCR protocol is proposed to decrease end-to-end delay time and prolong the network lifetime through choosing the suitable primary cluster head (PCH) and the secondary cluster head (SCH). The experimental results demonstrate that the effectiveness of the CCR protocol to satisfy the quality of service (QoS) requirements in increasing the network lifetime and raising the number of packets sent alike. Moreover, the CCR outperforms other state-of-the-art techniques in decreasing the overflow of data, and thus the network bandwidth usage is reduced.","['wireless sensor networks', 'routing', 'quality of service', 'bandwidth']"
154,"At present, deep learning has been widely adopted in medical image processing. However, the current deep neural networks depend on a large number of labeled training data, but medical images segmentation tasks often suffer from the problem of small quantity of labeled data because labeling medical images is a very expensive and time-consuming task. In order to overcome this difficulty, this paper proposes a new image augmentation strategy based on statistical shape model and three-dimensional thin plate spline, which can generate many simulated images from a small number of real images. Firstly, the shape information of the real labeled images is modeled with the statistical shape model, and a series of simulated shapes are generated by sampling from this model. Secondly, the simulated shapes are filled with texture using three-dimensional thin plate spline to generate the simulated images. Finally, the simulated images and the real images are used together for training deep neural networks. The proposed framework is a general data augmentation method that can be used in any anatomical structure segmentation tasks with any deep neural network architecture. We used two different datasets, including prostate MRI dataset and liver CT dataset, and used two different deep network structures, including multi-scale 3D Convolutional Neural Networks (multi-scale 3D CNN) and U-net. The experimental results showed that the proposed data augmentation strategy can improve the accuracy of existing segmentation algorithms based on deep neural networks.","['shape', 'deep learning', 'neural networks', 'training']"
155,"In non-cooperative communication scenarios, automatic modulation classification (AMC) is the premise of information acquisition. It has been a difficult issue for decades due to the attenuation and interference during wireless transmission. In this paper, a novel deep hierarchical network (DHN) based on convolutional neural network (CNN) is proposed for the AMC. The model is designed to combine the shallow features with high-level features. Thus, it can simultaneously have global receptive field and location information through multi-level feature extraction and does not require any transformation of the raw data. To make full use of limited data, a new method is proposed to use signal-to-noise ratio (SNR) as a weight in training instead of working as an indicator of system robustness. Furthermore, some other deep learning methods have been used to explore whether they could improve the performance of the proposed model. Several new techniques have been chosen to be applied in the DHN at last. Then, a detailed analysis of the improvement in network performance is provided. Combination of the DHN and the weighted-loss can achieve more than 93% classification accuracy which is the best performance in an open source dataset.","['modulation', 'feature extraction', 'deep learning', 'convolution', 'interference']"
156,"Energy-efficient and reliable data gathering using highly stable links in underwater wireless sensor networks (UWSNs) is challenging because of time and location-dependent communication characteristics of the acoustic channel. In this paper, we propose a novel dynamic firefly mating optimization inspired routing scheme called FFRP for the internet of UWSNs-based events monitoring applications. The proposed FFRP scheme during the events data gathering employs a self-learning based dynamic firefly mating optimization intelligence to find the highly stable and reliable routing paths to route packets around connectivity voids and shadow zones in UWSNs. The proposed scheme during conveying information minimizes the high energy consumption and latency issues by balancing the data traffic load evenly in a large-scale network. In additions, the data transmission over highly stable links between acoustic nodes increases the overall packets delivery ratio and network throughput in UWSNs. Several simulation experiments are carried out to verify the effectiveness of the proposed scheme against the existing schemes through NS2 and AquaSim 2.0 in UWSNs. The experimental outcomes show the better performance of the developed protocol in terms of high packets delivery ratio (PDR) and network throughput (NT) with low latency and energy consumption (EC) compared to existing routing protocols in UWSNs.","['routing', 'energy consumption', 'optimization', 'monitoring']"
157,"Advances in Deep Learning (DL) have provided alternative approaches to various complex problems, including the domain of spatial image steganalysis using Convolutional Neural Networks (CNN). Several CNN architectures have been developed in recent years, which have improved the detection accuracy of steganographic images. This work presents a novel CNN architecture which involves a preprocessing stage using filter banks to enhance steganographic noise, a feature extraction stage using depthwise and separable convolutional layers, and skip connections. Performance was evaluated using the BOSSbase 1.01 and BOWS 2 datasets with different experimental setups, including adaptive steganographic algorithms, namely WOW, S-UNIWARD, MiPOD, HILL and HUGO. Our results outperformed works published in the last few years in every experimental setting. This work improves classification accuracies on all algorithms and bits per pixel (bpp), reaching 80.3% on WOW with 0.2 bpp and 89.8% on WOW with 0.4 bpp, 73.6% and 87.1% on S-UNIWARD (0.2 and 0.4 bpp respectively), 68.3% and 81.4% on MiPOD (0.2 and 0.4 bpp), 68.5% and 81.9% on HILL (0.2 and 0.4 bpp), 74.6% and 84.5% on HUGO (0.2 and 0.4 bpp), using BOSSbase 1.01 test data.","['convolution', 'feature extraction', 'convolutional neural networks', 'convolutional neural network', 'deep learning']"
158,"The last half-decade has seen a surge in deep learning research on irregular domains and efforts to extend convolutional neural networks (CNNs) to work on irregularly structured data. The graph has emerged as a particularly useful geometrical object in deep learning, able to represent a variety of irregular domains well. Graphs can represent various complex systems, from molecular structure, to computer and social and traffic networks. Consequent on the extension of CNNs to graphs, a great amount of research has been published that improves the inferential power and computational efficiency of graph-based convolutional neural networks (GCNNs). The research is incipient, however, and our understanding is relatively rudimentary. The majority of GCNNs are designed to operate with certain properties. In this survey we review of the state of graph representation learning from the perspective of deep learning. We consider challenges in graph deep learning that have been neglected in the majority of work, largely because of the numerous theoretical difficulties they present. We identify four major challenges in graph deep learning: dynamic and evolving graphs, learning with edge signals and information, graph estimation, and the generalization of graph models. For each problem we discuss the theoretical and practical issues, survey the relevant research, while highlighting the limitations of the state of the art. Advances on these challenges would permit GCNNs to be applied to wider range of domains, in situations where graph models have previously been limited owing to the obstructions to applying a model owing to the domains' natures.","['convolution', 'deep learning', 'estimation', 'convolutional neural networks']"
159,"The wide coverage of satellite networks and the large bandwidth of terrestrial networks have led to an increasing research on the integration of the two networks (ISTN) for complementary advantages. However, most researches on routing mainly focus on the internal routing of the satellite network. Due to the point-to-area coverage of the channel characteristics between the satellites and the ground stations, the heterogeneity of ISTN has increased, which makes that the routing algorithm of the traditional satellite networks cannot be applied to the end-to-end routing of ISTN. Meanwhile, the data flows with elastic quality of service attribute make the routing pre-assign and resource allocation in ISTN much more complicated, which has been rarely researched before. In this paper, we first describe the unified network architecture based on software-defined network and model the data flow. Then, based on the considerations of latency, capacity, wavelength fragmentation, and load balancing, a heuristic service-oriented path computation algorithm for elastic data flows is proposed for the complex heterogeneity of the ISTN. The simulation shows that, the end-to-end routing mechanism can reduce the blocking rate of the ISTN, and our proposed algorithm greatly reduces the wavelength fragmentation and bandwidth consumption, and has a better performance on load balancing, with a slight disadvantage in latency when the network load is high.","['satellites', 'routing', 'bandwidth', 'quality of service']"
160,"Multiple-operators (multi-OPs) spectrum sharing mechanism can effectively improve the spectrum utilization in fifth-generation (5G) wireless communication networks. The secondary users are introduced to opportunistically access the licensed spectrum of idle operators (OPs). However, the identity privacy and data security issues raise great concerns about the secure spectrum sharing among multi-OPs. To address these challenges, a permissioned blockchain trust framework is proposed for the spectrum sharing in multi-OPs wireless communication networks in this paper. The Multi-OPs Spectrum Sharing (MOSS) smart contract is designed on the constructed permissioned blockchain to implement the spectrum trading among multi-OPs. Without the need of trustless spectrum broker, the MOSS smart contract enforces multi-OPs to share the spectrum truthfully and designs a punishment mechanism to punish malicious OPs. The MOSS smart contract is tested on the Remix integrated development environment (IDE) and the gas costs of MOSS smart contract is estimated. The performance analysis of the proposed permissioned blockchain trust framework demonstrates that the privacy, openness and fairness of the proposed solution are better than traditional spectrum allocation solutions.","['blockchain', 'wireless communication', 'security', 'privacy']"
161,"For the wireless sensor networks (WSNs) heterogeneous node deployment optimization problem with obstacles in the monitoring area, two new flower pollination algorithms (FPA) are proposed to deploy the network. Firstly, an improved flower pollination algorithm (IFPA) is proposed based on FPA, aiming at the shortcomings of the convergence speed is slow and the precision is not high enough of FPA. The nonlinear convergence factor is designed to correct the scaling factor of FPA, the Tent chaotic map effectively maintains the diversity of the population in the late iteration, and a greedy crossover strategy is designed to assist the remaining individual search with better individuals. Secondly, based on FPA, a non-dominated sorting multi-objective flower pollination algorithm (NSMOFPA) is proposed. The external archive strategy and leader strategy are introduced, to solve the global pollination problem. The proposed crowding degree method and the introduced elite strategy effectively maintain the diversity of the population. Then, IFPA is applied to WSN deployment aiming at optimizing coverage rate, simulation experiments show that IFPA can obtain a higher coverage rate with shorter iterations, which can save network deployment costs. Finally, applying NSMOFPA to the WSN deployment with optimization objectives for coverage rate, node radiation overflow rate and energy consumption rate. The experimental results verify that NSMOFPA has a good optimization effect and can provide a better solution for WSN deployment.","['optimization', 'wireless sensor networks', 'monitoring', 'energy consumption', 'convergence']"
162,"Cloud computing has been widely applied in numerous applications for storage and data analytics tasks. However, cloud servers engaged through a third party cannot be fully trusted by multiple data users. Thus, security and privacy concerns become the main obstructions to use machine learning services, especially with multiple data providers. Additionally, some recent outsourcing machine learning schemes have been proposed in order to preserve the privacy of data providers. Yet, these schemes cannot satisfy the property of public verifiability. In this paper, we present an efficient privacy-preserving machine learning scheme for multiple data providers. The proposed scheme allows all participants in the system model to publicly verify the correctness of the encrypted data. Furthermore, a unidirectional proxy re-encryption (UPRE) scheme is employed to reduce the high computational costs along with multiple data providers. The cloud server embeds noise in the encrypted data, allowing the analytics to apply machine learning techniques and preserve the privacy of data providers’ information. The results and experiments tests demonstrate that the proposed scheme has the ability to reduce computational costs and communication overheads.","['cloud computing', 'encryption', 'machine learning', 'privacy']"
163,"Smart City is a cutting edge technology driven city aimed at making our everyday lives safer and more convenient. Like every city it is comprised of structures. To uphold their safety these structures have to undergo periodical maintenance checks. However, checks are done manually, thus consume a significant amount of time and financial resources. Remote Structural Health Monitoring (RSHM) is a perfect solution for a Smart City, however limited research is done in applying Internet of Things (IoT) sensor nodes for permanent RSHM in applications that use bolted joints as a main fastening method. Most of the proposed solutions are not suitable, since either additional structural modifications are required, they violate standards, provide poor wireless communication range or still require human intervention. Therefore, this paper proposes an IoT sensor node named TenSense M30, which leverages Long Range (LoRa) communication technology to enable wide communication coverage and long life for monitoring the health of bolted joints in a Smart City environment. TenSense M30 has a miniature footprint and can be used as an add-on to current structures. Mechanical, hardware and embedded designs are presented and evaluated by means of simulation and practical tests. Results show that TenSense M30 is capable of precisely monitoring the pre-tension load of bolted joints, has a secured non-line of sight communication channel with a distance of 800 meters, has an expected lifetime of greater than 5 years powered by a number of coin cell batteries, and is safe under the pre-tension load.","['monitoring', 'wireless communication', 'batteries', 'internet of things']"
164,"Assuring the coverage towards the predefined set of targets, power-constrained wireless sensor networks (WSNs) consist of sensing devices (i.e., sensor nodes) that are associated with limited battery life and fixed sensing range. All the sensors are collectively responsible for covering these sets of objects. The standard target coverage problem is the one where continuous coverage is provided over a predefined set of targets for the maximum possible duration so that the scarce resource (battery power) can be optimally utilized. Therefore, in order to incorporate quality of service (QoS) in the network and ensure smooth monitoring of the desired target set, the paper addresses target Q-Coverage, which is one of the variants of standard target coverage problem where a target is covered by at least Q-sensors (pre-defined number) in every cover set. A cover set is a subset of sensors which cover whole targets in a single iteration. In this paper, a greedy heuristic based technique, i.e., maximum coverage small lifetime (MCSL) has been proposed, which restricts the usages of those sensors that poorly cover targets and promotes the usage of those sensors that have maximum coverage and energy. Simulations are performed on static wireless sensor network with varying Q values to test the efficiency of the proposed method. The performance of the proposed heuristic is compared with optimal upper bound based on network lifetime, and results prove that performance is improvised by 94%. The obtained results are further compared with the existing approaches to prove the superiority of the proposed work via extensive experimentations.","['wireless sensor networks', 'monitoring', 'sensors', 'quality of service']"
165,"Emotion recognition using miniaturised wearable physiological sensors has emerged as a revolutionary technology in various applications. However, detecting emotions using the fusion of multiple physiological signals remains a complex and challenging task. When fusing physiological signals, it is essential to consider the ability of different fusion approaches to capture the emotional information contained within and across modalities. Moreover, since physiological signals consist of time-series data, it becomes imperative to consider their temporal structures in the fusion process. In this study, we propose a temporal multimodal fusion approach with a deep learning model to capture the non-linear emotional correlation within and across electroencephalography (EEG) and blood volume pulse (BVP) signals and to improve the performance of emotion classification. The performance of the proposed model is evaluated using two different fusion approaches - early fusion and late fusion. Specifically, we use a convolutional neural network (ConvNet) long short-term memory (LSTM) model to fuse the EEG and BVP signals to jointly learn and explore the highly correlated representation of emotions across modalities, after learning each modality with a single deep network. The performance of the temporal multimodal deep learning model is validated on our dataset collected from smart wearable sensors and is also compared with results of recent studies. The experimental results show that the temporal multimodal deep learning models, based on early and late fusion approaches, successfully classified human emotions into one of four quadrants of dimensional emotions with an accuracy of 71.61% and 70.17%, respectively.","['electroencephalography', 'deep learning', 'sensors', 'convolutional neural network']"
166,"The latest Joint Video Exploration Team employs quad-tree plus binary-tree (QTBT) block partitioning structure, which can improve coding performance significantly than High Efficiency Video Coding with hugely increased encoding complexity. To address this issue, we propose a novel fast QTBT partition method through a convolutional neural network (CNN). Specifically, the proposed algorithm uses CNN to predict the QTBT partition depth range of 32 × 32 block directly according to the inherent texture richness of the image, rather than to judge split or not at each depth level. For training optimization, we introduce a misclassification penalty term combined with L2 HingeLoss function, which can further boost the classification accuracy. Experimental results demonstrate the effectiveness of our proposed method; our rate-distortion maintaining setting can achieve 42.33% complexity reduction with just 0.69% bitrate increase. Our low complexity setting can achieve 62.08% complexity reduction with 2.04% bitrate increase.","['encoding', 'training', 'optimization', 'convolutional neural network']"
167,"Acoustic seabed classification (ASC) is a fast and large-scale seabed sediment survey method. In particular, combining it with an automated classifier can theoretically achieve fast automatic seabed sediment classification. However, owing to the cost of sampling, a lack of labeled data for sediment classification based on seabed acoustic images impedes the training and deployment of classifiers. Herein, we use shallow-water, side-scan sonar images collected from the Pearl River Estuary combined with deep learning to study sediment classification and optimization methods for a small dataset of seabed acoustic images. In this paper, we applied different and deeper convolutional neural networks (CNNs) and used grayscale CIFAR-10 for pretraining to achieve large-span parameter migration and improve model performance. The best result in the experiment is a 3.459% error rate achieved by ResNet after fine tuning, verifying the improvement brought by our fine tuning strategy and the deeper models used in such tasks. The results of data enhancement based on generative adversarial networks (GANs) indicated that this method can improve the accuracy of sediment classification; however, the effects of GANs are limited and they are computationally expensive. Overall, our findings resolve, to an extent, the dilemma of using small datasets of seabed acoustic images for sediment classification and provide a framework for future studies on sediment classification, which has a certain significance in helping people better understand the seabed.","['training', 'convolutional neural networks', 'deep learning', 'convolutional neural network']"
168,"Wireless sensor networks (WSNs) are widely applied in various industrial applications. In this paper, we present an efficient data gathering scheme that guarantees the Quality of Service and optimizes the following network performance metrics as well as the end-to-end reliability in WSNs: (1) minimum total energy consumption; (2) minimum unit data transmitting energy consumption; and (3) maximum utilization efficiency defined as network lifetime per unit deployment. We first transform the performance optimization problem into a problem to optimize the following parameters: (1) deployed nodal number N*; (2) nodal placement d*; and (3) nodal transmission structure p*. Then, we prove that the optimization problem is solvable mathematically. For our observation, the sensor nodes close to the sink trend failed early since they consumed more energy with heavier relay traffic destined for the sink, which seriously affects the network performance. The key point of this optimization is adopting lower reliability requirements and shorter transmission distance for nodes near the sink. Consequently, this reduces the energy consumption of the nodes in the hotspot area. Meanwhile, it adopts higher reliability requirements and farther transmission distance for nodes far from the sink to make full use of the node residual energy, so as to optimize the network performance without harming network reliability. Numerical simulation results demonstrate that our optimal approach improves the network lifetime by 18%-48% and network utility by 17%, and guarantees desire reliability level.","['wireless sensor networks', 'optimization', 'reliability', 'energy consumption', 'quality of service']"
169,"A novel and efficient end-to-end learning model for automatic modulation classification is proposed for wireless spectrum monitoring applications, which automatically learns from the time domain in-phase and quadrature data without requiring the design of hand-crafted expert features. With the intuition of convolutional layers with pooling serving as the role of front-end feature distillation and dimensionality reduction, sequential convolutional recurrent neural networks are developed to take complementary advantage of parallel computing capability of convolutional neural networks and temporal sensitivity of recurrent neural networks. Experimental results demonstrate that the proposed architecture delivers overall superior performance in signal to noise ratio range above −10 dB, and achieves significantly improved classification accuracy from 80% to 92.1% at high signal to noise ratio range, while drastically reduces the average training and prediction time by approximately 74% and 67%, respectively. Response patterns learned by the proposed architecture are visualized to better understand the physics of the model. Furthermore, a comparative study is performed to investigate the impacts of various sequential convolutional recurrent neural network structure settings on classification performance. A representative sequential convolutional recurrent neural network architecture with the two-layer convolutional neural network and subsequent two-layer long short-term memory neural network is developed to suggest the option for fast automatic modulation classification.","['convolution', 'modulation', 'monitoring', 'convolutional neural networks']"
170,"With the development of cloud storage, more data owners are inclined to outsource their data to cloud services. For privacy concerns, sensitive data should be encrypted before outsourcing. There are various searchable encryption schemes to ensure data availability. However, the existing search schemes pay little attention to the efficiency of data users' queries, especially for the multi-owner scenario. In this paper, we proposed a tree-based ranked multi-keyword search scheme for multiple data owners. Specifically, by considering a large amount of data in the cloud, we utilize the TF × IDF model to develop a multikeyword search and return the top-k ranked search results. To enable the cloud servers to perform a secure search without knowing any sensitive data (e.g., keywords and trapdoors), we construct a novel privacypreserving search protocol based on the bilinear mapping. To achieve an efficient search, for each data owner, a tree-based index encrypted with an additive order and privacy-preserving function family is constructed. The cloud server can then merge these indexes effectively, using the depth-first search algorithm to find the corresponding files. Finally, the rigorous security analysis proves that our scheme is secure, and the performance analysis demonstrates its efficacy and efficiency.","['indexes', 'encryption', 'servers', 'security']"
171,"In wireless sensor networks, cross-layer optimization is a hot topic. In order to balance energy consumption a clustering based cross-layer optimization model is established in this paper. In this model, the network layer could get the related information from physical layer and the datalink layer to effectively route packets. Based on the cross-layer optimization model, an energy-efficient ring cross-layer optimization algorithm is proposed and a new routing algorithm called Leach-Cross-Layer Optimization (CLO) is also proposed for a ring monitoring domain. This Leach-CLO routing algorithm is based on the typical Leach algorithm. The experiment results show that this routing algorithm could effectively and efficiently balance the energy consumption in wireless sensor networks compared with related algorithms.","['wireless sensor networks', 'optimization', 'routing', 'energy consumption']"
172,"In recent years, service-oriented-based Internet of Things (IoT) has received massive attention from research and industry. Integrating and composing smart objects functionalities or their services is required to create and promote more complex IoT applications with advanced features. When many smart objects are deployed, selecting the most appropriate set of smart objects to compose a service by considering both energy and quality of service (QoS) is an essential and challenging task. In this paper, we reduced the problem of finding an optimal balance between QoS level and the consumed energy of the IoT service composition to a bi-objective shortest path optimization (BSPO) problem and used an exact algorithm named pulse to solve the problem. The BSPO has two objectives, minimizing the QoS including execution time, network latency, and service price, and minimize the energy consumption of the composite service. Experimental evaluations show that the proposed approach has short execution time in various complex service profiles. Meanwhile, it can obtain good performance in energy consumption and thus network lifetime while maintaining a reasonable QoS level.","['quality of service', 'energy consumption', 'optimization', 'internet of things']"
173,"The efficient classification of remote sensing images (RSIs) has become the key of remote sensing application. To tackle the high computational cost in the traditional classification method, in this paper we propose a new RSI classification method based on improved convolutional neural network (CNN) and support vector machine (SVM) (CNN-SVM). In this method, we first designed a seven-layer CNN structure and took the ReLU function as the activation function. We then inputted the RSI into the CNN model and extracted feature maps and replaced the output layer of the CNN network via training the feature maps in the SVM classifier. Next, taking the simulation experiments of MNIST handwritten digital dataset and UC Merced Land Use remote sensing dataset as examples, we tested and verified the proposed method in this experiment. Finally, the empirical study of volcanic ash cloud (VAC) classification from moderate resolution imaging spectroradiometer (MODIS) RSI was carried out and evaluated. The experimental results show that compared with the traditional methods, the proposed method has lower loss value and better generalization in modeling training; the total classification accuracy of VAC and Kappa coefficient reached 93.5% and 0.8502, respectively, and achieved preferable VAC identification and visual effects. It will enhance the classification accuracy to the massive remote sensing data.","['remote sensing', 'training', 'convolution', 'convolutional neural network']"
174,"Buildings are responsible for 33% of final energy consumption, and 40% of direct and indirect CO 2 emissions globally. While energy consumption is steadily rising globally, managing building energy utilization by on-site renewable energy generation can help responding to this demand. This paper proposes a deep learning method based on a discrete wavelet transformation and long short-term memory method (DWT-LSTM) and a scheduling framework for the integrated modelling and management of energy demand and supply for buildings. This method analyzes several factors including electricity price, uncertainty in climatic factors, availability of renewable energy sources (wind and solar), energy consumption patterns in buildings, and the non-linear relationships between these parameters on hourly, daily, weekly and monthly intervals. The method enables monitoring and controlling renewable energy generation, the share of energy imports from the grid, employment of saving strategy based on the user priority list, and energy storage management to minimize the reliance on the grid and electricity cost, especially during the peak hours. The results demonstrate that the proposed method can forecast building energy demand and energy supply with a high level of accuracy, showing a 3.63-8.57% error range in hourly data prediction for one month ahead. The combination of the deep learning forecasting, energy storage, and scheduling algorithm enables reducing annual energy import from the grid by 84%, which offers electricity cost savings by 87%. Finally, two smart active buildings configurations are financially analyzed for the next thirty years. Based on the results, the proposed smart building with solar Photo-Voltaic (PV), wind turbine, inverter, and 40.5 kWh energy storage has a financial breakeven point after 9 years with wind turbine and 8 years without it. This implies that implementing wind turbines in the proposed building is not financially beneficial.","['buildings', 'renewable energy sources', 'energy consumption', 'deep learning']"
175,"5G mobile communication systems promote the mobile network to not only interconnect people, but also interconnect and control the machine and other devices. 5G-enabled Internet of Things (IoT) communication environment supports a wide-variety of applications, such as remote surgery, self-driving car, virtual reality, flying IoT drones, security and surveillance and many more. These applications help and assist the routine works of the community. In such communication environment, all the devices and users communicate through the Internet. Therefore, this communication agonizes from different types of security and privacy issues. It is also vulnerable to different types of possible attacks (for example, replay, impersonation, password reckoning, physical device stealing, session key computation, privileged-insider, malware, man-in-the-middle, malicious routing, and so on). It is then very crucial to protect the infrastructure of 5G-enabled IoT communication environment against these attacks. This necessitates the researchers working in this domain to propose various types of security protocols under different types of categories, like key management, user authentication/device authentication, access control/user access control and intrusion detection. In this survey paper, the details of various system models (i.e., network model and threat model) required for 5G-enabled IoT communication environment are provided. The details of security requirements and attacks possible in this communication environment are further added. The different types of security protocols are also provided. The analysis and comparison of the existing security protocols in 5G-enabled IoT communication environment are conducted. Some of the future research challenges and directions in the security of 5G-enabled IoT environment are displayed. The motivation of this work is to bring the details of different types of security protocols in 5G-enabled IoT under one roof so that the future researchers will be benefited with the conducted work.","['security', 'internet of things', '5g mobile communication', 'protocols', 'privacy']"
176,"Adversarial examples are perturbed inputs that are designed (from a deep learning network's (DLN) parameter gradients) to mislead the DLN during test time. Intuitively, constraining the dimensionality of inputs or parameters of a network reduces the “space”in which adversarial examples exist. Guided by this intuition, we demonstrate that discretization greatly improves the robustness of the DLNs against adversarial attacks. Specifically, discretizing the input space (or allowed pixel levels from 256 values or 8bit to 4 values or 2bit) extensively improves the adversarial robustness of the DLNs for a substantial range of perturbations for minimal loss in test accuracy. Furthermore, we find that binary neural networks (BNNs) and related variants are intrinsically more robust than their full precision counterparts in adversarial scenarios. Combining input discretization with the BNNs furthers the robustness, even waiving the need for adversarial training for the certain magnitude of perturbation values. We evaluate the effect of discretization on MNIST, CIFAR10, CIFAR100, and ImageNet datasets. Across all datasets, we observe maximal adversarial resistance with 2bit input discretization that incurs an adversarial accuracy loss of just ~ 1% - 2% as compared to clean test accuracy against single-step attacks. We also show standalone discretization remains vulnerable to stronger multi-step attack scenarios necessitating the use of adversarial training with discretization as an improved defense strategy.","['robustness', 'training', 'neural networks', 'deep learning']"
177,"Recently, the techniques of Internet of Things (IoT) and mobile communications have been developed to gather human and environment information data for a variety of intelligent services and applications. Remote monitoring of elderly and disabled people living in smart homes is highly challenging due to probable accidents which might occur due to daily activities such as falls. For elderly people, fall is considered as a major reason for death of post-traumatic complication. So, early identification of elderly people falls in smart homes is needed to increase the survival rate of the person or offer required support. Recently, the advent of artificial intelligence (AI), IoT, wearables, smartphones, etc. makes it feasible to design fall detection systems for smart homecare. In this view, this paper presents an IoT enabled elderly fall detection model using optimal deep convolutional neural network (IMEFD-ODCNN) for smart homecare. The goal of the IMEFD-ODCNN model is to enable smartphones and intelligent deep learning (DL) algorithms to detect the occurrence of falls in the smart home. Primarily, the input video captured by the IoT devices is pre-processed in different ways like resizing, augmentation, and min-max based normalization. Besides, SqueezeNet model is employed as a feature extraction technique to derive appropriate feature vectors for fall detection. In addition, the hyperparameter tuning of the SqueezeNet model takes place using the salp swarm optimization (SSO) algorithm. Finally, sparrow search optimization algorithm (SSOA) with variational autoencoder (VAE), called SSOA-VAE based classifier is employed for the classification of fall and non-fall events. Finally, in case of fall event detected, the smartphone sends an alert to the caretakers and hospital management. The performance validation of the IMEFD-ODCNN model takes place on UR fall detection dataset and multiple cameras fall dataset. The experimental outcomes highlighted the promising performance of the IMEFD-ODCNN model over the recent methods with the maximum accuracy of 99.76% and 99.57% on the multiple cameras fall and UR fall detection dataset.","['feature extraction', 'cameras', 'artificial intelligence', 'deep learning']"
178,"The Internet of Things (IoT) introduces a new challenge for Database Management Systems (DBMS). In IoT, large numbers of sensors are used in daily lives. These sensors generate a huge amount of heterogeneous data that needs to be handled by the appropriate DBMS. The IoT has a challenge for the DBMS in evaluating how to store and manipulate a huge amount of heterogeneous data. DBMS can be categorized into two main types: The Relational DBMSs and the Non-relational DBMSs. This paper aims to provide a thorough comparative evaluation of two popular open-source DBMSs: MySQL as a Relational DBMS and MongoDB as a Non-relational DBMS. This comparison is based on evaluating the performance of inserting and retrieving a huge amount of IoT data and evaluating the performance of the two types of databases to work on resources with different specifications in cloud computing. This paper also proposes two prediction models and differentiates between them to estimate the response time in terms of the size of the database and the specifications of the cloud instance. These models help to select the appropriate DBMS to manage and store a certain size of data on an instance with particular specifications based on the estimated response time. The results indicate that MongoDB outperforms MySQL in terms of latency and the database size through increasing the amount of tested data. Moreover, MongoDB can save resources better than MySQL that needs resources with high capabilities to work with less performance.","['databases', 'cloud computing', 'internet of things', 'sensors']"
179,"Wireless Sensor Networks (WSNs) achieve much attention from various domains because of its easy maintenance, self-configuration, and scalability characteristics. It is comprised of small-sized sensors that interact with the Internet of Things (IoT) for observing and recording the physical conditions. The sensor nodes are autonomous and construct inter-communication topology with each other in an ad-hoc manner. However, the main restrictions of sensor nodes are their finite resources for energy management, data storage, transmission, and processing power. Different solutions have been addressed by researchers to overcome network performance due to bounded limitations of such battery-powered nodes, however, equalize the energy consumption and maintain the network throughput are the main research problems. Furthermore, due to the compromised nodes, the data is more prone to security vulnerabilities. Therefore, their security over the unpredictable network is other research concerns. Thus, the aim of this research article to propose a secure and energy-aware heuristic-based routing (SEHR) protocol for WSN to detect and prevent compromising data with efficient performance. Firstly, the proposed protocol makes use of an artificial intelligence-based heuristic analysis to accomplish a reliable, and intellectual learning scheme. Secondly, it protects the transmissions against adversary groups to attain security with the least complexity. Moreover, the route maintenance strategy is also achieved by using traffic exploration to reduce link failures and network dis-connectivity. The simulation results demonstrated the SEHR protocol improves the efficacy for network throughput by an average of 18%, packet drop ratio by 42%, end-to-end delay by 26%, energy consumption by 36%, faulty routes by 38%, network overhead by 44%, and computational overhead by 43% in dynamic scenarios as compared to existing work.","['routing', 'wireless sensor networks', 'sensors', 'security', 'artificial intelligence']"
180,"This paper proposes a new method based on a multiple branch cross-connected convolutional neural network (MBCC-CNN) for facial expression recognition. Compared with traditional machine learning methods, the proposed method can extract image features more effectively. In addition, in contrast to single-structure convolutional neural networks, the MBCC-CNN model is constructed based on the residual connection, Network in Network, and tree structure approaches together. It also adds a shortcut cross connection for the summation of the convolution output layer, which makes the data flow between networks more smooth, improves the feature extraction ability of each receptive field. The proposed method can fuse the features of each branches more effectively, which solves the problem of insufficient feature extraction of each branches and increases the recognition performance. The experimental results based on the Fer2013, CK+, FER+ and RAF data sets show that the recognition rates of the proposed MBCC-CNN method are 71.52%, 98.48%, 88.10% and 87.34%, respectively. Compared with some most recently work, the proposed method can provide better facial expression recognition performance and has good robustness. The python code can be download from https://github.com/scp19801980/Facial-expression-recognition .","['feature extraction', 'convolutional neural networks', 'neural networks', 'convolution', 'convolutional neural network', 'robustness']"
181,"In wireless sensor networks, it is a typical threat to source privacy that an attacker performs backtracing strategy to locate source nodes by analyzing transmission paths. With the popularity of the Internet of Things in recent years, source privacy protection has attracted a lot of attentions. In order to mitigate this threat, many proposals show their merits. However, they fail to get the tradeoff between multipath transmission and transmission cost. In this paper, we propose a constrained random routing mechanism, which can constantly change routing next-hop instead of a relative fixed route so that attackers cannot analyze routing and trace back to source nodes. First, we design a specific selection domain which is located around the sending node according to the dangerous distance and the wireless communication range. Then sending nodes calculate the selected weights of the candidate nodes according to their offset angles in this domain. Finally, the selected weights help to decide which node will become the next hop. In this way, attackers would be confused by the constantly changing paths. The simulation results prove that our proposal can achieve high routing efficiency in multi-path transmission, while only introducing a controllable energy consumption, end-to-end delay and redundant paths.","['privacy', 'routing', 'wireless sensor networks', 'wireless communication', 'energy consumption']"
182,"In recent years, rehabilitation has become a specialist field after a sports injury, and the evolution of rehabilitation has inevitably brought together a sports physiotherapist, sports doctor, and an orthopedic surgeon. For sports athletics, it is essential to determine the strategies to prevent injuries, optimize the rehabilitation, and improve performance. In the field of computer vision, deep learning has made a great success and the accuracy in image detection and image classification. Computer-aided physical rehabilitation evaluation involves assessing patient performance during the completion of prescribed rehabilitation exercises using sensory system data from the processing of movement. Since the rehabilitation assessment plays a vital role in improving patient outcomes and reducing healthcare costs, existing solutions lack versatility, robustness, and practical relevance. In this article, Hybridized Hierarchical Deep Convolutional Neural Network (HHDCNN) has been introduced to enhance the accuracy, image segmentation of sports athletics exercise rehabilitation. The main components of the framework include measurements to quantify motion performance, scoring of performance measurement features into numerical quality scores, and deep convolutional neural network models to generate quality scores of input movements through supervised learning. Compared to many traditional neural network algorithms, the image segmentation algorithm enhances the convergence speed of the network, shortens training time, and improves the accuracy of sports athletics exercise rehabilitation, which is good practice for the reconstruction of the sports rehabilitation exercise.","['deep learning', 'image segmentation', 'training', 'convolution']"
183,"Recently, convolutional neural networks (CNNs) have been introduced for hyperspectral image (HSI) classification and shown considerable classification performance. However, the previous CNNs designed for spectral-spatial HSI classification lay stress on the learning for the spatial correlation of HSI data and neglect the channel responses of feature maps. Furthermore, the lack of training samples remains the major challenge for CNN-based HSI classification methods to achieve better performance. To address the aforementioned issues, this paper proposes a new end-to-end pre-activation residual attention network (PRAN) for HSI classification. The pre-activation mechanism and attention mechanism are introduced into the proposed network, and a pre-activation residual attention block (PRAB) is designed, which allows the proposed network to carry adaptively feature recalibration of channel responses and learn more robust spectral-spatial joint feature representations. The proposed PRAN is equipped with two PRABs and several convolutional layers with different kernel sizes, which enables the PRAN to extract high-level discriminative features. Experimental results on three benchmark HSI datasets reveal that the proposed method is provided with competitive performance over several state-of-the-art HSI classification methods, especially when the training set size is relatively small.","['training', 'convolution', 'correlation', 'convolutional neural network']"
184,"Decentralizing multi-authority attribute-based encryption (ABE) has been adopted for solving problems arising from sharing confidential corporate data in cloud computing. For decentralizing multiauthority ABE systems that do not rely on a central authority, collusion resistance can be achieved using a global identifier. Therefore, identity needs to be managed globally, which results in the crucial problems of privacy and security. A scheme is developed that does not use a central authority to manage users and keys, and only simple trust relations need to be formed by sharing the public key between each attribute authority (AA). User identities are unique by combining a user's identity with the identity of the AA where the user is located. Once a key request needs to be made to an authority outside the domain, the request needs to be performed by the authority in the current domain rather than by the users, so, user identities remain private to the AA outside the domain, which will enhance privacy and security. In addition, the key issuing protocol between AA is simple as the result of the trust relationship of AA. Moreover, extensibility for authorities is also supported by the scheme presented in this paper. The scheme is based on composite order bilinear groups. A proof of security is presented that uses the dual system encryption methodology.","['encryption', 'privacy', 'cloud computing', 'resistance']"
185,"This paper describes a prototype of an intelligent Stress Monitoring Assistant (SMA), - the next generation of stress detectors. The SMA is intended for the first responders and professionals coping with exposure to extreme physical and psychological stressors, e.g. firefighters, combat military personnel, explosive ordnance disposal operatives, law enforcement officers, emergency medical technicians, and paramedics. Stress impacts human behavior and decision-making, which can be propagated between the team members. The SMA is an integral part of the Decision Support System, it is a component of the decision support perception-action cycle. We model this cycle as a cognitive dynamic system. The intelligent part of the SMA is designed using a) a residual-temporal convolution network for learning data from sensors and detection of stress features, and b) a reasoning mechanism based on a causal network for fusion at various levels. The SMA prototype has been tested using a multi-factor physiological dataset WEarable Stress and Affect Detection (WESAD). In both modes, the stress recognition and stress detection, the SMA achieves an accuracy of 86% and 98% for the WESAD dataset, respectively. This performance is superior to the known results in satisfying the requirements of reliable decision support.","['stress', 'sensors', 'monitoring', 'detectors']"
186,"The achievement of the boundary values of spectral efficiency is associated with the development of methods for generating and receiving signals with a compact spectrum. The reason is a constant increase in the capacity of existing communication channels caused by an increase in the volume of transmitted information. Moreover, the allocated frequency bandwidths have natural limitations, and they are almost all reached. The effective way to approach the boundary values of spectral efficiency is the application of spectrally efficient signals, such as FTN (Faster-Than-Nyquist) signals. This article proposes to synthesize optimal FTN signals that are more compact in the spectrum than RRC (root raised cosine) pulses-based signals. The criterion of maximum energy concentration in the occupied frequency bandwidth and the constraint on the cross-correlation coefficient are used to solve the optimization problem. The contribution of this work is the optimization of FTN signal shape. The obtained optimal FTN signals provide a 24% increase in spectral efficiency compared to the RRC pulses-based signals. At the same time, the energy loss stays almost unchanged. To the best of authors' knowledge, this is the first case when the frequency bandwidth reduction is achieved at practically no energy loss. The simulation modeling in channels with additive white Gaussian noise and fading channels is done with regard to the FTN-SC-FDE (Faster-Than-Nyquist - Single Carrier system with Frequency Domain Equalization) structure. The optimal FTN signals presented in this work can be used to increase the spectral efficiency of satellite broadcasting systems, such as DVB-S2/S2X.","['bandwidth', 'optimization', 'shape', 'correlation']"
187,"Random number generators (RNGs) are the foundation of strong security and privacy measures. With an increasing number of smart devices being connected to the Internet, the demand for secure communication will only increase. An important outgrowth of Internet-connected devices is the embedding of sensors. Yet, there remains a paucity of good protocols to provide sensor-based secure RNG seeds. In their raw form, sensor data are a weak source of RNG seeds for two reasons: 1) adversarial control - a malicious party gaining control of the sensor and generating a known data sequence and 2) collinearity across sensors - inherent correlated sequences generated because sensors are embedded in the same device. We propose a new seeding technique that leverages sensor data to provide secure seeds for RNG. Given the current proliferation of sensors and Internet-connectivity on smart devices, this technique could increase cybersecurity in a variety of domains, without additional cost.","['generators', 'privacy', 'protocols', 'security']"
188,"Underwater wireless sensor networks (UWSNs) based on magnetic induction (MI) have been recently proposed as a promising candidate for underwater networking due to its benefits, such as small transmission delay, low vulnerability to environment changes, multipath fading negligibility, and high bandwidth. Most of the UWSN applications are location dependent and, thus, localization plays an important functionality for obtaining sensor positions. In this paper, we first study an MI-based monitoring network in shallow sea, then focus on how to design an optimal node deployment strategy and a clustering algorithm to prolong network lifetime for a 3D-UWSN by reducing the network energy consumption. Using the Voronoi diagram, we propose a high-energy node priority clustering algorithm, in which a cluster head would be selected according to the remaining energy of sensor nodes and the geometry distance among them. Moreover, in order to improve the efficiency of data collection, we use the ant colony optimization to find the shortest path for autonomous underwater vehicle. The simulation results show that the proposed approach outperforms other conventional protocols in some certain scenarios.","['wireless sensor networks', 'energy consumption', 'monitoring', 'protocols']"
189,"The success of Convolutional Neural Networks is highly dependent on the selected architecture and the hyper-parameters. The need for the automatic design of the networks is especially important for complex architectures where the parameter space is so large that trying all possible combinations is computationally infeasible. In this study, Microcanonical Optimization algorithm which is a variant of Simulated Annealing method is used for hyper-parameter optimization and architecture selection for Convolutional Neural Networks. To the best of our knowledge, our study provides a first attempt at applying Microcanonical Optimization for this task. The networks generated by the proposed method is compared to the networks generated by Simulated Annealing method in terms of both accuracy and size using six widely-used image recognition datasets. Moreover, a performance comparison using Tree Parzen Estimator which is a Bayesion optimization-based approach is also presented. It is shown that the proposed method is able to achieve competitive classification results with the state-of-the-art architectures. When the size of the networks is also taken into account, one can see that the networks generated by Microcanonical Optimization method contain far less parameters than the state-of-the-art architectures. Therefore, the proposed method can be preferred for automatically tuning the networks especially in situations where fast training is as important as the accuracy.","['convolution', 'optimization', 'convolutional neural networks', 'training']"
190,"In order to achieve the safe and efficient energy use in the electric vehicle, the continuous and accurate monitoring of lithium-ion batteries (LIBs) has become a long-standing research hot spot. However, existing researches of LIBs state of charge (SOC) prediction are at the cost of unrefined vector representation and inadequate feature extraction, which have been unable to meet prediction requirements of LIBs SOC. Complementarily, in this study, a deep learning-based SOC prediction model is proposed to ensure reliable vector representation and sufficient feature extraction. In order to improve battery data representation, a recursive neural networks (RNNs)-based method is proposed. Then, aiming to fully extract feature information, a multi-channel extended convolutional neural networks (CNNs)-based method, which is fed with the well-trained vector representation, is proposed to accurately predict LIBs SOC. Based on the reliable vector representation and sufficient feature extraction, the proposed method can provide improved SOC prediction performance. Merits of the proposed method are verified using simulation test, which shows that the proposed method gives improved prediction performance of 4.3% and 11.3% compared with recurrent neural networks and Ah counting method, respectively.","['batteries', 'feature extraction', 'neural networks', 'convolutional neural networks']"
191,"We consider multi-antenna wireless systems aided by reconfigurable intelligent surfaces (RIS). RIS presents a new physical layer technology for improving coverage and energy efficiency by intelligently controlling the propagation environment. In practice however, achieving the anticipated gains of RIS requires accurate channel estimation. Recent attempts to solve this problem have considered the least-squares (LS) approach, which is simple but also sub-optimal. The optimal channel estimator, based on the minimum mean-squared-error (MMSE) criterion, is challenging to obtain and is non-linear due to the non-Gaussianity of the effective channel seen at the receiver. Here we present approaches to approximate the optimal MMSE channel estimator. As a first approach, we analytically develop the best linear estimator, the LMMSE, together with a corresponding majorization-minimization-based algorithm designed to optimize the RIS phase shift matrix during the training phase. This estimator is shown to yield improved accuracy over the LS approach by exploiting second-order statistical properties of the wireless channel and the noise. To further improve performance and better approximate the globally-optimal MMSE channel estimator, we propose data-driven non-linear solutions based on deep learning. Specifically, by posing the MMSE channel estimation problem as an image denoising problem, we propose two convolutional neural network (CNN)-based methods to perform the denoising and approximate the optimal MMSE channel estimation solution. Our numerical results show that these CNN-based estimators give superior performance compared with linear estimation approaches. They also have low computational complexity requirements, thereby motivating their potential use in future RIS-aided wireless communication systems.","['wireless communication', 'training', 'deep learning', 'convolutional neural network']"
192,"Ambient backscatter communication is an emerging and promising low-energy technology for the Internet of Things. In such a system, a tag sends a binary message to a reader by backscattering a radio frequency signal generated by an ambient source. The tag can operate without battery and without generating additional radio waves. However, the tag-to-reader link suffers from the source-to-reader interference. In this paper, we propose a polarization-based reconfigurable antenna in order to improve the robustness of the tag-to-reader link against the source-to-reader direct interference. More precisely, we compare different types of tags' antennas, different tags' encoding schemes, and different detectors at the reader. By using analysis, numerical simulations, and experiments, we show that a polarization-based reconfigurable tag with four polarization directions significantly outperforms a non-reconfigurable tag, and provides almost the same performance as an ideal reconfigurable tag with a large number of reconfigurable polarization patterns.","['encoding', 'interference', 'detectors', 'internet of things']"
193,"Social networks are playing an increasingly important role in information dissemination in wireless ad hoc networks with the wide adoption of the mobile Internet. In particular, in social application scenarios such as Facebook and Twitter, the questions of how to fully consider the social relationships between users to ensure the quality of service (QoS) urgently needs to be explored. This paper investigates an optimal social relay selection scheme with high intimacy requirements to maximize the system throughput for social-physical ad hoc networks with device-to-device (D2D) communication. Different from previous studies, we jointly consider feasible relays and multiuser cooperative mobility with satisfactory link reliability for throughput maximization. On the one hand, we formulate a nonlinear and nonconvex problem, which is typically NP-hard, for throughput optimization. Specifically, this optimization problem is divided into two subproblems: i) selecting the optimal relays and ii) determining the best mobility strategy in terms of throughput. On the other hand, we first convert the relay selection subproblem into an optimal stopping phase problem, and then propose a definition of a graph representing the degrees of interference between physical links, transforming the original optimization problem into a convex problem that is solvable using the Lagrange multiplier method. Based on the above, we propose the Relay selection and Link Interference Degree Graph (RS-LIDG) algorithm to solve the two subproblems. Numerical simulations verify that the proposed RS-LIDG method improves the throughput gain by 26.29%, 123.43%, and 236.47% compared to the intuitive method (IM), the social-trust-based random mobility selection method (STS-RM), and the physical-based random mobility selection method (PHS-RM), respectively.","['relays', 'quality of service', 'optimization', 'interference']"
194,"With the rapid adoption of the Internet of Things, it is necessary to go beyond fifth-generation applications and apply stringent high reliability and low latency requirements, closely related to strict delay demands. These requirements support massive network connectivity for multiple Internet of Things devices. Hence, in this paper, we optimize energy efficiency and achieve quality-of-service requirements by mitigating co-channel interference, performing efficient power control of transmitters, and harvesting energy using time-slot exchanges. Due to a nonconvex optimization problem, we propose an iterative algorithm for power allocation and time slot interchange to reduce the computational complexity. To achieve a high degree of ultra-reliability and low latency with quality-of-service-aware instantaneous reward under massive connectivity, we efficiently employ multiagent reinforcement learning by addressing the intelligent resource management problem via a novel Double Deep Q Network. The network prioritizes experience replay to exploit the best policy and maximize accumulative rewards. It also learns the optimal policy and enhances learning efficiency by maximizing its reward function to make decisions with high intelligence and guarantee strict ultra-reliability and low latency. The simulation result shows that the Double Deep Q Network with prioritized experience replay can guarantee stringent ultra-reliability and low latency. As a result, the co-channel interference between transmission links and the high-power consumption density associated with the massive connectivity of the Internet of Things devices are mitigated.","['internet of things', 'reliability', 'resource management', 'interference']"
195,"The scarce spectrum and power resources, the inter-beam interference, together with the high traffic demand, pose new major challenges for the next generation of Very High Throughput Satellite (VHTS) systems. Accordingly, future satellites are expected to employ advanced resource/interference management techniques to achieve high system spectrum efficiency and low power consumption while ensuring user demand satisfaction. This paper proposes a novel demand and interference aware adaptive resource management for geostationary (GEO) VHTS systems. For this, we formulate a multi-objective optimization problem to minimize the total transmit power consumption and system bandwidth usage while matching the offered capacity with the demand per beam. In this context, we consider resource management for a system with full-precoding, i.e., all beams are precoded; without precoding, i.e., no precoding is applied to any beam; and with partial precoding, i.e., only some beams are precoded. The nature of the problem is non-convex and we solve it by jointly using the Dinkelbach and Successive Convex Approximation (SCA) methods. The simulation results show that the proposed method outperforms the benchmark schemes. Specifically, we show that the proposed method requires low resource consumption, low computational time, and simultaneously achieves a high demand satisfaction.","['interference', 'optimization', 'bandwidth', 'satellites', 'resource management']"
196,"Next generation cellular networks are expected to connect billions of devices through its massive machine-type communication (mMTC) use case. To achieve this, waveforms with improved spectral confinement and high spectral efficiency compared to cyclic-prefix orthogonal division multiplexing (CP-OFDM) are required. In this paper, filter-bank muilticarrier with quadrature amplitude modulation (FBMC-QAM) is studied as an alternative to CP-OFDM for such applications. However, FBMC-QAM presents the challenge of high intrinsic interference due to the loss of complex orthogonality in time-frequency domain. Bit-interleaved coded modulation with iterative decoding (BICM-ID) has been shown to exhibit capacity-approaching decoding performance. Therefore, in this paper, an iterative interference cancellation (IIC) based BICM-ID receiver is designed to cancel the intrinsic interference in FBMC-QAM systems. The proposed receiver consist of an inner decoder, which combines iterative demodulation and interference cancellation, and an outer decoder which is a low density parity check (LDPC) channel decoder. To evaluate the convergence behaviour of the proposed receiver, extrinsic information transfer (EXIT) chart analysis is employed, where EXIT curves for the components of the iterative decoder are derived. Numerical results show that the intrinsic interference in FBMC-QAM systems can be removed by adopting the proposed IIC-based BICM-ID receiver. With a receiver that is able to successfully cancel intrinsic interference, FBMC-QAM becomes an interesting alternative waveform for asynchronous mMTC applications due to its superior frequency localization compared to CP-OFDM. Furthermore, it has been shown that FBMC-QAM with the IIC-based receiver can achieve similar bit-error-rate (BER) performance as a CP-OFDM benchmark under different fading channels. Finally, the complexity of the proposed receiver for FBMC-QAM is analysed and compared to the complexity of the CP-OFDM benchmark.","['interference', 'decoding', 'convergence', 'modulation', 'ofdm']"
197,"Federated learning is one of the most appealing alternatives to the standard centralized learning paradigm, allowing a heterogeneous set of devices to train a machine learning model without sharing their raw data. However, it requires a central server to coordinate the learning process, thus introducing potential scalability and security issues. In the literature, server-less federated learning approaches like gossip federated learning and blockchain-enabled federated learning have been proposed to mitigate these issues. In this work, we propose a complete overview of these three techniques, proposing a comparison according to an integral set of performance indicators, including model accuracy, time complexity, communication overhead, convergence time, and energy consumption. An extensive simulation campaign permits to draw a quantitative analysis considering both feedforward and convolutional neural network models. Results show that gossip federated learning and standard federated solution are able to reach a similar level of accuracy, and their energy consumption is influenced by the machine learning model adopted, the software library, and the hardware used. Differently, blockchain-enabled federated learning represents a viable solution for implementing decentralized learning with a higher level of security, at the cost of an extra energy usage and data sharing. Finally, we identify open issues on the two decentralized federated learning implementations and provide insights on potential extensions and possible research directions on this new research field.","['security', 'energy consumption', 'blockchain', 'machine learning']"
198,"This paper studies a bandwidth-limited federated learning (FL) system where the access point is a central server for aggregation and the energy-constrained user equipments (UEs) with limited computation capabilities (e.g., Internet of Things devices) perform local training. Limited by the bandwidth in wireless edge systems, only a part of UEs can participate in each FL training round. Selecting different UEs could affect the FL performance, and selected UEs need to allocate their computing resource effectively. In wireless edge FL systems, simultaneously accelerating FL training and reducing computing-communication energy consumption are of importance. To this end, we formulate a multi-objective optimization problem (MOP). In MOP, the model training convergence is difficult to calculate accurately. Meanwhile, MOP is a combinatorial optimization problem, with the high-dimension mix-integer variables, which is proved to be NP-hard. To address these challenges, a multi-objective evolutionary algorithm for the bandwidth-limited FL system (MOEA-FL) is proposed to obtain a Pareto optimal solution set. In MOEA-FL, an age-of-update-loss method is first proposed to transform the original global loss function into a convergence reference function. Then, MOEA-FL divides MOP intoNsingle objective subproblems by the Tchebycheff approach and optimizes the subproblems simultaneously by evolving a population. Extensive experiments have been carried out on MNIST dataset and a medical case called TissueMNIST dataset for both the i.i.d and non-i.i.d data setting. Experimental results demonstrate that MOEA-FL performs better than other algorithms and verify the robustness and scalability of MOEA-FL.","['training', 'convergence', 'energy consumption', 'optimization', 'bandwidth']"
199,"In this paper we demonstrate the application of Fully Convolutional Neural Network (FCN) for Frame Synchronization (FS) in bursty single carrier transmissions, commonly used in wireless sensor networks and Internet of Things (IoT) applications. Our approach shows greatly improved performance compared to noncoherent correlation-based methods under carrier phase and frequency offsets, especially for shorter preambles. Using a fully convolutional architecture allows the training of a deep filter, which we believe is more suited to signal processing tasks than more commonly used deep learning architectures with fully connected layers. In terms of deployment within a wider communications system, it could be treated similarly to a typical signal processing filter, which means it can be deployed to inputs of arbitrary length. Additionally, because the proposed model is composed only of convolutional layers, the entire model benefits from the weight sharing property of convolutional filters, and results in a greatly reduced memory footprint compared to that of similar models containing fully connected layers.","['correlation', 'training', 'deep learning', 'internet of things']"
