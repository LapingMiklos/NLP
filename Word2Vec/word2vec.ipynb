{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4151b0",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57981b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb1ce499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11024\n"
     ]
    }
   ],
   "source": [
    "RE_WORD = r'\\w+'\n",
    "re_word = re.compile(RE_WORD, re.U)\n",
    "\n",
    "DATA_SET = [\n",
    "    \"data/frankenstein.txt\",\n",
    "    \"data/moby_dick.txt\",\n",
    "    \"data/freud.txt\", \n",
    "    \"data/wiki.txt\", \n",
    "    \"data/middlemarch.txt\", \n",
    "    \"data/pride_and_prejudice.txt\", \n",
    "    \"data/alice_in_wonderland.txt\",\n",
    "    \"data/bleakhouse.txt\",\n",
    "    \"data/crime_and_punishment.txt\",\n",
    "    \"data/room_with_a_view.txt\"\n",
    "]\n",
    "\n",
    "def extract_vocab(paths: list[str]) -> dict[str, int]:\n",
    "    vocab = dict()\n",
    "    for path in paths:\n",
    "        with open(path, encoding=\"utf-8\") as file:\n",
    "            for word in (word.lower() for word in re.findall(re_word, file.read())):\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] = vocab[word] + 1\n",
    "\n",
    "    words = (word for word, count in vocab.items() if count > 8)\n",
    "    return { word:idx for idx, word in enumerate(words) }\n",
    "\n",
    "def tokenize(text: str, vocab: dict[str, int]) -> list[str]:\n",
    "    return [word.lower() for word in re.findall(re_word, text) if word in vocab]\n",
    "\n",
    "def load_data(path: str) -> str:\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "        return text\n",
    "\n",
    "vocab = extract_vocab(DATA_SET)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0a8e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_one_hot(batch: list[list[int]], vocab_size: int) -> torch.Tensor:\n",
    "    t = torch.zeros((len(batch), vocab_size), dtype=torch.float32)\n",
    "\n",
    "    for i, indeces in enumerate(batch):\n",
    "        t[i, indeces] = 1.0\n",
    "    \n",
    "    return t\n",
    "\n",
    "def tokens_to_indeces(tokens: list[str], vocab: dict[str, int]) -> list[int]:\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "def gen_text_to_tensors(text: str, vocab: dict[str, int], window_size: int, batch_size: int):\n",
    "    tokens = tokenize(text, vocab)\n",
    "    indeces = tokens_to_indeces(tokens, vocab)\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "\n",
    "    words = []\n",
    "    context = []\n",
    "    for i in range(half_window, len(indeces) - half_window):\n",
    "        words.append([indeces[i]])\n",
    "        context.append(indeces[i-half_window:i] + indeces[i+1:i+1+half_window])\n",
    "\n",
    "        if (i % batch_size == 0):\n",
    "            yield batch_to_one_hot(words, len(vocab)), batch_to_one_hot(context, len(vocab))\n",
    "            words = []\n",
    "            context = []\n",
    "        \n",
    "    yield batch_to_one_hot(words, len(vocab)), batch_to_one_hot(context, len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d5da1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "VECTOR_SIZE = 100\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(VOCAB_SIZE, VECTOR_SIZE),\n",
    "    nn.Linear(VECTOR_SIZE, VOCAB_SIZE),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a0b49237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lapin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in DATA_SET:\n",
    "        text = load_data(data)\n",
    "        \n",
    "        for Y, X in gen_text_to_tensors(text, vocab, window_size=5, batch_size=1024):\n",
    "            outputs = model(X.float())\n",
    "            loss = loss_fn(outputs, Y.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.354 0.473 love sex\n",
      "0.470 0.871 tiger cat\n",
      "1.000 1.000 tiger tiger\n",
      "0.492 0.736 book paper\n",
      "0.524 0.607 computer keyboard\n",
      "0.516 0.548 computer internet\n",
      "0.500 -0.801 telephone communication\n",
      "0.354 0.076 television radio\n",
      "0.484 -0.412 media radio\n",
      "0.370 -0.295 drug abuse\n",
      "0.238 -0.114 bread butter\n",
      "0.400 0.637 doctor nurse\n",
      "0.324 0.366 professor doctor\n",
      "0.362 0.305 student professor\n",
      "-0.076 0.690 smart student\n",
      "0.162 -0.500 smart stupid\n",
      "0.416 0.814 company stock\n",
      "0.616 0.666 stock market\n",
      "-0.676 0.360 stock phone\n",
      "-0.638 0.590 stock egg\n",
      "-0.254 0.857 stock live\n",
      "-0.816 0.193 stock life\n",
      "0.492 0.734 book library\n",
      "0.624 0.918 bank money\n",
      "0.546 0.164 wood forest\n",
      "0.830 0.311 money cash\n",
      "0.716 0.413 king queen\n",
      "0.692 0.225 jerusalem israel\n",
      "0.530 0.214 jerusalem palestinian\n",
      "-0.676 0.581 holy sex\n",
      "0.724 0.048 maradona football\n",
      "0.806 0.044 football soccer\n",
      "0.326 0.977 football tennis\n",
      "0.512 -0.702 tennis racket\n",
      "0.346 0.072 arafat peace\n",
      "0.530 -0.034 arafat terror\n",
      "-0.500 0.040 arafat jackson\n",
      "0.676 0.834 law lawyer\n",
      "0.476 0.795 movie star\n",
      "0.346 0.567 movie critic\n",
      "0.584 0.764 movie theater\n",
      "0.470 -0.600 physics chemistry\n",
      "-0.024 -0.712 space chemistry\n",
      "0.108 -0.192 alcohol chemistry\n",
      "0.692 0.887 vodka gin\n",
      "0.626 -0.738 vodka brandy\n",
      "-0.738 0.785 drink ear\n",
      "0.192 0.680 drink mouth\n",
      "0.374 0.602 drink eat\n",
      "0.570 0.731 baby mother\n",
      "-0.470 0.966 drink mother\n",
      "0.792 0.366 gem jewel\n",
      "0.858 0.589 journey voyage\n",
      "0.766 0.975 boy lad\n",
      "0.820 0.979 coast shore\n",
      "0.804 0.939 magician wizard\n",
      "0.504 -0.025 food fruit\n",
      "0.420 0.090 bird cock\n",
      "0.254 0.058 brother monk\n",
      "-0.108 0.957 lad brother\n",
      "-0.584 0.950 cemetery woodland\n",
      "-0.124 0.881 coast hill\n",
      "-0.630 0.937 forest graveyard\n",
      "-0.384 0.647 shore woodland\n",
      "-0.816 0.076 monk slave\n",
      "-0.370 0.499 coast forest\n",
      "-0.816 0.889 lad wizard\n",
      "-0.584 -0.130 glass magician\n",
      "-0.892 0.725 noon string\n",
      "0.684 -0.563 money dollar\n",
      "0.816 0.311 money cash\n",
      "0.654 -0.544 money wealth\n",
      "0.514 -0.416 money property\n",
      "0.458 -0.316 money possession\n",
      "0.700 0.918 money bank\n",
      "0.546 -0.385 money deposit\n",
      "-0.338 0.447 money operation\n",
      "0.400 -0.243 tiger animal\n",
      "-0.046 0.938 tiger organism\n",
      "0.124 0.014 tiger fauna\n",
      "0.174 -0.714 tiger zoo\n",
      "0.400 -0.046 psychology anxiety\n",
      "0.370 0.843 psychology fear\n",
      "0.484 0.595 psychology depression\n",
      "0.284 -0.733 psychology doctor\n",
      "0.642 -0.002 psychology freud\n",
      "0.538 0.842 psychology mind\n",
      "0.446 0.889 psychology health\n",
      "0.342 0.950 psychology science\n",
      "0.116 0.920 psychology discipline\n",
      "0.496 0.865 psychology cognition\n",
      "0.690 0.880 planet star\n",
      "0.616 0.870 planet moon\n",
      "0.604 0.893 planet sun\n",
      "0.622 -0.103 planet galaxy\n",
      "0.584 -0.798 planet space\n",
      "0.170 -0.584 precedent example\n",
      "-0.230 -0.649 precedent information\n",
      "-0.438 -0.116 precedent cognition\n",
      "0.330 -0.464 precedent law\n",
      "-0.500 -0.621 precedent collection\n",
      "-0.646 -0.854 precedent group\n",
      "0.316 0.316 cup coffee\n",
      "-0.520 0.490 cup article\n",
      "-0.262 -0.298 cup object\n",
      "-0.570 -0.726 cup entity\n",
      "0.450 -0.708 cup drink\n",
      "0.000 0.142 cup food\n",
      "-0.616 0.356 cup substance\n",
      "0.180 -0.721 cup liquid\n",
      "-0.638 0.459 energy secretary\n",
      "0.018 -0.857 energy laboratory\n",
      "0.356 0.469 computer laboratory\n",
      "0.212 0.940 weapon secret\n",
      "-0.082 -0.673 investigation effort\n",
      "0.632 -0.598 news report\n",
      "-0.088 0.871 image surface\n",
      "0.268 -0.512 discovery space\n",
      "-0.524 0.886 sign recess\n",
      "-0.556 0.055 wednesday news\n",
      "-0.106 0.491 computer news\n",
      "-0.262 0.929 atmosphere landscape\n",
      "-0.400 -0.786 president medal\n",
      "0.262 0.417 record number\n",
      "0.244 0.938 skin eye\n",
      "0.300 0.095 japanese american\n",
      "-0.218 -0.557 theater history\n",
      "-0.400 0.673 prejudice recognition\n",
      "0.518 0.868 century year\n",
      "-0.368 0.712 century nation\n",
      "-0.762 0.666 delay racism\n",
      "-0.338 -0.578 delay news\n",
      "0.326 0.944 minister party\n",
      "-0.050 0.879 peace plan\n",
      "-0.150 0.828 attempt peace\n",
      "0.312 0.038 government crisis\n",
      "0.188 0.616 energy crisis\n",
      "0.512 -0.286 announcement news\n",
      "-0.450 0.055 announcement effort\n",
      "0.406 -0.726 stroke hospital\n",
      "0.294 -0.457 victim emergency\n",
      "0.582 0.929 treatment recovery\n",
      "-0.006 -0.058 journal association\n",
      "0.038 0.392 doctor liability\n",
      "0.406 -0.564 liability insurance\n",
      "-0.312 -0.379 school center\n",
      "0.476 0.007 hundred percent\n",
      "0.050 0.939 death row\n",
      "0.338 -0.368 lawyer evidence\n",
      "0.576 0.913 life death\n",
      "-0.100 -0.326 life term\n",
      "-0.106 -0.672 board recommendation\n",
      "-0.350 0.615 governor interview\n",
      "-0.262 0.187 peace atmosphere\n",
      "-0.412 0.418 peace insurance\n",
      "0.000 0.395 travel activity\n",
      "0.288 0.850 competition price\n",
      "-0.174 0.362 consumer confidence\n",
      "-0.050 -0.807 consumer energy\n",
      "0.612 0.897 credit card\n",
      "0.062 0.266 credit information\n",
      "0.606 -0.824 hotel reservation\n",
      "0.082 -0.394 arrangement accommodation\n",
      "-0.638 0.551 month hotel\n",
      "0.794 0.942 type kind\n",
      "0.200 0.457 arrival hotel\n",
      "0.344 0.810 bed closet\n",
      "0.600 0.655 closet clothes\n",
      "-0.038 0.967 situation conclusion\n",
      "-0.224 0.398 situation isolation\n",
      "-0.550 -0.537 direction combination\n",
      "0.288 0.821 street place\n",
      "0.776 0.633 street avenue\n",
      "0.376 -0.136 street block\n",
      "-0.012 0.862 street children\n",
      "0.562 0.054 cell phone\n",
      "-0.224 -0.031 media trading\n",
      "-0.424 0.247 media gain\n",
      "0.526 -0.216 dividend payment\n",
      "0.296 0.885 dividend calculation\n",
      "0.268 0.547 oil stock\n",
      "-0.324 0.543 announcement production\n",
      "0.200 -0.052 announcement warning\n",
      "-0.224 -0.665 profit warning\n",
      "0.526 0.908 profit loss\n",
      "0.476 0.724 dollar profit\n",
      "0.218 0.592 dollar loss\n",
      "0.700 0.577 computer software\n",
      "0.662 0.918 network hardware\n",
      "0.426 0.742 phone equipment\n",
      "0.182 -0.853 equipment maker\n",
      "-0.324 0.080 five month\n",
      "-0.274 0.271 report gain\n",
      "0.578 0.305 liquid water\n",
      "0.406 0.372 game victory\n",
      "0.538 0.598 game team\n",
      "0.238 0.189 game series\n",
      "0.394 0.719 game defeat\n",
      "-0.288 0.954 seven series\n",
      "0.494 -0.166 seafood sea\n",
      "0.668 0.359 seafood food\n",
      "0.740 -0.699 seafood lobster\n",
      "0.562 0.124 lobster food\n",
      "0.140 -0.726 lobster wine\n",
      "0.244 0.298 food preparation\n",
      "0.268 -0.823 video archive\n",
      "-0.188 0.838 start year\n",
      "-0.106 0.379 start match\n",
      "0.194 0.771 game round\n",
      "0.672 0.893 championship tournament\n",
      "-0.462 0.132 line insurance\n",
      "-0.212 0.942 day summer\n",
      "0.126 -0.072 summer nature\n",
      "0.506 0.837 day dawn\n",
      "0.662 -0.377 nature environment\n",
      "0.762 -0.846 environment ecology\n",
      "0.250 0.487 nature man\n",
      "0.660 0.732 man woman\n",
      "0.050 0.492 man governor\n",
      "0.588 0.895 soap opera\n",
      "0.376 0.795 opera performance\n",
      "0.188 0.169 life lesson\n",
      "-0.188 0.321 focus life\n",
      "0.250 -0.061 production crew\n",
      "0.544 -0.501 television film\n",
      "0.238 0.678 lover quarrel\n",
      "-0.612 -0.634 possibility girl\n",
      "-0.250 0.867 population development\n",
      "-0.338 0.854 morality importance\n",
      "-0.262 0.712 morality marriage\n",
      "0.488 0.056 mexico brazil\n",
      "0.282 0.903 gender equality\n",
      "0.088 -0.155 change attitude\n",
      "0.250 0.490 family planning\n",
      "-0.474 0.813 opera industry\n",
      "-0.824 -0.331 sugar approach\n",
      "-0.362 0.355 practice institution\n",
      "-0.062 -0.812 ministry culture\n",
      "0.350 0.816 problem challenge\n",
      "0.462 0.155 country citizen\n",
      "0.150 -0.151 planet people\n",
      "-0.206 -0.322 development issue\n",
      "-0.306 0.701 experience music\n",
      "-0.274 0.261 music project\n",
      "0.112 0.831 glass metal\n",
      "0.062 -0.566 exhibit memorabilia\n",
      "0.438 -0.899 museum theater\n",
      "-0.124 -0.701 observation architecture\n",
      "0.306 -0.298 space world\n",
      "0.238 -0.017 preservation world\n",
      "0.538 -0.937 admission ticket\n",
      "0.206 -0.522 shower flood\n",
      "0.250 0.804 disaster area\n",
      "0.268 0.788 governor office\n",
      "-0.244 -0.811 architecture century\n",
      "MSE:  0.5125618014464597\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.transpose(list(model.parameters())[0].detach().clone(), 0, 1)\n",
    "\n",
    "wordsim353 = pd.read_csv(\"wordsim353/combined.csv\")\n",
    "\n",
    "not_in_vocab = set()\n",
    "wordsim353['Word 1'] = wordsim353['Word 1'].apply(lambda w: w.lower())\n",
    "wordsim353['Word 2'] = wordsim353['Word 2'].apply(lambda w: w.lower())\n",
    "wordsim353['Human (mean)'] = wordsim353['Human (mean)'].apply(lambda m: m / 5 - 1)\n",
    "\n",
    "count = 0\n",
    "error = 0\n",
    "not_in_vocab = set()\n",
    "for _, row in wordsim353.iterrows():\n",
    "    w1 = row['Word 1']\n",
    "    w2 = row['Word 2']\n",
    "    human = row['Human (mean)']\n",
    "\n",
    "    if w1 not in vocab:\n",
    "        not_in_vocab.add(w1)\n",
    "        continue\n",
    "    if w2 not in vocab:\n",
    "        not_in_vocab.add(w2)\n",
    "        continue\n",
    "\n",
    "    sim = F.cosine_similarity(embeddings[vocab[w1]].unsqueeze(0), embeddings[vocab[w2]].unsqueeze(0)).tolist()[0]\n",
    "    error += (human - sim) ** 2\n",
    "    count += 1\n",
    "    print(f'{human:.3f}, {sim:.3f}, {w1}, {w2}')\n",
    "\n",
    "\n",
    "print(\"MSE: \", error / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/embeds.json\", \"+w\", encoding=\"utf-8\") as file:\n",
    "    file.write(json.dumps({ word:embeddings[idx].tolist() for word, idx in vocab.items() }))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
