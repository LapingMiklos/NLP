{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4151b0",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57981b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb1ce499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11024\n"
     ]
    }
   ],
   "source": [
    "RE_WORD = r'\\w+'\n",
    "re_word = re.compile(RE_WORD, re.U)\n",
    "\n",
    "DATA_SET = [\n",
    "    \"data/frankenstein.txt\",\n",
    "    \"data/moby_dick.txt\",\n",
    "    \"data/freud.txt\", \n",
    "    \"data/wiki.txt\", \n",
    "    \"data/middlemarch.txt\", \n",
    "    \"data/pride_and_prejudice.txt\", \n",
    "    \"data/alice_in_wonderland.txt\",\n",
    "    \"data/bleakhouse.txt\",\n",
    "    \"data/crime_and_punishment.txt\",\n",
    "    \"data/room_with_a_view.txt\"\n",
    "]\n",
    "\n",
    "def extract_vocab(paths: list[str]) -> dict[str, int]:\n",
    "    vocab = dict()\n",
    "    for path in paths:\n",
    "        with open(path, encoding=\"utf-8\") as file:\n",
    "            for word in (word.lower() for word in re.findall(re_word, file.read())):\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 1\n",
    "                else:\n",
    "                    vocab[word] = vocab[word] + 1\n",
    "\n",
    "    words = (word for word, count in vocab.items() if count > 8)\n",
    "    return { word:idx for idx, word in enumerate(words) }\n",
    "\n",
    "def tokenize(text: str, vocab: dict[str, int]) -> list[str]:\n",
    "    return [word.lower() for word in re.findall(re_word, text) if word in vocab]\n",
    "\n",
    "def load_data(path: str) -> str:\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "        return text\n",
    "\n",
    "vocab = extract_vocab(DATA_SET)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0a8e702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_one_hot(batch: list[list[int]], vocab_size: int) -> torch.Tensor:\n",
    "    t = torch.zeros((len(batch), vocab_size), dtype=torch.float32)\n",
    "\n",
    "    for i, indeces in enumerate(batch):\n",
    "        t[i, indeces] = 1.0\n",
    "    \n",
    "    return t\n",
    "\n",
    "def tokens_to_indeces(tokens: list[str], vocab: dict[str, int]) -> list[int]:\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "def gen_text_to_tensors(text: str, vocab: dict[str, int], window_size: int, batch_size: int):\n",
    "    tokens = tokenize(text, vocab)\n",
    "    indeces = tokens_to_indeces(tokens, vocab)\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "\n",
    "    words = []\n",
    "    context = []\n",
    "    for i in range(half_window, len(indeces) - half_window):\n",
    "        words.append([indeces[i]])\n",
    "        context.append(indeces[i-half_window:i] + indeces[i+1:i+1+half_window])\n",
    "\n",
    "        if (i % batch_size == 0):\n",
    "            yield batch_to_one_hot(words, len(vocab)), batch_to_one_hot(context, len(vocab))\n",
    "            words = []\n",
    "            context = []\n",
    "        \n",
    "    yield batch_to_one_hot(words, len(vocab)), batch_to_one_hot(context, len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d5da1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "VECTOR_SIZE = 100\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(VOCAB_SIZE, VECTOR_SIZE),\n",
    "    nn.Linear(VECTOR_SIZE, VOCAB_SIZE),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a0b49237",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in DATA_SET:\n",
    "        text = load_data(data)\n",
    "        \n",
    "        for Y, X in gen_text_to_tensors(text, vocab, window_size=5, batch_size=1024):\n",
    "            outputs = model(X.float())\n",
    "            loss = loss_fn(outputs, Y.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f6ac022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.354, -0.620, love, sex\n",
      "0.470, 0.807, tiger, cat\n",
      "1.000, 1.000, tiger, tiger\n",
      "0.492, 0.857, book, paper\n",
      "0.524, 0.474, computer, keyboard\n",
      "0.516, 0.685, computer, internet\n",
      "0.500, 0.515, telephone, communication\n",
      "0.354, 0.667, television, radio\n",
      "0.484, -0.293, media, radio\n",
      "0.370, 0.142, drug, abuse\n",
      "0.238, 0.593, bread, butter\n",
      "0.400, -0.281, doctor, nurse\n",
      "0.324, 0.661, professor, doctor\n",
      "0.362, 0.770, student, professor\n",
      "-0.076, 0.817, smart, student\n",
      "0.162, -0.204, smart, stupid\n",
      "0.416, -0.092, company, stock\n",
      "0.616, -0.462, stock, market\n",
      "-0.676, 0.385, stock, phone\n",
      "-0.638, 0.429, stock, egg\n",
      "-0.254, -0.410, stock, live\n",
      "-0.816, -0.106, stock, life\n",
      "0.492, 0.308, book, library\n",
      "0.624, 0.877, bank, money\n",
      "0.546, 0.436, wood, forest\n",
      "0.830, 0.676, money, cash\n",
      "0.716, -0.311, king, queen\n",
      "0.692, -0.047, jerusalem, israel\n",
      "0.530, -0.035, jerusalem, palestinian\n",
      "-0.676, -0.588, holy, sex\n",
      "0.724, 0.105, maradona, football\n",
      "0.806, 0.287, football, soccer\n",
      "0.326, 0.934, football, tennis\n",
      "0.512, -0.824, tennis, racket\n",
      "0.346, 0.085, arafat, peace\n",
      "0.530, 0.062, arafat, terror\n",
      "-0.500, -0.091, arafat, jackson\n",
      "0.676, 0.510, law, lawyer\n",
      "0.476, -0.339, movie, star\n",
      "0.346, -0.804, movie, critic\n",
      "0.584, 0.778, movie, theater\n",
      "0.470, 0.744, physics, chemistry\n",
      "-0.024, 0.349, space, chemistry\n",
      "0.108, 0.636, alcohol, chemistry\n",
      "0.692, -0.609, vodka, gin\n",
      "0.626, -0.396, vodka, brandy\n",
      "-0.738, 0.208, drink, ear\n",
      "0.192, -0.645, drink, mouth\n",
      "0.374, 0.882, drink, eat\n",
      "0.570, 0.757, baby, mother\n",
      "-0.470, -0.406, drink, mother\n",
      "0.792, 0.506, gem, jewel\n",
      "0.858, 0.474, journey, voyage\n",
      "0.766, 0.988, boy, lad\n",
      "0.820, 0.508, coast, shore\n",
      "0.804, 0.165, magician, wizard\n",
      "0.504, -0.463, food, fruit\n",
      "0.420, -0.699, bird, cock\n",
      "0.254, 0.007, brother, monk\n",
      "-0.108, 0.617, lad, brother\n",
      "-0.584, 0.757, cemetery, woodland\n",
      "-0.124, 0.348, coast, hill\n",
      "-0.630, 0.744, forest, graveyard\n",
      "-0.384, 0.521, shore, woodland\n",
      "-0.816, 0.160, monk, slave\n",
      "-0.370, 0.376, coast, forest\n",
      "-0.816, 0.473, lad, wizard\n",
      "-0.584, 0.624, glass, magician\n",
      "-0.892, -0.362, noon, string\n",
      "0.684, 0.329, money, dollar\n",
      "0.816, 0.676, money, cash\n",
      "0.654, 0.379, money, wealth\n",
      "0.514, 0.889, money, property\n",
      "0.458, 0.452, money, possession\n",
      "0.700, 0.877, money, bank\n",
      "0.546, -0.213, money, deposit\n",
      "-0.338, 0.848, money, operation\n",
      "0.400, 0.002, tiger, animal\n",
      "-0.046, 0.800, tiger, organism\n",
      "0.124, 0.860, tiger, fauna\n",
      "0.174, 0.874, tiger, zoo\n",
      "0.400, 0.560, psychology, anxiety\n",
      "0.370, 0.768, psychology, fear\n",
      "0.484, 0.020, psychology, depression\n",
      "0.284, -0.417, psychology, doctor\n",
      "0.642, -0.019, psychology, freud\n",
      "0.538, 0.582, psychology, mind\n",
      "0.446, 0.087, psychology, health\n",
      "0.342, 0.224, psychology, science\n",
      "0.116, -0.112, psychology, discipline\n",
      "0.496, 0.491, psychology, cognition\n",
      "0.690, 0.374, planet, star\n",
      "0.616, 0.754, planet, moon\n",
      "0.604, 0.798, planet, sun\n",
      "0.622, 0.052, planet, galaxy\n",
      "0.584, -0.361, planet, space\n",
      "0.170, -0.605, precedent, example\n",
      "-0.230, 0.209, precedent, information\n",
      "-0.438, 0.452, precedent, cognition\n",
      "0.330, -0.794, precedent, law\n",
      "-0.500, -0.602, precedent, collection\n",
      "-0.646, -0.558, precedent, group\n",
      "0.316, -0.091, cup, coffee\n",
      "-0.520, 0.833, cup, article\n",
      "-0.262, 0.458, cup, object\n",
      "-0.570, 0.149, cup, entity\n",
      "0.450, -0.669, cup, drink\n",
      "0.000, -0.014, cup, food\n",
      "-0.616, 0.450, cup, substance\n",
      "0.180, 0.330, cup, liquid\n",
      "-0.638, -0.468, energy, secretary\n",
      "0.018, 0.067, energy, laboratory\n",
      "0.356, 0.419, computer, laboratory\n",
      "0.212, -0.372, weapon, secret\n",
      "-0.082, -0.631, investigation, effort\n",
      "0.632, 0.445, news, report\n",
      "-0.088, 0.562, image, surface\n",
      "0.268, 0.473, discovery, space\n",
      "-0.524, 0.735, sign, recess\n",
      "-0.556, -0.011, wednesday, news\n",
      "-0.106, 0.557, computer, news\n",
      "-0.262, 0.949, atmosphere, landscape\n",
      "-0.400, 0.792, president, medal\n",
      "0.262, 0.424, record, number\n",
      "0.244, 0.855, skin, eye\n",
      "0.300, -0.034, japanese, american\n",
      "-0.218, -0.369, theater, history\n",
      "-0.400, 0.629, prejudice, recognition\n",
      "0.518, 0.936, century, year\n",
      "-0.368, 0.722, century, nation\n",
      "-0.762, -0.874, delay, racism\n",
      "-0.338, 0.523, delay, news\n",
      "0.326, 0.598, minister, party\n",
      "-0.050, 0.822, peace, plan\n",
      "-0.150, 0.271, attempt, peace\n",
      "0.312, 0.815, government, crisis\n",
      "0.188, -0.108, energy, crisis\n",
      "0.512, 0.824, announcement, news\n",
      "-0.450, 0.129, announcement, effort\n",
      "0.406, -0.298, stroke, hospital\n",
      "0.294, 0.708, victim, emergency\n",
      "0.582, 0.702, treatment, recovery\n",
      "-0.006, 0.108, journal, association\n",
      "0.038, -0.375, doctor, liability\n",
      "0.406, 0.323, liability, insurance\n",
      "-0.312, -0.223, school, center\n",
      "0.476, -0.889, hundred, percent\n",
      "0.050, 0.305, death, row\n",
      "0.338, 0.237, lawyer, evidence\n",
      "0.576, 0.900, life, death\n",
      "-0.100, -0.477, life, term\n",
      "-0.106, 0.611, board, recommendation\n",
      "-0.350, 0.760, governor, interview\n",
      "-0.262, -0.115, peace, atmosphere\n",
      "-0.412, 0.810, peace, insurance\n",
      "0.000, 0.849, travel, activity\n",
      "0.288, 0.589, competition, price\n",
      "-0.174, -0.272, consumer, confidence\n",
      "-0.050, -0.659, consumer, energy\n",
      "0.612, -0.093, credit, card\n",
      "0.062, -0.468, credit, information\n",
      "0.606, -0.493, hotel, reservation\n",
      "0.082, -0.548, arrangement, accommodation\n",
      "-0.638, 0.585, month, hotel\n",
      "0.794, 0.985, type, kind\n",
      "0.200, -0.702, arrival, hotel\n",
      "0.344, 0.173, bed, closet\n",
      "0.600, -0.081, closet, clothes\n",
      "-0.038, 0.581, situation, conclusion\n",
      "-0.224, 0.548, situation, isolation\n",
      "-0.550, 0.336, direction, combination\n",
      "0.288, 0.930, street, place\n",
      "0.776, 0.579, street, avenue\n",
      "0.376, 0.454, street, block\n",
      "-0.012, 0.576, street, children\n",
      "0.562, 0.691, cell, phone\n",
      "-0.224, 0.230, media, trading\n",
      "-0.424, 0.522, media, gain\n",
      "0.526, -0.298, dividend, payment\n",
      "0.296, 0.777, dividend, calculation\n",
      "0.268, 0.628, oil, stock\n",
      "-0.324, -0.329, announcement, production\n",
      "0.200, 0.101, announcement, warning\n",
      "-0.224, 0.700, profit, warning\n",
      "0.526, 0.490, profit, loss\n",
      "0.476, 0.869, dollar, profit\n",
      "0.218, 0.625, dollar, loss\n",
      "0.700, 0.597, computer, software\n",
      "0.662, 0.327, network, hardware\n",
      "0.426, -0.530, phone, equipment\n",
      "0.182, 0.300, equipment, maker\n",
      "-0.324, -0.526, five, month\n",
      "-0.274, 0.687, report, gain\n",
      "0.578, 0.473, liquid, water\n",
      "0.406, -0.041, game, victory\n",
      "0.538, 0.861, game, team\n",
      "0.238, 0.552, game, series\n",
      "0.394, 0.646, game, defeat\n",
      "-0.288, -0.495, seven, series\n",
      "0.494, -0.734, seafood, sea\n",
      "0.668, -0.001, seafood, food\n",
      "0.740, -0.367, seafood, lobster\n",
      "0.562, 0.556, lobster, food\n",
      "0.140, -0.693, lobster, wine\n",
      "0.244, -0.319, food, preparation\n",
      "0.268, -0.871, video, archive\n",
      "-0.188, 0.450, start, year\n",
      "-0.106, 0.742, start, match\n",
      "0.194, 0.326, game, round\n",
      "0.672, -0.055, championship, tournament\n",
      "-0.462, -0.035, line, insurance\n",
      "-0.212, 0.570, day, summer\n",
      "0.126, 0.213, summer, nature\n",
      "0.506, 0.366, day, dawn\n",
      "0.662, -0.546, nature, environment\n",
      "0.762, -0.593, environment, ecology\n",
      "0.250, 0.382, nature, man\n",
      "0.660, 0.801, man, woman\n",
      "0.050, 0.430, man, governor\n",
      "0.588, 0.680, soap, opera\n",
      "0.376, 0.085, opera, performance\n",
      "0.188, -0.329, life, lesson\n",
      "-0.188, -0.618, focus, life\n",
      "0.250, -0.782, production, crew\n",
      "0.544, -0.228, television, film\n",
      "0.238, -0.058, lover, quarrel\n",
      "-0.612, 0.222, possibility, girl\n",
      "-0.250, 0.382, population, development\n",
      "-0.338, 0.796, morality, importance\n",
      "-0.262, 0.755, morality, marriage\n",
      "0.488, -0.018, mexico, brazil\n",
      "0.282, 0.258, gender, equality\n",
      "0.088, 0.889, change, attitude\n",
      "0.250, -0.621, family, planning\n",
      "-0.474, 0.892, opera, industry\n",
      "-0.824, -0.520, sugar, approach\n",
      "-0.362, -0.457, practice, institution\n",
      "-0.062, -0.935, ministry, culture\n",
      "0.350, -0.035, problem, challenge\n",
      "0.462, 0.660, country, citizen\n",
      "0.150, 0.240, planet, people\n",
      "-0.206, -0.726, development, issue\n",
      "-0.306, 0.672, experience, music\n",
      "-0.274, 0.494, music, project\n",
      "0.112, 0.528, glass, metal\n",
      "0.062, -0.537, exhibit, memorabilia\n",
      "0.438, -0.837, museum, theater\n",
      "-0.124, 0.512, observation, architecture\n",
      "0.306, 0.189, space, world\n",
      "0.238, -0.382, preservation, world\n",
      "0.538, -0.441, admission, ticket\n",
      "0.206, 0.778, shower, flood\n",
      "0.250, 0.229, disaster, area\n",
      "0.268, -0.114, governor, office\n",
      "-0.244, -0.715, architecture, century\n",
      "MSE:  0.4330275550649799\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.transpose(list(model.parameters())[0].detach().clone(), 0, 1)\n",
    "\n",
    "wordsim353 = pd.read_csv(\"wordsim353/combined.csv\")\n",
    "\n",
    "not_in_vocab = set()\n",
    "wordsim353['Word 1'] = wordsim353['Word 1'].apply(lambda w: w.lower())\n",
    "wordsim353['Word 2'] = wordsim353['Word 2'].apply(lambda w: w.lower())\n",
    "wordsim353['Human (mean)'] = wordsim353['Human (mean)'].apply(lambda m: m / 5 - 1)\n",
    "\n",
    "count = 0\n",
    "error = 0\n",
    "not_in_vocab = set()\n",
    "for _, row in wordsim353.iterrows():\n",
    "    w1 = row['Word 1']\n",
    "    w2 = row['Word 2']\n",
    "    human = row['Human (mean)']\n",
    "\n",
    "    if w1 not in vocab:\n",
    "        not_in_vocab.add(w1)\n",
    "        continue\n",
    "    if w2 not in vocab:\n",
    "        not_in_vocab.add(w2)\n",
    "        continue\n",
    "\n",
    "    sim = F.cosine_similarity(embeddings[vocab[w1]].unsqueeze(0), embeddings[vocab[w2]].unsqueeze(0)).tolist()[0]\n",
    "    error += (human - sim) ** 2\n",
    "    count += 1\n",
    "    print(f'{human:.3f}, {sim:.3f}, {w1}, {w2}')\n",
    "\n",
    "\n",
    "print(\"MSE: \", error / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5568910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/embeds.json\", \"+w\", encoding=\"utf-8\") as file:\n",
    "    file.write(json.dumps({ word:embeddings[idx].tolist() for word, idx in vocab.items() }))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
